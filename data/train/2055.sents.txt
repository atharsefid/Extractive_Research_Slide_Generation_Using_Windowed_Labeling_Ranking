Data analysis and retrieval is a widely-used component in existing artificial intelligence systems.
However, each request has to go through each layer across the I/O stack, which moves tremendous irrelevant data between secondary storage, DRAM, and the on-chip cache.
This leads to high response latency and rising energy consumption.
To address this issue , we propose Cognitive SSD, an energy-efficient engine for deep learning based unstructured data retrieval.
In Cog-nitive SSD, a flash-accessing accelerator named DLG-x is placed by the side of flash memory to achieve near-data deep learning and graph search.
Such functions of in-SSD deep learning and graph search are exposed to the users as library APIs via NVMe command extension.
Experimental results on the FPGA-based prototype reveal that the proposed Cognitive SSD reduces latency by 69.9% on average in comparison with CPU based solutions on conventional SSDs, and it reduces the overall system power consumption by up to 34.4% and 63.0% respectively when compared to CPU and GPU based solutions that deliver comparable performance.
Unstructured data, especially unlabeled videos and images, etc., have grown explosively in recent years.
It is reported that the unstructured data occupies up to 80% of storage capacity in commercial datacenters [10].
Once being stored and managed in the cloud machines, the massive amount of unstructured data leads to intensive retrieval requests issued by users, which pose significant challenge to the processing throughput and power consumption of a datacenter [19].
Consequently, it is critical to support fast and energy-efficient data retrieval in the cloud service infrastructure to reduce the total cost of ownership (TCO) of datacenters.Unfortunately, conventional content-based multimedia data retrieval systems suffer from the issues of inaccuracy, power inefficiency, and high cost especially for large-scale unstructured data.
data retrieval system composed of CPU/GPU and conventional storage devices based on a compute-centric architecture [14].
When a data retrieval request arrives from the internet or the central server, the CPU has to reload massive potential data from disk into the temporary DRAM [14] and match the features of the query with those of the loaded unstructured data to find the relevant targets.
This compute-centric architecture is confronted with several critical sources of overhead and inefficiency.
(1) The current I/O software stack significantly burdens the data retrieval system when it simply fetches data from the storage devices on retrieval requests [60], as shown in Fig. 1(a).
The situation is even worse since it is reported the performance bottleneck has migrated from hardware (75∼50us [11]) to software (60.8us [48]) as traditional HDDs are replaced by non-volatile memory [41,48].
(2) Massive data movement incurs energy and latency overhead in the conventional memory hierarchy.
This issue becomes severe as the scale of data under query increases because the relevant data at the low-level storage must travel across a slow I/O interface (e.g., SATA), main memory and multi-level caches before reaching the compute units of CPU or GPUs [24], which is depicted in Fig. 1(a).
To address these issues, as shown in Fig. 1(b), this work aims to tailor a unified data storing and retrieval system within the compact storage device, and eliminate the major IO and data moving bottleneck.
In this system, retrieval requests are directly sent to the storage devices, and the target data analysis and indexing are completely performed where the unstruc-tured data resides.
Building such a data retrieval system based on the proposed Cognitive SSD bears the following design goals: (1) providing a high accuracy, low latency, and energy efficient query mechanism affordable in a compact SSD, (2) exploiting the internal bandwidth of flash devices in an SSD for energy-efficient deep learning based data processing, and (3) enabling developers to customize the data retrieval system for different dataset.
These points are stated in detail below.First, instead of relying on the general-purpose CPU or GPU devices in Fig. 1(a), we must have a highly computationefficient yet accurate data retrieval architecture in consideration of the SSD form factor and cost.
A conventional data retrieval framework is inaccurate or too computationally expensive to be implemented within a resource-constrained SSD.
In this work, we are the first to propose a holistic data retrieval mechanism by combining the deep learning and graph search algorithm (DLG), where the former could extract the semantic features of unstructured data and the latter could improve database search efficiency.
The DLG solution achieves much higher data retrieval accuracy and enables user-definable computation complexity through deep learning model customization, making it possible to implement a flexible and efficient end-to-end unstructured data retrieval system in the SSD.Second, although DLG is a simple and flexible end-to-end data retrieval solution, embedding it into SSDs still takes considerable effort.
We designed a specific hardware accelerator that supports deep hashing and graph search simultaneously, DLG-x, to construct the target Cognitive SSD without using power-unsustainable CPU or GPU solutions.
However, the limited DRAM inside an SSD is mostly used to cache the metadata for flash management, leaving no free space for the deep learning applications.
Fortunately, we have proved that the bandwidth of internal flash interface surpasses that of external IO interface in a typical SSD, which matches the bandwidth demand of the DLG-x with proper data layout mapping.
By rebuilding the data path in the SSD and deliberately optimizing the data-layout related to deep learning models and graphs on NAND flash, the DLG-x could fully exploit internal parallelism and directly access data from NAND flash bypassing the on-board DRAM.Finally, as we introduce deep learning technology into the SSD, we must expose the software abstraction of Cognitive SSD to users and developers to process different data structures with different deep learning models.
Thus, we abstract the underlying deep learning mechanism, feature analysis, and data structure indexing mechanism as user-visible calls by utilizing the NVMe protocol [6] for command extension.
Not only can users' requests trigger the DLG-x accelerator to search the target dataset for query-relevant structures, but also system developers can freely configure the deep hashing architecture with different representation power and overhead for different dataset and performance requirement.
In contrast to conventional ad-hoc solutions, Cognitive SSD allows system developers to adjust the retrieval accuracy as well as the real-time performance of the data retrieval service through provided APIs.
Meanwhile, Cognitive SSD also supports the flexible combination of special commands to achieve different data retrieval related tasks, like in-storage data categorization and hashing-only functions.
In summary, we make the following novel contributions:1 We propose Cognitive SSD, to enable within-SSD deep learning and graph search by integrating a specialized deep learning and graph search accelerator (DLG-x).
The DLGx directly accesses data from NAND flash without crossing multiple memory hierarchies to decrease data movement path and power consumption.
To the best of our knowledge, this work is the first to combine the deep learning and graph search methods for fast and accurate data retrieval in SSD.
2 We employ Cognitive SSD to build a serverless data retrieval system, which completely abandons the conventional data query mechanism in orthodox compute-centric systems.
It can independently respond to data retrieval requests at real-time speed and low energy cost.
It can also scale to a multi-SSD system and significantly reduces the hardware and power overhead of large-scale storage nodes in data centers.
3 We build a prototype of Cognitive SSD on the Cosmos plus OpenSSD platform [7] and use it to implement a data retrieval system.
Our evaluation results demonstrate that Cognitive SSD is more energy-efficient than a multimedia retrieval system implemented on CPU and GPU, and reduces latency by 69.9% on average compared to the implementation with CPU.
We also show that it outperforms conventional computing and storage node used in the data center when Cognitive SSD scales out to form smart lightweight storage nodes that include connected Cognitive SSD array.
Content-based unstructured data retrieval systems aim to search for certain data entries from the large-scale dataset by analyzing their visual or audio content.
Fig. 2 depicts a typical content-based retrieval procedure consists of two main stages: feature extraction, and database indexing.
Feature extraction generates the feature vector for the query data, and database indexing searches for similar data structures in storage with that feature vector encoded in a semantic space.
Feature Extraction and Deep Learning.
The rise of deep learning transfers the focus of researches to deep convolution neural network (DCNN) [38] based features [61], as it provides better mid-level representations [37,40].
Fig. 2 nonlinear transformation that we do over the input signal, (3) pooling layers, which down-sample the input channels for scale and other types of invariance, and (4) fully connected (FC) layer, which performs linear operations between the features and the learned weights to predict the categorization or other high level characteristics of input data.
Such a neural network is flexible and can be designed to have different hyper-parameters, like the number of the convolution and pooling layers stacked together and the dimension and number of convolution filters.
Changing these parameters will impact the generalization ability and also computational overhead of neural networks, which are usually customizable for different dataset or application scenarios [47].
Some prior work directly employs the high-dimension output vector of the FC layer for data retrieval and is thought too expensive in terms of memory footprint and computation complexity [39].
Thus, we adopt deep hashing [38] to achieve effective yet condensed data representation.
Fig. 2 exemplifies a deep hashing architecture, Hash-AlexNet, where a hash layer follows the last layer of AlexNet [35] to project the data feature learned from AlexNet into the hash space, and the generated hash code can be directly used to index the relevant data structures and get rid of the complex data preprocessing stage.Database indexing: Graph-based approximate nearest neighbor search(ANNS) methods named NSG [27], a complement to deep hashing, achieves both accurate and fast data retrieval results, as was proved in previous work [17,26].
The main idea of NSG is mapping the query hash code into a graph.
The vertex of the graph represents an instance, and the edge stands for the similarities between entities, where the value of the edge represents the strength of similarity.
On top of that, NSG can iteratively check neighbors' neighbors in the graph to find the true neighbors of the query based on the neighbor of a neighbor is also likely to be a neighbor concept.
In this manner, the NSG could avoid unnecessary data checking to reduce retrieval latency.
In summary, deep hashing followed by graph search can perform low-latency and high-precision retrieval performance compared to traditional solutions using brute-force search or hash algorithms.
Meanwhile, it also makes the retrieval framework more compact and efficient because of the similar compute patterns and data stream, so that they can fit into compact and power-limited SSDs.
For hardware-software co-design, there are two directions in SSD research: open-channel SSD and near-data processing (NDP).
While open-channel SSD enables direct flash memory access via system software [15,42,44,59], near-data processing (NDP) moves computation from the system's main processors into memory or storage devices [16,18,25,46,50,51,56].
In NDP, Morpheus [52] provides a framework for moving computation to the general-purpose embedded processors on NVMe SSD.
FAWN [13] uses low-power processors and flash to handle data processing and focuses on a key-value storage system.
SmartSSD [34] introduces the Smart SSD model, which pairs in-device processing with a powerful host system capable of handling data-oriented tasks without modifying the operating system.
[53] supports fundamental database operations including sort, scan, and list intersection by utilizing Samsung SmartSSD.
[23] investigates by simulation the possibility of employing the embedded ARM processor in SSDs to run SGDs, which is a key components of neural network training.
However, none of them can handle deep learning processing due to the performance limit of the embedded processor.
Thereby, [22] presents intelligent solidstate drives (iSSDs) that embed stream processors into the flash memory controllers to handle linear regression and kmeans workloads.
[43] integrates programmable logics into SSDs to achieve energy-efficiency computation for web-scale data analysis.
Meanwhile, [32] also uses FPGAs to construct BlueDBM that uses flash storage and in-store processing for cost-effective analytics of large datasets, such as graph traversal and string search.
GraFBoost [33] focuses on the acceleration of graph algorithms on an in-flash computing platform instead of deep learning algorithms as this work.
Cognitive SSD Prior active disks are integrating either general purpose processors incapable of handling highthroughput data or specialized accelerators with only the support of simple functions like scanning and sorting.
These in-disk computation engines are unable to fulfill the requirement of high-throughput deep neural network (DNN) inference because computation-intensive DNNs generally rely on power-consuming CPU or GPUs in the case of data analysis and query tasks.
To enable energy-efficient DNN, prior work proposes a variety of energy-efficient deep learning accelerators.
For example, Diannao and C-Brain map large DNNs onto a vectorized processing array and employ a data tiling policy to exploit locality in neural parameters [20,49].
Eyeriss applies the classic systolic array architecture to the inference of CNN, and outperforms CPU and GPU in energy efficiency dramatically [21].
However, these researches focus on optimizing the internal structure of accelerator and relied on large-capacity SRAM or DRAM instead of external nonvolatile memory.
In contrast to these works and prior active SSD designs, we propose Cognitive SSD, the first work that enables the storage device to employ deep learning to conduct in-storage data query and analysis.
It is designed to replace the conventional data retrieval system and contains a flashaccessing accelerator (DLG-x) for deep learning and graph search.
The DLG-x is deliberately reshaped to take advantage of the large flash capacity and high internal bandwidth, and it is also re-architected to enable graph search to target indexing.
Target Workload.
As shown in Fig. 2, this work combines the strengths of deep hashing and graph search technique (DLG) to reduce the complexity of retrieval systems on the premise of high accuracy, which makes it possible to offload retrieval task from CPU/GPU into the resource-constraint Cognitive SSD.
Based on that, we build an end-to-end data retrieval system that supports multimedia data retrieval such as audio, video and text.
For example, audio can be processed by recurrent or convolutional neural network models on Cognitive SSD to generate hash codes, which act as an index for retrieving relevant audio data inside the SSD.
In this paper, image retrieval is used as a showcase.
As shown in Fig. 3, Cognitive SSD is designed to support the major components in the framework of DLG, allowing developers to customize and implement data retrieval solutions.
Such a near-data retrieval system consists of two major components: the DLG library running in lightweight server that manages user requests, and the Cognitive SSD is plugged into the host server via the PCIe interface.
As shown in Table 1, as the interface of Cognitive SSD system, the DLG library is established by leveraging the Vendor Specific Commands in the I/O command set of the NVMe protocol.
It contains a configuration library and a user library.
The configuration library enables the administrator to choose and deploy different deep learning models on the Cognitive SSD quickly according to application demand.
After the feature-extracting deep hashing model has been deployed on the Cognitive SSD, a data processing request arriving at the host server could send and establish a query session to it by invoking the APIs provided by the user library.
Then, the runtime system on the embedded processor of the Cognitive SSD receives and parses the request to activate the corresponding DLG-x module, which is associated with the user-created session.
Next, we elaborate on the software and hardware design details of Cognitive SSD.
Update Deep Learning Models: Because the choice of deep learning models significantly impacts the data retrieval system performance, the system administrator must be able to customize a specific deep hashing model according to the complexity and volume of database, and the quality of service measured by response latency or request throughput.
Thereby, the configuration library provides a DLG-x compiler compatible with popular deep learning frameworks (i.e., Caffe) to allow the administrator to train the new deep learning model and generate corresponding DLG-x instructions offline.
Then, the administrator can update the learning model running on the Cognitive SSD by updating the DLG-x instructions.
The updated instructions are sent to the instruction area allocated in the NAND flash and stay there until a model change command (DLG_config in Fig. 3) is issued.
Meanwhile, the DLGx compiler also reorganizes the data layout of the DLG algorithm to fully utilize the internal flash bandwidth according to the structures of neural network model and graph, before the parameters of deep learning model and graphs are written to the NAND flash.
The physical address of weight and graph structure information is recorded in the DLG-x instruction.
In this manner, the DLG-x obtains the physical address of required data directly at runtime, instead of adopting the address translation or look-up operations that incur additional overhead.
More details about the data reorganization scheme are introduced in § 4.
Data Plane: The data plane provides SSD_read and SSD_write APIs for users to control data transmission between the host server and the Cognitive SSD.
These two commands operate directly on the physical address bypassing the flash translation layer.
Users can invoke these APIs to inject data sent from users to the data cache region or the NAND flash on the Cognitive SSD based on the parameter of data address and data size.
Afterwards, users can use those addresses to direct the operands in other APIs.
Task Plane: To improve the scalability of the DLG-x accelerator that supports deep hashing neural networks and graph search algorithms, we abstract the function of the DLG-x into three APIs in the task plane of user library: DLG_hashing, DLG_index, and DLG_analysis.
These APIs are established using the C0h, C1h, and C2h commands of NVMe I/O protocol, respectively.
All of them possess two basic parameters carried by NVMe protocol DWords: the data address indicating the data location in Cognitive SSD, and the data size in bytes.First, the DLG_hashing API is designed to extract the condensed feature of input data and map it into the hash or seman- tic space, which is fundamental in a data retrieval system and useful for other analysis functions like image classification or categorization.
This command contains an extended parameter: hashcode length, which determines the capacity of the carried information.
For example, compared to the database with 500 objects types, the database with 1000 objects needs a longer hash code to avoid information loss.
Second, the DLG_index API is abstracted from the graph search function of the DLG-x.
It also includes an extended parameters: T, represents the number of search results configured by users based on the applications scenarios.
Finally, the DLG_analysis API allows users to analyze the input data using the data analysis and processing ability of deep neural networks and it also possesses a reserved field for user-defined functions.
These task APIs are the abstraction of the key near-data processing kernels provided by Cognitive SSD, and they can be invoked independently or combinedly to develop different in-SSD data processing functions.
For instance, users could combine the DLG_hashing and DLG_index APIs to accomplish data retrieval on a large-scale database, where DLG_hashing maps the features of query data to a hash code and DLG_index uses it to search for the top-T similar instances.
Note that the DLG-x accelerator occupies a noticeable portion of the flash bandwidth once activated, which perhaps degrades the performance of normal I/O requests.
To alleviate this problem, instead of letting the task or I/O scheduler wait until the request is completed (denoted as Method A), the DLG task scheduler receives the NVMe command sent from host with doorbell mechanism and actively polls the completion status of the DLG-x periodically (denoted as Method B) to decide if the next request is dispatchable.
We tested the normal read/write bandwidth of Cognitive SSD prototype described in § 5.1 with the Flexible IO Tester (fio) benchmark [4], under the worst-case influence where the DLG-x accelerator operations occupied all the Cognitive SSD channels.
Experiments (Table 2) demonstrate that adopting Method B only causes a drop of 27%-44% in the normal I/O bandwidth whilst using Method A decreases almost 91% of the read/write bandwidth averagely when the DLG-x accelerator is busy dealing with the over-committed retrieval tasks.
More importantly, though SSDs often have compact DRAM to cache data or metadata, the internal DRAM capacity can hardly satisfy the demand of the deep learning, which is notorious for its numerous neural network parameters.
Worse still, the basic firmware like FTL and other components also occupy major memory resources.
Therefore, the NAND flash controller is exposed to the DLG-x accelerator, which enables the DLGx to read and write the related working data directly from NAND flash, bypassing the internal DRAM.
3.3 The Procedure of data retrieval in Cognitive SSD model have been generated and written to the corresponding region by leveraging the DLG-x compiler and the DLG_config command shown in Fig. 3.
The Hash-AlexNet is the developer designated neural network for feature extraction of input data.
Then, when the host DLG library captures a retrieval request, it packages and writes the user input data from the designated host memory space to Cognitive SSD through the SSD_write API.
Meanwhile, the DLG_hashing command carrying the address of input data is sent to Cognitive SSD for hash code generation.
Receiving the command, the request scheduler of the cognitive runtime parses it and notifies the DLG-x accelerator to start a hashing feature extraction session.
Then, the DLG-x automatically fetches input query data from the command-specified data address and then loads deep learning parameters from NAND flash.
Meanwhile, the other command, DLG_index, is sent and queued by the task scheduler.
After the hash code is produced, the DLG_index is dispatched to invoke the graph search function in the DLG-x and uses the hash result to search the data graphs for relevant data entries.
In this case, the DLG-x keeps fetching graph data from the NAND flash and sends the final retrieval results to the host memory once the task is finished.
In contrast to a traditional hardware accelerator [20], the DLGx accelerator is designed to directly obtain the majority of the working-set data from NAND flash.
Fig. 4 illustrates the high-level diagram.
The DLG-x accelerator has two activation buffers (InOut Buffer) and double-banked weight buffers.
The intermediate results of each neural network layers are temporarily stored in the activation buffers, while the weight buffers act as a bridge buffer between the Neural Processing Engine (NPE) and the flash, which stream out the large quantity of neural parameters to the NPE.
The NPE comprises a set of processing engines (PEs), which can perform fixed-point convolutions, pooling, and activation function operations.
The Graph Search Engine (GSE) cooperates with the NPE and is responsible for graph search with the hash code generated by NPE.
Both the NPE and GSE are managed by the control unit that fetches instructions from memory.
Considering the I/O operation granularity of NAND flash, we reorganize the data layout of neural networks including both the static parameters and the intermediate feature data, to exploit the high internal flash bandwidth.
L compute = OP compute OP PE = 2 × K c × K h × K w × O h × O w 2 × N PE(2)L data = S param BW m f lash = K c × K h × K w BW m f lash(3)Where the OP compute and S param is the operation number and the parameters volume of a convolutional layer.
OP PE gauges the performance of the DLG-x measured in operations/cycle.
To avoid NPE stalls,we must have L compute >= L data , and O w is usually equal to O h , so we haveO w >= N PE /BW m f lash(4)The above equation indicates that if only the width and height of the output feature map is larger than or equal to the right side of formula 4, which is four in our prototype with N PE = 256 and BW m f lash = 16bytes/cycle, the NPE will not stall.
For example, in the Hash-AlexNet mentioned in § 2.1, the minimum width of the output feature map in convolution layers is 7, which already satisfies in inequality 4 design.
However, in the FC layers, L compute is smaller than L data , so the data transfer time becomes the bottleneck.
Thereby, the DLG-x accelerator only uses a column of PEs to deal with a FC layer because our prototype hardware design only supports eight channels, which does not meet inequality 4 with M = 128 and consequently causes PE underutilization.
Besides, the parameter-induced flash reads will be minimized if the size of the weight buffer meets the condition: S bu f f er >= Max(K c × K h × K w ).
The parameters exceeding the size of weight buffer will be repetitively fetched from the flash.
To further improve the performance, we utilize pingpong weight buffers to overlap the data loading latency with computation.
Data Layout in flash devices: Owing to the bandwidth analysis on the base of multi-channels data transmission, we propose flash-aware data layout to fully exploit flash bandwidth with the advanced NAND flash command-read page cache command [11].
The read page cache sequential command provided by NAND flash manufacturer can continuously load the next page within a block into the data register () while the previous page is being read to the buffer of the DLG-x or the cache region of the SSD from the cache register().
Thus, based on the NAND flash architecture with the provided page cache command, we choose to split the convolution kernels and store them into flash devices for parallel fetch.
As shown in Fig. 5, assuming there are N k convolution kernels with S k kernel size, and M NAND flash channels are used by the DLG-x accelerator, each convolution kernel is divided into S K /M sub-blocks and all such sub-blocks are interleaved to the flash channels.
The convolution kernels exceeding the size of a page are placed into continuous address space in the NAND flash because the cache command reads out the next page automatically without any extra address or operation.
Data Flow: Taking the Hash-AlexNet as an example, when a request arrives at the DLG-x accelerator, the input data and the first kernel of the first convolution layer is transferred in parallel to the InOut buffer-0 and the weight buffer-0.
After that, the DLG-x accelerator begins to compute the output feature map and stores them into the InOut buffer-1.
When the first kernel is processed, the second kernel is being transferred from the NAND flash to weight buffer-1, then followed by the third and fourth kernel in sequence.
Once the hash code is generated, it is sent to the graph search registers of NPE to locate the data structures similar to the query data if the DLG task scheduler decodes and dispatches a following DLG_index command.
For fast and accurate database indexing, the DLG-x accelerator fuses the deep learning and graph search mechanism into unified hardware, and reuses the computation and memory resources for higher efficiency.
Once the hash code of the query data has been generated, the DLG-x uses it to initially index the corresponding data graphs and searches for the closest data entries from graphs.The graph search method originates from Navigating Spreading-out Graph [27] (NSG), which well fits the limited memory space of the Cognitive SSD for the large-scale multimedia data retrieval.
The NSG algorithm includes an offline stage and an online stage.
In the offline phase, the NSG method constructs a directed K nbors − NN graph for the storage data structures to be retrieved.
In a graph, a vertex represents a data entry by keeping its ID and hash code.
The unique ID represents a file and the hash code is the feature vector of this file, which could be obtained by invoking the DLG_hashing API in advance.
The bit-width of ID (W id ) and hash code (W hash_code ) are user-configurable parameters in the API.
In a graph, a vertex may be connected to many vertices, which have different distances from each other.
However, only the top-K nbors closest vertices of a vertex could be defined as its "neighbors", where K nbors is also a reconfigurable parameter and enables users to pursue the trade-off between accuracy and retrieval speed.
The DLG-x accelerator only accelerates the online retrieval stage and the database update occurs offline because the latter task is infrequent.
The database update consists of hash code extraction stage and K nbors − NN graph construction stage, where the former is accelerated by the DLG-x accelerator and the latter is completed with the DLG library on CPUs.
At offline graph construction, it takes about 10∼100 seconds to update the K nbors − NN graph on millionscale data on a server CPU.
The hardware architecture for online graph search is presented in Fig. 4.
The graph search function of the DLG-x starts from evaluating the distance of random initial vertices in the graph and walks the whole graph from vertex to vertex in the neighborhood to find the closest results.
As shown in Fig. 4, to maximize the utilization of on-chip memory, the weight buffer and InOut buffer are reused to store the neighbors of vertices and the search results of the graph search engine respectively.
Since the Hamming distance (H-distance) is an integer value, the InOut buffer is divided into blocks according to the range of H-distance.
For instance, the first block of the InOut buffer B 0 only stores the vertices with zero Hamming distance away from the query vertex, and the second block B 1 corresponds to the distance of one hop.
The last area B others stores the vertices with Hamming distance larger than the final value V f inal , where the V f inal is a re-definable parameter and calculated with formula 5.
V f inal = S bu f f er D block ×W id − 1(5)In the above equation, S bu f f er is the on-chip buffer size of the DLG-x accelerator and D block represents the number of vertex IDs that can be stored in each block.
W id is usually equal to 32bits.
For instance, in our design, with S bu f f er = 256KB and D block = 5000, it is easy to have V f inal = 12.
Note that the limited size of the region B others cannot hold all the distant data vertices generated at runtime, and thus B others is configured to a ring buffer to accommodate the incoming vertices cyclically.
A Vertex Detector Unit (VDU) is inserted to check whether the selected vertex has been evaluated.
In VDU, the vertex will be discarded once found to have been walked before, otherwise it will be sent to the NPE unit to compute the Hamming distance from the query vertex.
With the distance provided by the NPE, the Buffer Address Generator (BAG) module allocates memory space in the InOut buffer for the vertex and then puts the vertex into the assigned areas of the InOut buffer.
The unevaluated vertices will be fetched from the InOut buffer and the neighbors of these vertices are loaded from the weight buffer by the control unit.
Meanwhile, the control unit will finally return the top-T closest vertices when the number of vertex in the InOut buffer reaches the threshold configured by users, where T is also configured by users via the DLG_index API.
Data Layout for fast In-SSD NSG search: NAND flash read operations are performed at a page granularity (16KB), so that every time the DLG-x accesses the neighborhoods of one vertex (250bytes), it must read one whole page from flash, which perhaps causes low bandwidth utilization if locality is not well preserved.
In our design that K nbors = 25, W id = 32bits and W hash_code = 48bits.
Inspired by the intuition that the neighbor of a neighbor is also likely to be a neighbor of the query data in the graph, we can infer that the neighbors of the accessed vertex will be used soon due to the spatial locality.
Therefore, as shown in Fig. 6, V 0 and all its neighbors (V 1 ,V 2 , ...,V 25 ), are continuously aligned and stored from the beginning of a page.
As a result, such a layout with redundancy is able to reduce flash access by 37x compared to a non-optimized graph layout.
However, such a layout cause duplicates of vertices in storage and sacrifice additional storage space for better data access performance, which is worthwhile regarding the large capacity of SSDs.
Besides data layout transformation, the bit-width of the hash code is also worth elaborating.
Due to the limited page size S page , W hash_code and K nbors must conform to the resource constraint given by:K 2 nbors × (W hash_code +W id ) < S page(6)Generally S page = 16KB, and W id is 32-bit wide and can represent 2 32 files.
Because the parameters W hash_code and K nbors directly impact the deep hashing performance by influencing the indexing accuracy and also the memory bandwidth consumption during graph search, once S page is determined, W hash_code and K nbors must be adjusted to reach a perfect balance between accuracy and retrieval time at the offline stage.
Thus, the DLG-x must support different parameter formats in order to achieve best-effort computing efficiency for databases of different volume and complexity.
Note that our graph layout and the according searching strategy are adapted to the underlying hardware for higher energy efficiency, and they will lead to a marginal amount of query accuracy losses compared to the original algorithm.
Table 3 indicates the accuracy loss compared with the original lossless DLG algorithm (denoted as Original) on the CIFAR-10 [9] and ImageNet [45] datasets.
The result shows that when T=1000, the accuracy drops by 2.26% and 4.36%, as a sideeffect of the ∼ 37x performance boost.
Fortunately, the DLG library APIs are flexible enough to allow the developers to trade-off between accuracy and performance by manipulating the API arguments.
Data Flow: we show an example to brief the overall flow of the DLG-x based data retrieval.
Firstly, when a query comes, the DLG-x fetches the input data and parameters of the deep learning model from the NAND flash into the InOut buffer and the weight buffer of the DLG-x respectively.
Then, the NPE unit generates the hash code for the input data and writes it to the graph search registers of the NPE unit.
After that, the DLG-x transfers the K nbors − NN graph from the NAND flash array to the weight buffer.
At the first stage, the initial vertices are calculated and sent into the corresponding areas of the InOut buffer.
At the second stage, the DLG-x control unit reads the first unevaluated vertex from the InOut buffer in ascending order of Hamming distance.
Then, the graph search engine obtains the neighbors of the unevaluated vertex from the NAND flash and transfers the neighbors to NPE to generate their Hamming distances from the query vertex as well.
Next, the Buffer Address Generator unit generates the write addresses for these neighbor vertices in the InOut buffers according to the calculated Hamming distance and writes these vertices to the InOut buffers.
Meanwhile, the counter in the graph search engine determines whether the termination signal should be issued by monitoring the total number of vertices stored in the InOut buffer.
Once the termination signal is generated, the Cognitive SSD runtime reads out the vertices from the InOut buffer and then transfers the ID-directed results stored in NAND flash to the host server via the PCIe interface.
To explore the advantages of the Cognitive SSD system, we implemented it on the Cosmos plus OpenSSD platform [7].
The Cosmos plus OpenSSD platform consists of an XC7Z045 FPGA chip, 1GB DRAM, an 8-way NAND flash interface, an Ethernet interface, and a PCIe Gen2 8-lane interface.
A DLG-x accelerator is designed with DeepBurning [55] and integrated to the modified NAND flash controllers, and they are all implemented on the programmable logic of XC7Z045.
The Cognitive SSD runs its firmware on a Dual 1GHz ARM Cortex-A9 core of XC7Z045.
The Cognitive SSD is plugged into the host server via a PCIe link.
The host server manages the high-level requests and maintains the DLG library for API calls.
Fig. 7 shows the Cognitive SSD prototype constructed for this work.
We first selected the content-based image retrieval system (CBIR) based on deep hashing and graph search (DLG) algorithm as workload and evaluated the performance of DLG solution compared to other conventional solution ( §5.3).
we evaluated the DLG-x of the Cognitive SSD prototype in §5.4, and deployed the Cognitive SSD prototype to a single node and multi-node system, and evaluated them in §5.5, and §5.6, respectively.
Except for the Cognitive SSD prototype, our experimental setup also consists of a baseline server running Ubuntu 14.04 with two Intel Xeon E5-2630 CPU@2.20GHz, 32GB DRAM memory, four 1TB PCIe SSDs and an NVIDIA GTX 1080Ti.
Meanwhile, we implemented the CBIR system in C++ on the baseline server, where the deep hashing is built on top of Caffe [31].
Based on this platform, we constructed four solutions baselines: B-CPU, B-GPU, B-FPGA, and B-DLG-x.
For B-CPU, the DLG algorithm runs on the CPU.
For B-GPU, the deep hashing runs on the GPU and graph search runs on the CPU.
For B-FPGA, we use ZC706 FPGA board [12] to replace Cognitive SSD, and the deep hashing runs on ZC706 FPGA board and graph search runs on the CPU.
B-DLG-x implements the DLG algorithm on ZC706 FPGA board without any near-data processing technique compared to Cognitive SSD.
Experimental Setup.
We used the precision at top T returned samples (Precision@T), measuring the proportionality of corrected retrieved data entries, to verify the performance of our deep hashing method on different models and datasets [36].
The performance is contrasted with traditional hash methods with 512-dimensional GIST feature, including Locality-Sensitive Hashing (LSH) [54] and Iterative Quantization (ITQ) [28].
The used datasets are listed in Table 4.
Evaluation.
Fig. 8(a)-(d) shows the Precision@T on different datasets with different deep hashing models.
Due to the poor performance of LSH and ITQ [28] methods on ImageNet, we added the AlexNet-ITQ (A+ITQ) and AlexNet-CCA-ITQ (AC+ITQ) methods [58] that uses the output of the FC layer on Alexnet as the feature vector for search.
Our DLG solution performs better than the other approaches on different datasets with different scales regardless of the choice of T value, especially compared to the conventional hash method.
It also shows the robustness of the DLG solution when deploying on a real-world system.
Meanwhile, Fig. 8(d) shows the performance of our approach is significantly improved when the code length increases to 64 bits.
Thereby, the DLG-x accelerator is configured to support different hash code length to achieve the trade-off between retrieval accuracy and latency.
Experimental Setup.
We implemented the deep hashing and graph search algorithm (DLG) on the DLG-x accelerator of Cognitive SSD and compared the latency and power of the DLG-x to the solutions based on CPU and GPU, where we ignore the FPGA baseline because its computational units are the same as the DLG-x.
Firstly, we only compared the latency and power of the deep hashing unit on the DLG-x running various deep hashing models to CPU and GPU, where GPU only reports the total power consumed by NVIDIA GTX 1080Ti without the power of the CPU.
Secondly, we only evaluated the latency of graph search function on the DLGx with respect to different number (T) of top retrieved data entries on the CIFAR-10 and ImageNet dataset.
Performance.
Firstly, Table 5 shows the latency of the DLGx on various deep hashing schemes outperforms the solution based on CPU.
While the latency of the DLG-x is higher than GPU because of the hardware resource and frequency limitation, it consumes less power compared to GPU.
More importantly, the latency of the CPU and GPU on Table 5 only contains the computation delay of neural network without the delay of parameters transmission between storage and memory.
When considering the delay of parameters transmission between storage and memory, the deep hashing occupies about 87.7% and 73.9% of the total processing time on the DLG-x accelerator and CPU baseline, on average, respectively.
And the GPU only accounts for 3.5% of the total runtime on average because the high-speed data processing capability of GPU makes the data transmission becomes the bottleneck of system.
Besides, the latency of deep hashing on the GPU occupies 54.17∼26.06% without considering the delay of data movement because the latency of graph search increases with the increase of T value.
Secondly, in this experiment, we utilized the Hash-AlexNet model to generate the hash code database for the construction of a K nbors − NN graph on the CIFAR-10 and ImageNet dataset.
We compared the retrieval speed of the DLG-x accelerator with two counterparts: the brute-force search method that evaluates all the hash codes stored in the database, and the CPU executed graph search algorithm.
The result is depicted in Fig. 9.
For the CIFAR-10 dataset, the DLG-x accelerator is 111.51-90.
22x and 37.12-36.73x faster than the brute-force method and the CPU-run graph search algorithms respectively, while for the ImageNet dataset it achieves a 5334.4-498.5x and 12.5-3.4x speed up over the latter two baselines.
As introduced in § 4.3, the retrieval accuracy is only degraded by 2.26% and 4.36% when T = 1000 on the CIFAR-10 and ImageNet datasets, respectively.
Power Consumption.
We measured and compared the power consumption of Cognitive SSD system with four baselines: B-CPU, B-GPU, B-FPGA, and B-DLG-x by using a power meter under two different situations: (1) IDLE: No retrieval requests need to respond, and (2) ACTIVE: A user continuously accesses the Cognitive SSD system.
The result is illustrated in Table 6.
When the Cognitive SSD+CPU system is IDLE, its power consumption is slightly higher than B-CPU and B-GPU because the Cognitive SSD prototype board IDLE power is higher than that of the PCIe SSD and GPU.
For active power, when delivering comparable data retrieval performance, the Cognitive SSD system reduces the total power consumption by up to 34.4% and 63.0% compared with B-CPU and B-GPU.
Simply replacing the GPU with the FPGA board reduces power consumption by 40.7%.
Furthermore, putting the DLG-x on an identical FPGA board without the NDP decrease power consumption by 43.72%, which is attributed to the efficiency of hardware specialization.
Placing the DLG-x into the Cognitive SSD system further eliminates power consumption by another 19.3%, which is the benefit of near-data processing.
In the case of the Cognitive SSD+CPU solution, the power of CPU is low because it is only responsible for instruction dispatch without any data transfer between storage and CPU.
In other cases, the CPU is not only in charge of data transfer management but also for instruction dispatch or executing the graph search algorithm.
FPGA Resource Utilization.
The placement and routing were completed with Vivado 2016.2 [8].
Table 7 We implemented the CBIR system by using the ImageNet dataset on the Cognitive SSD with a baseline server, where the baseline server is only responsible for receiving and sending retrieval requests to Cognitive SSD.
The deep hashing architecture is a Hash-AlexNet network and the hash code length is 48 bits.
As shown in Fig. 10, we built a web-accessible CBIR system based on web framework CROW [30] to evaluate the latency and query per second (QPS) of the system by simulating the user requests sent to the URL address via ApacheBench (ab) [1].
The latency measurement indicates the time between issuing a request and the arrival of the result.
The QPS is a scalability measuring metric characterizing the throughput of the system.
The latency and QPS are affected by the software algorithm and the hardware performance of the system.
Meanwhile, we utilized the metric of QPS per watt (QPS/Watt) to evaluate the energy-efficiency of the system.
Evaluation.
We evaluated the performance of the Cognitive SSD system and four baselines under the assumption that data (weight/graph) cannot be accommodated in DRAM and must travel across the SSD cache, I/O interface, and DRAM before reaching a compute unit.
The performance of the Cognitive SSD system and four baselines are shown in Fig. 11.
With the increased number of top images retrieved, the retrieval time spent on the DLG-x accelerator will rise.
It leads to increased retrieval latency and decreased QPS for the Cognitive SSD.
Meanwhile, we also observe the 95% requests complete in time in experiments when write operations and garbage collection are inactive on the Cognitive SSD.
Note that write operations and garbage collection are rare for the Cognitive SSD compared to read operations and usually occur offline.
The workloads on the Cognitive SSD are readonly, which sustains the latency of the Cognitive SSD at a steady level with little fluctuation.
Besides, in comparison to the B-CPU, the Cognitive SSD reduces latency by 69.9% on average.
The performance improvement stems from the high-speed of data processing on the DLG-x accelerator compared to B-CPU.
Due to the overhead of data movement caused by the bandwidth limitation of the I/O interface and onboard memory, the latency of B-FPGA and B-DLG-x is higher than B-GPU.
Compared to the B-FPGA and B-DLG-x baselines, the Cognitive SSD reduces latency by 63.79% and 63.02% on average, which benefits from near-data processing.
The average retrieval speed of B-GPU is 1.11x faster than Cognitive SSD because the execution of deep hashing costs more time on the resource-limited DLG-x compared to powerful GPUs.
However, Cognitive SSD is more energy-efficient (QPS/Watt) than a GPU-integrated system by 2.44x, which is shown in Fig. 11(b).
More importantly, the Cognitive SSD is implemented with FPGA and the operating frequency is only 100MHz.
The performance will be better if the Cognitive SSD is implemented with ASIC or escalated operating frequency.
Experimental Setup.
We evaluated the performance and the scalability of the Cognitive SSD when it scales into a multinode cluster system.
Fig. 12(a) shows the architecture of a conventional low-cost small-scale cluster system in a data warehouse.
The cluster system consists of 10 worker servers and 1 Nginx [5] server.
The Nginx server is responsible for load balancing.
The worker nodes connect to the Nginx server by using TCP connections.
In this case, we constructed four cluster system baselines by extending above four baselines: BC-CPU, BC-GPU, BC-FPGA, and BC-DLG-x.
We issued requests to measure the QPS and the latency per request of the Cognitive SSD based cluster system when multiple users are accessing the web service shown in Fig. 10 concurrently via the ab tool.
Evaluation.
Fig. 13(a) illustrates that when concurrent users equal to 400, all evaluated schemes rise slowly in experiments, which is limited by the thread of worker server and the node number of clusters.
Meanwhile, the variation of peak QPS is due to the change of the performance bottleneck in different solutions.
For example, the saturation performance of BC-GPU and CMC is constrained by the thread of the server while that of BC-CPU, BC-FPGA, and BC-DLG-x is determined by the latency of deep hashing inference and data movement.
Fig. 13(b) shows that the QPS and latency per request of four baselines and CMC also change with the scale of the node cluster when the number of concurrent users reaches 400.
As the number of nodes increases, the QPS gradually increases to the peak value, and the latency gradually decreases owing to improvement of service parallelism.
When the nodes increase, the load-balancing mechanism of Nginx prevents a large hotspot formation in the cluster, which greatly increases the waiting time of requests.
We also measured the power consumption of the CMC system while running the CBIR service and compared it to BC-CPU, BC-GPU, BC-FPGA, and BC-DLG-x.
When the cluster system is active, as shown in Fig. 14, the power consumption of a single node in the BC-CPU and BC-GPU are respectively 1.52x and 2.70x higher than a single node in the CMC.
Similarly, Fig. 15 indicates the power of BC-CPU and BC-GPU is 1.45x and 2.46x than that of a CMC.
Meanwhile, we also compared the QPS/Watt of CMC with other four baseline in Fig. 13(c).
is higher than the other four baselines because of short data movement path and the energy-efficient DLG-x accelerator.
When the cluster system is IDLE, the power consumption is slightly higher than BC-CPU and BC-GPU because of the power consumption of the Cognitive SSD prototype is higher than the enterprise SSD.
It is noted that Fig. 15 indicates the power consumption of the Cognitive SSD only occupies about 15.93% (IDLE) and 14.08% (ACTIVE) of the entire system in the CMC architecture.
The Cognitive SSD contains one Dual 1GHz ARM Cortex-A9 core, which could run embedded Linux system and has lower power consumption compared with the Intel Xeon CPU.
Thereby, as shown in Fig. 12(b), to further reduce power consumption, we proposed the architecture of the host-free cluster (HFC) system, where the Cognitive SSD is directly connected to the Ngnix server via TCP connection, and the embedded Linux system runs a simple NAND flash management daemon and crow web framework.We measured the performance of the host-free cluster system under the same experimental setup, which is illustrated in Fig. 13, Fig. 14, and Fig. 15.
Fig. 14 shows that the power dissipation of a single node in the host-free cluster system is reduced by up to 89.2%, 93.9%, and 83.6% compared with that of BC-CPU, BC-GPU, and the original CMC when system is active.
Considering the host server contains two Intel Xeon E5-2630 CPU that outperforms the dual Cortex-A9 in the Cognitive SSD, thereby, we measured the QPS per watt (QPS/Watt) to illustrate the energy-efficiency of the HFC system.
The result (Fig. 13(c)) shows that when system concurrency is low, HFC delivers better energy-efficiency than the other four baselines and even better than the CMC architecture.
The reason is that using high-performance machines to handle infrequent requests results in low energy-efficiency.
Therefore, Fig. 13(c) witnesses the energy-efficiency of HFC relatively decreases with the increasing concurrency of the system.
The performance growth of HFC under different node numbers also project that the level-off throughput is limited by the embedded CPU power instead of the DLG-x.
In analysis, the HFC system will have much lower power consumption and higher performance if the Cortex-A9 processor is replaced by the latest Cortex-A series, e.g., a quad-core or octo-core Cortex-A75.
Therefore, connecting the Cognitive SSD directly via interconnects contributes to much higher energy efficiency in Cognitive SSD system and guaranteed service throughput as well.
In this paper, we have introduced the Cognitive SSD, a neardata deep learning device that actively performs low latency, low power and high accuracy unstructured data retrieval.
We have designed and implemented the Cognitive SSD with a direct flash-access deep hashing and graph search accelerator, to combat the complex software stack and inefficient memory hierarchy barriers in the conventional multimedia data retrieval systems.
Our prototype demonstrates that the Cognitive SSD reduces latency by 69.9% on average compared to CPU, and more than 34.4% and 63.0% power saving against CPU and GPU respectively.
Furthermore, the Cognitive SSD can scale to a multi-SSD system and significantly reduces the cost and power overhead of large-scale storage nodes in data centers.
The demo of the retrieval system based on Cognitive SSD is available at [3] and part of the source code is available at [2].
We thank our shepherd, Joseph Tucek, and the anonymous ATC reviewers for their valuable and constructive suggestions.
We thank the professor Jiafeng Guo of the CAS key lab of network data science and techology for his supports and suggestions.
This work was supported in part by the National Natural Science Foundation of China under Grant 61874124, Grant 61876173,Grant 61432017, Grant 61532017, Grant 61772300 and YESS hip program No.YESS2016qnrc001.
