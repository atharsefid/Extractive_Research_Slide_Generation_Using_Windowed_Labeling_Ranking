Many tasks that leverage web search users' implicit feedback rely on a proper and unbiased interpretation of user clicks.
Previous eye-tracking experiments and studies on explaining position-bias of user clicks provide a spectrum of hypotheses and models on how an average user examines and possibly clicks web documents returned by a search engine with respect to the submitted query.
In this paper, we attempt to close the gap between previous work, which studied how to model a single click, and the reality that multiple clicks on web documents in a single result page are not uncommon.
Specifically, we present two multiple-click models: the independent click model (ICM) which is reformulated from previous work, and the dependent click model (DCM) which takes into consideration dependencies between multiple clicks.
Both models can be efficiently learned with linear time and space complexities.
More importantly, they can be incrementally updated as new click logs flow in.
These are well-demanded properties in reality.
We systematically evaluate the two models on click logs obtained in July 2008 from a major commercial search engine.
The data set, after preprocessing, contains over 110 thousand distinct queries and 8.8 million query sessions.
Extensive experimental studies demonstrate the gain of model-ing multiple clicks and their dependencies.
Finally, we note that since our experimental setup does not rely on tweaking search result rankings, it can be easily adopted by future studies.
The constantly growing web search traffic makes search activity logs a valuable source of information for understanding user preferences, which can be leveraged for many practical tasks including personalization (e.g., [18,20]) and optimization of search ranker (e.g., [1,11]).
As one of the most direct and reliable evidence of user experience, clicks in the search result page can be easily logged and aggregated.
One of the most challenging problems in click log analysis is to model the underlying mechanism that gives rise to clicks.
Because of its importance, many researchers have investigated how to model user clicks (e.g., [6,12,14]) and make use of developed models for downstream applications such as click log generation [9] and search engine evaluation [7].
A key trade-off in click model design is making reasonable user behavior assumptions for good predictive power, while preserving model efficiencies and robustness so that it can be applied to millions and billions of queries which is the typical in reality.An important piece of work by Craswell et al. [6] proposed a cascade model and compared it with four basic models for user clicks.
Specifically, the cascade model abstracts user clicks by taking two premise assumptions, both of which were supported by existing user studies ( [12,13]) and were adopted by other click model studies (e.g., [9,17]).
The first assumption is the examination hypothesis, which states that a document must be examined before being clicked.
It decouples the randomness of a click into a position-dependent probability of being viewed and a document-dependent probability of being clicked.
This, to some extent, neutralizes the position-bias that is inherent in user clicks [14].
The second assumption is the linear traversal hypothesis, which suggests that users examine document abstracts by traversing the search result page from top to the bottom.
Based on these two assumptions, the cascade model depicts how the first click arises: a web user examines documents oneby-one in order until the first click; after that the user never comes back.
The capability of the cascade model is demonstrated with a controlled experiment based on real web users, which shows significant improvement over competing models.
However, it assumes that a user abandons examination of web documents upon the first click.
This unfortunately restricts the modeling power to query sessions with at most one click, which leaves the gap open for real-world applications where multiple clicks are possible, especially for informational queries [4] which have a relatively large average number of clicks per query session.Therefore, in this study, we investigate how to model query sessions with multiple clicks.
We find that some previous work already shed lights on how to model multiple clicks, e.g., [6,7].
By gleaning their ideas we reformulate them as the independent click model (ICM), because it assumes (1) each web document in the search result is examined with probability one regardless of where it appears, and (2)click probabilities at different positions are independent.As our experiments suggest, despite its simplicity, ICM actually performs very well for some tasks, e.g., predicting the first clicked position, but falls short for other challenging tasks like predicting where the last click happens.Because clicks are inherently dependent, we propose to incorporate dependencies in modeling user clicks.
We therefore propose the dependent click model (DCM) which generalizes the cascade model to multiple clicks by including a set of position-dependent parameters to model probabilities that an average user returns to the search result page and resumes the examination after a click.
In return, it offers uniformly better performance than ICM, and does very well on the challenging task of predicting last clicked position.Although DCM models dependencies between clicks as well as examination at different positions, it turns out to be efficient as well.
Its worst time and space complexities are linear to the number of distinct web documents.
Furthermore, DCM as well as ICM can be incrementally updated, which is a nice property for click log analysis in web search, considering click logs flow in as a data stream constantly.
This will help propel its applications to handle the overwhelming web traffic nowadays.
In comparison, a complicated model with sophisticated approximate inference algorithms generally does not scale well and incrementally update would be a non-trivial task for it.In summary, we make the following contributions in this study:• We investigate how to model user searches with multiple clicks, which closes the gap between previous work and reality.
Specifically, two models (ICM and DCM) are put forward, with the former one gleaned from previous studies and the latter one developed by ourselves.
• We propose a set of experiments that can be easily adopted by future studies.
The experiments presented in [6] for the cascade model, though good and convincing, is quite difficult to reproduce; it relies on tweaking the ranking algorithm in search engine implementation and presenting these results to real web search users.
• We present an extensive experimental study based on a data set containing over 8.8 million query sessions.
Results demonstrate the gain of modeling multiple clicks and their dependencies in between.The rest of this paper is organized as follows.
We first elaborate on the two models in Section 2, and then present experimental studies in Section 3.
Section 4 discusses related work, and this paper is concluded in Section 5.
A web search user initializes a query session by submitting to the search engine a query string, or a query.
Any resubmission or reformulation of the same query are regarded as distinct query sessions.
The user may decide to click some or none of the web documents returned by the search engine.
Search engine click logs then contain the submitted query, a ranked list of returned documents, whether each of them is clicked or not, and other information that might be useful.
Here we focus on the first search result page, and discard any other elements in this page, e.g., sponsored ads and related search.
We use document impression to indicate appearance of web documents in search result pages at certain positions, or ranks.
Documents in higher positions appear before those in lower positions.Click models learn from user clicks to help understand and incorporate users' implicit feedback.
In this paper, we follow a probabilistic approach which treats user clicks as random events, and the goal is to design generative models which are able to approximate underlying probabilities of clicks with high accuracy.
Furthermore, we expect that distribution and statistics of samples generated from these models will match those of the empirical data closely.For a given query, we assume that each document d is associated with a document relevance r d ∈ [0,1], which is the probability that it is considered to be relevant to the query.
Document impression in the query session of interest is denoted by {d1, . . . , dM }, where M is the number of documents shown in the first page.
Click models that adopt the examination hypothesis specify examination probabilities e d i ,i = P(examination at position i | document impression d) for 1 ≤ i ≤ M as well as click probabilities e d i ,i defined in a similar fashion.
Single-click models are adopted from previous work in [6], which focused on modeling click position-bias.
The simplest approach includes a universal click probability r, such thatc d i ,i = r,(1)whereas a more reasonable assumption is that when there is no position-bias, clicks only depend on document impression:c d i ,i = r d i .
(2)A more elaborate approach assumes that a user examines documents in the search result page one-by-one from the top position until the first click, andc d i ,i = r d i i−1 j=1 (1 − r d j ).
(3)A click in position i implies that all positions above are skipped, i.e., not clicked, and this click probability depends on the relevance of documents that appear at positions 1 ≤ j < i.
This cascade model delivers better performance than a number of competing models overall, but unfortunately, works less favorably for bottom positions [6], which is mostly due to the single-click assumption.
Without positional-bias, click events at different positions are independent of each other.
The single-click model specified in Eq.
2 can be directly applied to a whole query session, and this idea was mentioned in [6,7].
Another way to derive this model is by assuming that users always come back after clicks in the cascade examination processes, and fitting multiple cascade models to the multiple clicks.
Since each position is examined with probability one, there is actually no position-bias and we obtain the same model with independent click probabilities.
So here we reformulate this model as follows:e d i ,i = 1, c d i ,i = r d i ,(4)and name it the independent click model (ICM) in this paper.
We will compare it with the model introduced next that considers dependent click probabilities in a query session.
Given the actual click event {C1, . . . , CM } in a query session as well as the document impression {d1, . . . , dM }, the log-likelihood for ICM is given byℓICM = M i=1Ci log rd i + (1 − Ci) log(1 − r d i ).
(5) Given a query and its corresponding query sessions in the training data, learning the ICM is to find r d for every distinct document d to maximize the log-likelihood ℓICM .
The optimal value is achieved by simply setting r d to the empirical click probability:r d = # Click on d # Impression of d .
(6)Therefore, we can set up two counting statistics to each document d, and parse only once through the training data to get all such counts, and finally compute all document relevance estimates.
This leads to an learning algorithm with linear time complexity with respect to the number of query sessions and linear space complexity with respect to the number of distinct query-document pairs.
When new data are available, we can do fast update and re-computation based on these counts, also in linear time and space complexities.To evaluate multiple-click models in Section 3, we compute log-likelihood on the test data, and also draw samples from the ICM given document impressions for each query session in the test data:Ci ∼ Bernoulli(r d i ).
(7)Then they are compared with the ground truth to obtain performance measures.
In the cascade model a user always leaves the result page upon the first click and never come back.
We propose to include a position-dependent parameter λi to reflect the chance that the user would like to see more results after a click at position i.
In case of skip (no click), the next document is examined with probability one.
λi's is a set of user behavior parameters shared over multiple query sessions.
This user model is shown in Figure 1.
Examination and click probabilities in DCM can be specified in an iterative fashion (1 ≤ i ≤ M ):e d 1 ,1 = 1, c d i ,i = e d i ,i r d i , e d i+1 ,i+1 = λic d i ,i + (e d i ,i − c d i ,i ),(8)from which the following closed-form equations can be derived: Figure 1: The user model of DCM, in which r d i is the document relevance of di, and λi is the user behavior parameter for position i.e d i ,i = i−1 j=1 1 − r d j + λj r d j ,(9)c d i ,i = r d i i−1 j=1 1 − r d j + λjr d j .
(10)This completes the formal specification of the dependent click model (DCM), in which examine probabilities and click probabilities at different positions i become interdependent.
The log-likelihood for a query session with one or more clicks is given byℓDCM = l−1 i=1 Ci(log r d i + log λi) + (1 − Ci) log(1 − r d i ) + C l log r d l + (1 − C l ) log(1 − r d l ) + log 1 − λ l + λ l n j=l+1 1 − r d j ,(11)≥ l i=1 Ci log r d i + (1 − Ci) log(1 − r d i ) + l−1 i=1 Ci log λi + log(1 − λ l ).
(12)If there is no click in this session, then the log-likelihood is a special case with l = M, C l = λ l = 0.
We carry out DCM learning by maximizing the lower bound of log-likelihood in Eq.
12.
Document relevance estimate for a document d is given by:r d = # Click on d # Impression of d before position l .
(13)which is the empirical conditional click probability of d given it appears higher than or at position l. And the best estimate for the user behavior parameter λi = 1 − # query sessions when last clicked position = i # query sessions when position i is clicked ,for 1 ≤ i ≤ M − 1, which is the empirical probability of position i being a not-last-clicked position over all query sessions in the training set.
Compared with ICM, we only need additional 2(M − 1) global counts for λi's to carry out relevance estimate and parameter learning, which are still linear algorithms.
Similar incremental updates are also applicable.An important difference of DCM from ICM is that clicks indicate both relevance and examination.
So if a document is not clicked, it can be attributed to either the document abstract is examined but not relevant enough to be clicked, or it appears lower than other documents that draw the user attention away.
This explain-away effect is reflected in Eq.
13 by a smaller denominator which only counts impression before last clicks than that in Eq.
6 which counts every impression.
However, for top-ranked documents that always appear before the last clicked position, the difference between two models is minor.
This is consistent with the understanding of ICM as the model obtained by fitting multiple cascade models together in a straightforward way.Finally, we give the sampling procedure for DCM which draws examination variables Ei and click variables Ci oneby-one starting from the top position:E1 = 1; If Ei = 0, Ci = 0, Ei+1 = 0; else Ci ∼ Bernoulli(r d i ), Ei+1 ∼ Bernoulli(1 − Ci + λiCi).
(15) We report our experimental studies in this section, which is based on over 8.8 million queries sessions after data preprocessing, sampled from the click log of a major commercial search engine in July 2008.
Experimental results indicate that both multiple-click models fit the reality well: 8 times better log-likelihood results than a baseline approach.
Especially, DCM offers much better last clicked position prediction than ICM and more reasonable click distributions by virtue of its modeling assumption that clicks at different positions are interdependent.
In the following, we start with experimental setup in Section 3.1, and proceed with detailed results in Sec. 3.2, and finally conclude with a summary of experimental findings in Section 3.3.
The data set is obtained by sampling the click log of a major commercial search engine during July 2008.
The click log consists of the query string, the time-stamp, document impression data (URLs of top-10 documents in the first page) and click data (whether each document is clicked or not) for each query session.
Only query sessions with at least one click are kept for better data quality since we find from additional meta-information that clicks on ads, query suggestions or other elements are much more likely to appear for the ignored sessions with no clicks.
It also provides clearer comparison of performances on predicting the first and last clicked position.
For each query, we sort its query sessions by time-stamp and split them into training set and test set of equal sizes.
The number of query sessions in the training set is 4,804,633.
Then these queries are categorized according to the query frequency in the test set.
Top 0.16% (178) most frequently searched queries (also known as head queries) with frequencies greater than 10 3.5 are not included in the subsequent results on test set because most search engines already do very well on these queries.
After data preprocessing, the test set consists of 4,028,209 query sessions For each query, document relevance estimates are computed using Eq.
6 and Eq.
13 for ICM and DCM respectively on the training data.
But for documents which appear very few times in the training set and which appear only in the test set, document relevance are replaced by position relevance, which are computed for each position in a similar way, for deriving log-likelihood and other metrics in the test set.
This has a smoothing effect on the document relevance, and leads to better performance for the evaluation on the test data.
Since the additional counts that we need to keep in the computation, 2M for each query, is usually much smaller than the cost saving from low-frequency documents, the time and space complexities can also be reduced.
The cut off of minimum number of impression for document relevance computation is set adaptively according to the query frequency category from 1 to 6.
Finally, to avoid infinite values in the evaluation, we further imposes a lower bound of 0.01 and an upper bound of 0.99 on the learned relevance values for both models as well as user behavior parameters in DCM.Parsing the data from the hard disk and loading them into main memory takes around 45 minutes.
All the subsequent experiments are carried out in a server machine, with 2.67GHz CPU cores, 32GB memory, Windows Server 2008 64-bit OS, and MATLAB R2008a installed.
The computational time for training DCM is no more than 7 minutes.
Figure 2 presents log-likelihood curves for different query frequencies, where larger log-likelihood results indicate better fit on the test data.
Besides DCM and ICM, the baseline model specified in Eq.
1 is also implemented.
DCM achieves larger performance gain for more frequent queries, and consistently outperforms ICM by over 10% when the query frequency is over 100.
Both DCM and ICM have over 8 times larger likelihood than the baseline; the difference is only less significant for tail queries of frequencies less than 10.
The DCM curve goes below ICM for queries with frequencies less than 10 1.5 .
But this does not imply that we should always apply ICM to model these queries.
Instead, we suggest that lower confidence should be given in document relevance estimates derived from click models for these tail queries.
We could still record counting statistics for these queries, but document relevance estimates should be reliable when new data flow in and the amount of training data is enough to obtain a good fit.
We now focus on clicks and test whether samples generated from ICM and DCM provide good match of first and last clicked position compared with the empirical data.
Given each query session in the test set, we use the document relevance learned from the training set to determine the click probability.
For ICM, clicks are sampled for each position independently, whereas for DCM, sampling starts from the top ranked document and ends at either the first non-examined position or the last (10th) position.
For both models, we collect 100 samples with at least one click, then first and last clicked position are identified from the simulated click data and compared with the ground truth to compute RMS errors.
This is the most time-consuming part in the model evaluation experiments and takes around one hour to finish.
To reflect the inherent randomness in user click behavior, we also compute for each query the standard deviation of first and last clicked position and take a weighted mean over different queries to approximate the lower bound of RMS error.
This corresponds to the "optimal" curves in Figure 3.
We expect a model that gives consistently best fit of click data would have the smallest margin with respect to the optimal error, and this margin also reflects the robustness of model prediction since the RMS error metric takes account of both bias and variance in prediction.
Finally, we aggregate results over all queries and compare the distribution of first and last clicks from two click models with the empirical distribution of the test data, which corresponds to the "empirical" curves in Figure 4.
RMS errors for ICM and DCM are close for first clicked position because their model assumptions are the same until the first click.
Predicting last clicked position turns out to be a more difficult task as demonstrated by higher error curves in Figure 3(b) than 3(a).
With a position-dependent modeling assumption, DCM outputs more reasonable last click estimates than ICM, reducing the RMS error gap from the optimal curve by around 30%.
Figure 4 illustrates generally slower than geometric decrease with the position for the empirical probabilities of both first and last clicks.
DCM matches these probabilities very well at the top 5 positions.
The higher tail of empirical curves is probably due to user scrolling behaviors, especially for informational queries which have a higher click through rate.
And we suspect that users may examine documents in a different fashion when they scroll to the bottom of the search result page, so that the 10th position receives even more last clicks than the two above.
However, they contribute to a fairly small fraction of overall results: clicks after position 6 represent only 6.1% of the total number.
For ICM samples, documents that appears in lower positions may receive more clicks than the ground truth because of the position-independent assumption.
This results in overestimation of last click probabilities for these positions in Figure 4(b).
On the other hand, the document relevance estimates in ICM is smaller than those in DCM, due to a larger denominator in computing the empirical probabilities.
This Table 1.
Darker and lower curves correspond to more frequent queries.under-estimation has a more significant effect on documents which usually appear in lower positions and after the last clicked position.
Therefore, the first click probability distribution derived from ICM has a lower tail than the empirical curve, as shown in Figure 4(a).
A unique property of DCM is that examination probabilities could be computed for each query session and they are aggregated together to provide a hint on user attention over different positions, which corresponds to the dashed curve in Figure 5.
The first position is always examined from the modeling assumption, followed by a geometrically decreasing pattern after position 2.
Compared with the DCM click curve, the gap between them reflects the log conditional click probabilities for each position, which suggests larger probabilities for both top and bottom positions.
Note that both curves go below the empirical click for the last position, and this bias is attributed to user behaviors beyond the modeling assumption as discussed for first and last click distributions.
Moreover, putting the empirical click and last click probabilities together, it seems that a number of users tend to click the bottom few positions simultaneously, leading to an slightly upward tail in Figure 4(b) for empirical curves.Figure 6 displays detailed examination probabilities for different query frequencies.
All of them share similar decreasing pattern but differ in absolute values.
The trend is that less frequent queries tend to be examined in greater depth, and we also observe more clicks per query session in the click log for them.
The extensive experimental study carried out in this study has demonstrated the gain of modeling multiple clicks and their dependencies.
Specifically, both multiple-click models achieve 8 times larger likelihood on the test data than a baseline approach, and DCM is 7% percent better than ICM.
Despite its simplicity, ICM actually performs very well for predicting the first clicked position, but falls short for the challenging task of predicting where the last click happens compared with DCM (15% margin) which effectively captures click dependencies.
Click distributions derived from DCM matches empirical ground truth on the test data very well, especially for the top 5 positions which receive over 90% of total clicks.
Finally, it is interesting to point out the fact that the category of most frequent queries, which has no more than 8 percent of multiple-click query sessions, benefits most from the multiple-click model.
So it is the amount of data per query that plays an important role in boosting the performance of click models.
Previous work most relevant to this paper is the cascade model presented in [6], earlier work carried out by Dupret et al. [7,9], and eye tracking studies in [12,13].
They have been discussed in Section 1.
Joachims [11] presented a pioneering study to exploit clickthrough data for optimizing the ranking function for search engines.
Pairwise preference feedback, such as web document i is more relevant as web document j, are extracted from click logs and used to train a ranking support vector machine (ranking SVM) to output a retrieval function most concordant with these partial orderings.
It was extended by Radlinski et al. [15], and an algorithm was proposed to detect a sequence of reformulated queries from the same user to learn an improved function.
Radlinski et al. [16] followed this line of study for optimizing ranking functions but takes an alternative active-learning approach to control documents presented to users in search result pages for obtaining more helpful feedback as the next-round training data.
The approach we take in this paper is different from these previous studies in that clicks are treated as random events under an explicit user model and document relevance is interpreted as click probabilities upon examination.Clickthrough data could be also combined with other implicit measures or browsing data available from query logs to improve web search.
The studies by Agichtein et al. proposed to extract a spectrum of features from browsing and click activities as well as textual data to train a better ranker [1] and estimate user preference [2].
An earlier work in evaluating these implicit measures appeared in [10].
Note that these additional information may not be able to be collected everyday due to the huge search volume.
And it may also be subject to high level of noise, e.g., web page dwelling time may be inaccurate if a user locks the screen to have a break with the browser open.One of the earliest publications on large scale query log analysis [19] appeared in 1999 which presented interesting statistics as well as a simple correlation analysis from the Alta Vista search engine.
Xue et al. [21] proposed to use clickthrough data to improve graph-based static ranking algorithms.
Bilenko et al. [3] presented a novel study in identifying "search trails" from user activity logs and used a random-walk based algorithm for improved retrieval accuracy.
In [5] Carterette et al. proposed a logistic model for relevance prediction using scores obtained from human judges.There is a very recent paper on click models by Dupret and Piwowarski [8] in SIGIR'08, but we were limited by time to implement their models and algorithms on our data set and compare with the DCM we presented.
So here we give a brief discussion from a theoretical point of view on these two independent threads of click model studies.
Both DCM and user browsing models proposed in [8] adopt the examination hypothesis and the linear traversal hypothesis, but they differ in the specification of examination probabilities over different positions.
In DCM, they are derived from a first-order Markovian examination processes.
In user browsing models, the specification is more complicated and involves M (M +1)/2 user behavior parameters compared with (M − 1) for DCM.
Accordingly, parameter learning and document relevance estimation cost more time and space in user browsing models.
They are performed by an iterative algorithm under the coordinate ascent framework in [8], which requires multiple scans through the training data and each scan is an order of magnitude more expensive than DCM.
Incremental updates for user browsing models would also be a non-trivial task.
DCM, on the other hand, features a simpler specification and more efficient algorithms, but this does not necessarily imply less effective or less robust performance.
Due to the differences in experimental setting as well as the data set property between [8] and this study, the reported results are not directly comparable.
However, it is interesting to point out that in [8] the evaluation metrics is the perplexity, which focuses on prediction accuracy at each position individually, whereas evaluation metrics in this paper, in particular log-likelihood and last clicked position prediction, are based on click prediction over different positions in a query session.
To achieve good performance in these two metrics, a model has to be able to capture click dependencies between different positions.
We attempt to close the gap between previous work and the reality for modeling query sessions with multiple clicks, by presenting two click models: ICM and DCM.
Theoretical derivation shows that both models have linear time and space complexities and a desired property which allows fast incremental computation.
Extensive experimental studies demonstrate that DCM makes good trade-off between model complexity and efficiency, and offers better performance than ICM.
Performance gain is most significant on log-likelihood and predicting last clicked position, where the ability to model click dependencies is a must.
Furthermore, our experimental setup does not rely on tweaking search engine algorithms or active user participation, so it can be easily adopted or reproduced by future studies.
Our future work includes designing click models with more robust performance and flexible user behavior assumptions, as well as comparing the performance of existing click models.
We would like to thank Nick Craswell for the discussion and comments, and the reviewers for suggestions that improved the presentation.
We are also grateful to Ethan Tu and Li-Wei He for their help.
Our thanks to Kaisen Lin and Alex Rasmussen for their comments on the manuscript.
