New non-volatile memory technologies offer unprecedented performance levels for persistent storage.
However, to exploit their full potential, a deeper performance characterization of such devices is required.
In this paper, we analyze a NVM-based block device-the Intel Optane SSD-and formalize an "unwritten contract" of the Optane SSD.
We show that violating this contract can result in 11x worse read latency and limited throughput (only 20% of peak bandwidth) regardless of parallelism.
We present that this contract is relevant to features of 3D XPoint memory and Intel Optane SSD's controller/interconnect design.
Finally, we discuss the implications of the contract.
New NVM technologies provide unprecedented performance levels for persistent storage.
Such devices offer significantly lower latency than Flash-based SSD and can be a costeffective alternative to DRAM.
One excellent example is Intel's 3D XPoint memory [1,25] from Intel and Micron, available on the open market under the brand name Optane [11].
It is available in various form factors, including Optane memory [9] (a caching layer between DRAM and block device), Optane SSD [10] (a block device), and Optane DC Persistent Memory [8].
Among these devices, the Optane SSD is currently the most cost-effective and widely available option.Optane SSD offers numerous opportunities for applications.
For example, Intel's Memory Direct Technology (IMDT) [7] enables the use of Optane SSD as a DRAM alternative.
Use cases of IMDT/Optane SSD include Memcached [4], Redis [5], and Spark [6].
Evaluations of those scenarios demonstrate the potential role of Optane SSD as a cost-effective alternative of DRAM.
Optane SSDs are also deployed to support key workloads in Facebook [22,23], both as a caching layer between DRAM and Flash SSD for RocksDB and for crucial workloads such as machine learning.
According to Eisenman et al. [23], Facebook stores embeddings of trained neural networks on Optane SSDs.Using new technology effectively requires understanding its performance and reliability characteristics.
For traditional devices, such as hard-drives and Flash-based SSDs, these are well known [16,17,20,21,26,30,33].
However, for new devices like the Optane SSD, much remains unclear, and is thus the focus of our work.In terms of immediate performance, we summarize six Accesses rule).
Fourth, to achieve optimal latency, the user needs to control the overall load of both reads and writes (Control Overall Load rule).
Fifth, to exploit the bandwidth of Optane SSD, clients should never issue requests less than 4KB (Avoid Tiny Accesses rule).
Sixth, to get the best latency, requests issued to Optane SSD should align to eight sectors (Issue 4KB Aligned Requests rule).
Finally, when serving sustained workloads, there is no cost of garbage collection in Optane SSD (Forget Garbage Collection rule).
Overall, these rules are relevant to features of 3D XPoint memory and Optane SSD's controller/interconnect design.
The unwritten contract provides numerous implications for systems and applications.
According to the Access with Low Request Scale rule and the Control Overall Load rule, the design of heterogeneous storage systems including Optane SSD and Flash SSD must be carefully considered.
Many rules (e.g., Random Access is OK) present opportunities and new challenges to external data structure design for Optane SSD.
Finally, the Random Access is OK rule may enable new applications on Optane SSD that don't work well on existing technologies.The rest of this paper is structured as follows.
We describe the unwritten contract and explain the insights for each rule in Section 2.
We provide the implications of the unwritten contract in Section 3, conclude in Section 4 and point to potential research directions in Section 5.
Table 1.
We begin with six rules related to immediate performance like latency and throughput.
We then present rules for sustainable performance.-0.2 -0.1 -0.2 -0.2 -0.2 -0.2 -0.2 -0.2 -0.1 -0.3 -0.2 -0.2 -0.2 -0.2 0.0 -0.2 -0.3 -0.2 -0.2 -0.2 -0.2 0.1 -0.1 -0.2 -0.2 -0.2 -0.2 -0.2 0.3 0.0 -0.1 -0.1 -0.2 -0.1 -0.2 The Optane SSD is based on 3D XPoint memory which is said to provide up to 1,000 times lower latency than NAND Flash [2].
Does 3D XPoint memory always lead to better performance for Optane SSD compared to Flash SSD?
We answer this question with our first rule: to obtain low latency, Optane SSD users should issue small requests and maintain a small number of outstanding IOs.
This rule is needed not only to extract low latency but also enough to exploit the full bandwidth of the Optane SSD.
We uncovered this rule when quantitatively comparing Intel Optane SSD 905P (960GB) with a "high-end" Flash SSD: Samsung 970 Pro (1TB) [12].
From our experiments, we found that Optane SSD does not show improvement for workloads with many outstanding requests.We compare Optane and Flash SSDs with random readonly and write-only workloads.
Each workload has two variables: request size and queue depth (QD) (or number of inflight I/Os).
Figure 1 compares the two devices in terms of average access latency.
The temperature T in each rectangle shows the scaled difference between the latencies of the two systems; T > 0 indicates Optane has smaller latency, whileT < 0 indicates Flash SSD has smaller latency.
Specifically, T = L higher −L lower L lower.
As shown, Optane SSD and Flash SSD outperform each other in different cases.For read-only workloads, Optane SSD has lower latency than Flash SSD when request size and queue depth are small.
Specifically, when the request size 16KB, Optane SSD is better than Flash SSD.
Thanks to 3D XPoint memory's low access latency, the read latency from Optane SSD can be 8.4x faster than Flash SSD.
However, when QD> 8 and request size> 16KB, Flash SSD achieves lower latency, by as much as 40%.
This difference occurs because the latency of Optane SSD increases linearly with higher queue depths (e.g., the latency of a 4KB random read with QD= 64 is 8x slower than QD= 8 and is 11x slower than QD= 1).
Similarly, for write-only workloads, Optane SSD has lower latency for small requests and low QD, while Flash SSD is again better in the opposite cases; however, the difference for writes is not as high as for reads (Optane is up to 1.7x faster than Flash).
This result is due to Flash SSDs log-structured layout and buffering optimizations.
Flash SSD outperforms Optane SSD when request size> 4KB and QD> 2, which includes most of our tested workloads.
The device's internal parallelism dictates its behavior when serving workloads with high request scale.
We are thus motivated to uncover the internals of the Optane SSD.
Like Flash SSD, Optane SSD utilizes a RAID-like organization of memory dies (Figure 2).
Through a fine-grained experiment [18,19] we examine a critical parameter for internal parallelism: the interleaving degree, or number of channels.
We maintain a read stream with stride S from the devices, where S is the distance between two consecutive chunks (a chunk is 4KB or 8 sectors as shown in Section 2.6).
Figure 3 presents the throughput of workloads with various strides.
An individual line represents workloads with the same QD.Optane SSD has a significantly smaller interleaving degree (7) than Flash SSD (128).
In Figure 3, the distance between the lowest dips in each line indirectly indicates the interleaving degree of the device.
For Optane SSD, we observe the pattern is 7, though the difference between dips is not visible until QD= 8.
Our finding agrees with the hardware description of Optane SSD [3]: it has a controller connected to seven channels, each of which is connected to memory dies.The tested Flash SSD reaches its lowest throughput every 128 chunks.
For high queue depths (e.g., 16), it presents a richer pattern with dips at different levels, indicating copious levels of parallelism (channel, package and die).
Overall, Optane SSD has limited internal parallelism, compared to Flash SSD.
This characteristic explains the The limited internal parallelism of the Optane SSD impacts its throughput in two ways.
First, the tested Flash SSD achieves larger maximum read (3500MB/s) and write (2700MB/s) bandwidth than the Optane SSD (2500MB/s); it outperforms Optane SSD serving workloads with large request scale.
Flash SSD's richer internal parallelism enables it to serve more parallel requests.
Second, Optane SSD can achieve peak throughput when serving small requests with low queue depth.
This is due to Optane SSD's limited internal parallelism which requires only a small number of requests to utilize all of its resources.
Hence, the rule to access the device at a low scale not only guides users to obtain low access latency but also is enough to achieve full bandwidth.The influence of contention in Optane SSD is modest compared to that in Flash SSD.
The dips in Figure 3 are due to concurrent requests contending for shared resources (e.g., channels).
In Flash SSD, contention significantly restricts overall throughput; for example, with QD = 16, S = 127 chunks, read throughput is 88 MB/s, which is only 6% of the maximum throughput with the same queue depth.
Although parallel requests to Optane SSD can also introduce contention (limiting throughput to within 86% of maximum), this influence is much less than that in Flash SSD.
With hard drives or SSDs, clients often expect better performance from sequential than random accesses.
With Optane SSD, this is no longer true.
Optane SSD is a random access block device, where clients can observe the same performance for random and sequential workloads.We study Optane SSD's average latency when serving ran- dom and sequential workloads; throughput and tail latency yield similar results.
Each workload maintains four worker threads, while each thread issues IOs randomly or in a sequential stream.
As shown in Figures 4 and 5, on Optane SSD, for requests > 1KB, random and sequential workloads achieve comparable performance.
The difference between sequential and random latency is within 17% for reads and within 5% for writes.
In contrast, Flash SSD prefers sequential over random reads, especially at a low request scale; sequential reads can be 7x faster.
Flash SSD achieves much better sequential performance due to prefetching and simplified address translation.
For writes, Flash SSD presents similar random and sequential latency due to log-structuring.
However, as we will show in Section 2.7, log-structuring introduces significant overhead when the device fills; Optane does not have such concerns.The Random Accesse is OK rule in Optane SSDs occurs due to the ability to perform in-place updates in 3D XPoint memory.
In Optane SSD, there is no difference in address translation costs for random versus sequential workloads; in Section 2.7, we will show that the mapping policy in Optane SSD is based on logical addresses.
In addition, as indicated by our read latency study, there is no prefetching for sequential reads within Optane SSD.Workloads with 1KB requests are special on Optane SSD compared both to other request sizes and to Flash SSD.
We investigate this in the next rule.
The Optane SSD contains shared resources (e.g., channels).
To avoid contention, the Avoid Crowded Accesses rule dictates that clients of Optane SSD should never issue parallel accesses to a single chunk (4KB).
We uncover this rule by investigating the 1KB workload performance shown in Figure 4 and 5.
In Optane SSD, sequential 1KB accesses can increase latency by 63% for reads and by 3.6x for writes, compared to random 1KB accesses.We study the difference between random and sequential accesses for small requests by performing parallel accesses to a single 4KB chunk.
In this experiment, we randomly choose chunks from the device, then issue P parallel reads to different sectors within one chunk.
Figure 6 Figure 7: Latency of Mixed Reads and Writes of latencies for different values of P.
We observe a "stair" pattern for QD> 1.
For each line, the number of levels equals the queue depth and the steps occur at evenly-spaced intervals.
This pattern indicates queuing and/or contention across the issued parallel requests.
Parallel small requests to a single chunk introduce contention.
This experiment illustrates why sequential 1KB workloads have worse performance than random 1KB workloads: although random 1KB accesses may introduce contention, sequential 1KB accesses must introduce contention.
To achieve optimal latency from Optane SSD, the client must control the overall load of both reads and writes.
This rule indicates distinct performance characteristics between Optane SSD and Flash SSD.We discover this rule by looking into the performance of Optane SSD serving mixed reads and writes.
In the experiment, we issue random 4KB requests, varying the percentage of writes from 0% to 100%, with QD= 64 (large enough to achieve full throughput for both Optane SSD and Flash SSD).
Figure 7 shows the access latency of Optane SSD and Flash SSD.
Within Optane SSD, reads and writes are treated equally.
Specifically, on Optane SSD, each type of request is served with the same latency and the latency is related to the overall load, not to the percentage of writes.Flash SSD exhibits distinct characteristics for read-versus write-dominated workloads.
On the left side of Figure 7, for Flash SSD, the read latency is similar to that in read-only workloads with the same queue depth (38% slower than Optane SSD).
However, the write latency is similar to that of a pure-write workload with very low queue depth (and only 19% of that on Optane SSD).
With an increasing number of writes, reads to Flash SSD achieve poor latency due to the influence of writes; when the workload is write-dominated, read latency can be as high as 1.1ms (10x Optane access latency).
Does the byte-addressabilily of 3D XPoint memory enable efficient tiny accesses to Optane SSD?
We answer this question with our rule to Avoid Tiny Accesses: to exploit bandwidth of the SSD, the client must not issue requests less than 4KB.
Figure 8 shows the latency and throughput of random reads less than 4KB, with separate lines for two sectors and eight sectors requests.
As shown, the latency of two sector requests is the same as eight sector (4KB) requests.
However, the throughput of tiny requests is limited by the maximum IOPS supported by Optane SSD (575K); for 1KB requests, throughput is only 20% of the full bandwidth of the device.
Given these two results, there is no benefit in issuing requests smaller than 4KB to Optane SSD.
To achieve the best latency, requests issued to Optane SSD should always align to eight sectors.
We present the difference between aligned and misaligned requests.
In the experiment, we measure the latency of individual read requests (QD= 1); each read is issued to a position A+offset, where A is a random position aligned to 32KB and offset is a 512-byte sector within that 32KB.
Figure 9 shows the read latency of requests issued to different offsets, averaged over a half million requests to the same offset.
Each line represents a workload with a different request size.
In contrast to what one might expect given 3D XPoint's byte-addressability, Optane SSD favors aligned requests.
Requests of one sector have the same latency no matter the offset, and larger requests aligned to eight sectors always get the lowest latency.
For a request crossing the boundary of 4KB, its latency is linearly correlated to the part it issues to the second chunk after the boundary.
The difference between the high and low latencies of 4KB requests is 21%.
Note that the eight-sector chunk here is not related to a concept like a page or block in Flash SSD.
We now study the long-term performance of Optane SSD.
First, we explore its performance when the device gets full.
According to our experiments, there is no need to worry about garbage collection in Optane SSD.
We examine sustained 4KB random and sequential writes on Optane SSD and Flash SSD over three hours.
The device is completely unmapped using the trim command before each experiment; the two devices tested share a similar capacity (960GB vs. 1TB).
We maintain QD= 32 for each workload.
Figure 10 presents sustained write performance.
For Flash SSD, after the device becomes full, write throughput drops significantly because subsequent writes constantly trigger garbage collection.
After about 6000 seconds, write throughput stabilizes: sequential throughput is around 350MB/s and random throughput is 170MB/s (only 7% of the maximum throughput).
The throughput of sequential writes is better than random because of lower garbage collection cost.
Different from Flash SSD, Optane SSD maintains maximum throughput for sustained writes.
The flat throughput for Optane SSD indicates no cost for garbage collection.Finally, we study the mapping policy (LBA→PBA) in Optane SSD by comparing three workload variations.
The first is the same workload as for the interleaving experiments in Fig- ure 3: blocks are first written in logical address order and then read back in that same LBA order (LBA-order write:LBAorder read).
The second workload preconditions the working zone with random writes (random write:LBA-order read).
The third workload preconditions with random writes, but then reads in the order in which the chunks were written (random write:written-order read).
Figure 11 shows the throughput of the three workloads.For Flash SSD, when the read order does not match the write order, the throughput pattern disappears; therefore, its mapping policy is not based on LBA.
Flash SSD uses a mapping policy based on written-order (log-structured) and therefore the throughput pattern only occurs when we read according to the written order; this is why Flash SSD requires garbage collection.
Optane SSD behaves quite differently; no matter how we precondition the device, the pattern occurs when reading according to LBA.
Hence, Optane SSD likely adopts LBA-based mapping.
This design is enabled by 3D XPoint memory's capability to perform in-place updates.
As a result, Optane SSD doesn't require garbage collection and can deliver sustainable performance over time.
The following implications can be drawn from our unwritten contract.
We have two audiences in mind; first, those who design systems for Optane SSD; second, those who combine Flash and Optane in a hybrid setting.The Random Access is OK rule suggests possible restructuring of external data structures on Optane SSD.
Previous designs try hard to convert unstructured accesses into sequential ones, which is now less necessary.
Applications which behave poorly on Flash thus become potential consumers of Optane.
The No Crowded Accesses rule, No Tiny Access rule, and Alignment rule suggest pitfalls that fine-grained data structures must be aware.Heterogeneous storage also needs careful design.
The motivation for hybrid designs comes directly from the Low Request Scale rule: Flash SSD outperforms Optane SSD in some cases.
Optane SSD provides low access latency for workloads with low request scale, while workloads with high request scale might prefer Flash SSD.
For workloads with mixed reads and writes, the Control Overall Load rule suggests that read-dominated workloads should be deployed on Flash SSD to achieve low write latency.
However, write-intensive workloads prefer the Optane SSD, thus avoiding excessive read latency (influenced by writes in classic Flash SSD).
Finally, our results for sustainable performance suggest that when devices become full (and thus would cause garbage collection on an SSD), Optane may be a better choice.
We analyze a popular NVM-based block device: the Intel Optane SSD.
We formalize the rules that Optane SSD users need to follow.
We provide experiments to present the impact when violating each rule, and examine the internals of Optane SSD to provide insights for each rule.
The unwritten contract provides implications and points to directions for potential research on NVM-based devices.
We thank the anonymous reviewers and Anirudh Badam (our shepherd) for their feedback.
This material was supported by the Microsoft Gray Research Lab and funding from NSF grants CNS-1763810 and CNS-1838733.
VII The whole stack: The storage hierarchy is changing.
The stack includes not only DRAM and HDD/SSD, but also Optane SSD and Optane Persistent Memory.
What does this richer hierarchy mean for the operating system [13], the file system [27], and applications?
nThe unwritten contract introduces some open questions requiring discussion and future research: The unwritten contract introduces some open questions requiring discussion and future research:
