The high packet rates handled by network appliances and similar software-based packet processing applications place a challenging load on caches such as flow caches.
In these environments, both hit rate and cache hit latency are critical to throughput.
Much recent work, however, has focused exclusively on one of these two desiderata, missing opportunities to further improve overall system throughput.
This paper introduces Bounded Linear Probing (BLP), a new cache design optimized for network appliances.
BLP works well across different workloads and cache sizes by balancing between hit rate and lookup latency.
To accompany BLP, we also present a new, lightweight cache eviction policy called Probabilistic Bubble LRU that achieves near-optimal cache hit rate (assum-ing the algorithm is offline) without using any extra space.
We make three main contributions: a theoretical analysis of BLP, a comparison between existing and proposed cache designs using microbenchmarks, and an end-to-end evaluation of BLP in the popular Open vSwitch (OvS) system.
Our end-to-end experiments show that BLP is effective in practice: replacing the microflow cache in OvS with BLP improves throughput by up to 15%.
Network virtualization is a core infrastructure component for cloud computing.
In virtualized networks, virtual switches route packets between virtual machines (VMs) and between VMs and the outside world.
Like the VMs themselves, the virtual switch resides in the hypervisor.
The high speed of modern NICs-40Gb/s, 100Gb/s, and even 200Gb/s [2]-makes virtual switches a critical network performance bottleneck.Many software-based network systems, such as appliances, middleboxes, packet analytic frameworks, and virtual switches, rely on fast flow caches to achieve good average-case performance [10,38].
These environments impose challengingand, indeed, somewhat contradictory-requirements upon the caches they use.
First, of course, they benefit from high hit rates.
But, either to avoid wasting memory or to fit in faster levels of the CPU cache, they also strive to be compact.
In addition, because of the high rates at which packet-centric systems operate, the flow cache lookups must have extremely low latency.
* work started while at Carnegie Mellon University These competing requirements place such systems in an interesting middle ground compared to much of the prior work, which usually fall into one of the two extremes.
Higher-level caching systems such as web caches and memcached often adopt comparatively expensive cache designs and replacement algorithms to maximize hit rate [9,26].
On the other hand, CPU caches have such tight timing requirements that they use very simple set associative designs that sacrifice hit rate for extremely low access time measured in clock cycles.In this paper, we present the design, theoretical analysis, and empirical evaluation of a new cache design called Bounded Linear Probing, or BLP, that provides higher cache hit rates than simple set-associative designs, while remaining fast and hardware-friendly.
BLP achieves low latency by ensuring purely local access to the cache data structure: Look-ups require a single read that spans at most two consecutive CPU cache lines.
At the same time, BLP allows non-local propagation of full buckets.
A basic set-associative cache provides only one location for a given set of objects.
BLP allows those objects to creep into later bins, and over repeated inserts and evictions, this property allows high-occupancy bins to shift some of their load to nearby, less-occupied bins.
To better serve skewed workloads, we accompany it with a cache eviction algorithm called Probabilistic Bubble LRU, or PBLRU, that fulfills the same design goals as BLP: It requires no extra space, adds little latency overhead and achieves near-optimal cache hit rate.BLP is a simple and effective design for performancecritical software caches; despite its simplicity, we believe it to be a novel design point in the space of "cache table" designs, and provide a theoretical analysis of why it provides an improved hit rate over basic set-associative designs that access the same number of elements.
The result is a design that performs nearly as well as the fastest set-associative designs, with hit rates that are closer to that of more advanced, yet expensive, designs such as cuckoo or hopscotch-based caches.
We validate these results empirically using both microbenchmarks and by incorporating BLP into Open vSwitch [30], the most popular virtual switch, which is widely used in production.
Replacing the microflow cache in OvS with BLP improves throughput by up to 15%: Its lookup latency is about 10 clock cycles longer than that of the basic set-associative design, but BLP's increased cache hit rate more than compensates for the higher latency.
In contrast, many of the more expensive Megaflow Cache Hash Table Hash Table with Mask Hash Table with Mask … Hash Table with Mask Update Miss Figure 1: Flow Caching Hierarchy in Open vSwitch cache designs that can achieve high cache hit rates do not justify their huge latency penalties.
Our new cache eviction algorithm PBLRU further improves the throughput by up to 10% even if the workload is only modestly skewed.
Open vSwitch achieves high performance through extensive flow caching.
Open vSwitch's caching hierarchy consists of three layers: a microflow cache, a megaflow cache, and a caching-aware packet classifier, as illustrated in Figure 1.
The first cache that a packet encounters in OvS is the microflow cache, which caches forwarding decisions for each transport connection (or microflow).
The microflow cache is a hash table that maps microflows to OpenFlow flows if there is an exact match using all the packet header fields.
If the packet misses in the microflow cache, then OvS does a lookup in its megaflow cache.
This cache supports wildcard matching but does not use flow priorities.
The megaflow cache is a set of n hash tables, each with a unique wildcard mask.
For each hash table, the lookup key is the packet header after applying the mask associated with the table.
These hash tables are reactively created and populated by the packet classifier.
Because looking up a packet in the megaflow cache searches all n hash tables, it is more expensive than a microflow cache lookup.
Therefore, the cache hit rate of the microflow cache (the first cache) is critical to the performance of OvS.The key observation that inspired our work is that although a microflow cache miss is expensive, it is not immensely more expensive than a microflow hit.
In typical deployments, where the average number of hash table searches per megaflow lookup is small (as noted in Section 7.2 of Pfaff et al. [30]), the microflow miss penalty is only hundreds to thousands of cycles on modern server CPUs.
Hence, making the correct tradeoff between cache hit rate and lookup latency is crucial to the system throughput.
In contrast, much of the previous work on software cache designs focuses primarily on improving cache hit rate [9,14,18,13].
In the situations studied by previous work, optimizing for hit rate makes sense: the cache misses in these systems were much more expensive than a hit because they often involved querying very slow backend services such as a database.
The cache hit rate, therefore, determines not only the throughput, but also end-to-end request latency [14,27].
The rest of this paper uses the OvS microflow cache as a case study to analyze and evaluate various design options and demonstrate the effectiveness of our new caching algorithm, Bounded Linear Probing (BLP).
We show how BLP can balance cache hit rate (and thus miss penalty) and lookup latency to improve the throughput of Open vSwitch compared to alternate designs.
Caching is a common and effective technique for speeding up network packet processing; existing solutions include hardware-based [12,38,29] and software-based [10] approaches.
Many early hardware routers used flow caching to achieve fast average-case performance.
In the modern era, most hardware routers and switches have moved to more costly, but guaranteed-performance designs, such as TCAMs, to be able to provide their maximum forwarding rate under arbitrary (and possibly malicious) traffic.
Software switches, however, broadly retain a cache-based design [3,36].
Caching is typically managed using a hash table as its basic data structure, but unlike the "full" problem of a general hash table, caches gain an extra degree of freedom: By definition, they do not need to store all possible keys and may choose to evict an existing item.One of the contributions of this paper is to explore the tradeoff between the cache's hit rate and the lookup/insertion cost imposed by its hash table structure.
To illustrate this tradeoff, we begin in Section 4 by describing points that operate at two extremes of the spectrum: First, a basic setassociative cache, in which an item can be stored only in one of m different slots shared by all other items that hash to the same bucket (row) of the hash table.
This design is fast but achieves a relatively low hit rate.
Next, we introduce two more advanced cache designs that incorporate ideas from cuckoo and hopscotch hashing, which can achieve much higher table occupancy (and thus hit rates), but at the cost of more expensive inserts and lookups.
In the rest of this section, we present prior work on fast caches, including a brief introduction to cuckoo and hopscotch hashing.
Cuckoo [28] and hopscotch [21] hashing both aim to achieve high table occupancy (upwards of 90%) in an "open-addressed" hash table design, i.e., one that does not need to use linked lists to store data items.
The pointer chasing of a linked-list design adds substantial lookup latency, and the pointers themselves can add substantial memory overhead, especially when the entries in the table are small, which is the case for flow caches.
that it allows a key to be placed in one of the H buckets starting from the one it is hashed to.
Although the focus of this work is on software caches, there are many parallels to related work on hardware caches.Cache hit rate versus lookup latency Balancing the cache hit rate and lookup latency has been studied in the context of DRAM hardware caches.
Alloy Cache [31], for example, improved performance over prior work by reducing the hit latency, even though doing so slightly reduced the hit rate.Set-associative caches Hardware caches are often organized into rows (i.e., buckets) and ways (i.e., slots).
An m-way set-associative cache uses a subset of the address bits to index into a row; the cache block (cache line) can be stored in one of the row's m ways.
To balance the load across rows, researchers have proposed using a hash of the block address as the index [23] as is commonly done in software-based hash tables and caches.
Skewed-associative caches [35] extend this idea and allow each way to be indexed with a different hash function.
In an m-way skewed-associative cache, a cache block B could be stored in row h i (B) for way i, for 0 ≤ i < m.Inspired by cuckoo hashing, zcache [34] is an extension of skewed-associative caching.
Instead of replacing one of the m existing blocks on a cache miss, it performs a breadth-first search to find additional eviction candidates.
After picking a victim entry, it relocates blocks on the cuckoo path to accommodate the new block.
These designs are not wellsuited for high speed, low latency software caches for packet processing, as they require several cache line reads per lookup.
A large amount of prior work on caching [7,20,32,9,8,14,13] focuses on cache eviction policies.
Improved policies, ranging from LRU and LFU to modern alternatives such as LHD [9], increase cache hit rate under skewed workload distributions by biasing eviction towards likely less-useful candidates.The majority of prior cache eviction algorithms require additional tracking metadata to implement their eviction policies.
In contrast, our new algorithm, PBLRU, adds no space overhead.
The most related work to our algorithm is an earlier paper by Zhang and Xue [39] that explores the same bubbling idea.
We discuss the differences between PBLRU and their algorithm, DC-Bubble, in Section 5.3.
We begin by presenting two baseline cache designs, a setassociative option and a "cuckoo-like" option, and analyze their expected hit rates.
We then introduce bounded linear probing and its analysis using the same framework.To understand the expected hit rate, we assume that the working set is fixed, and that each lookup key is drawn uniformly at random from that working set.
We only analyze uniform distributions in this section, for the following two reasons: a) prior works studied caching performance on uniformly-distributed workloads [25] and b) the expected hit rate under the uniform distribution is easier to analyze, yet it provides a lower bound on the hit rate under any other distribution (see Appendix C for a formal argument).
We use α to denote the ratio of the working set size to the number of entries in the cache table, which we call the oversubscription factor.
When α < 1, the cache has more capacity than there are items in the working set.
We determine hit rate in terms of α, and then provide numerical interpretations for some values observed in the OvS workloads, such as α = 0.95.
In Section 7, we show empirical hit rate curves for real implementations across a range of α values.All of the designs we evaluate use some amount of setassociativity.
The caches are partitioned into n buckets, each containing m entries.
To determine if an item is in a bucket, the implementation examines whether it is stored in any of the entries in the bucket.
The table contains a total of n × m entries, and the working set has size α × n × m. To store a key-value pair (k, v), one hashes the key and determines in a table-specific way a set of (one or more) buckets that could hold the key, and stores both fingerprint(k) and v in an appropriate entry.
In OvS, v is a pointer to a megaflow cache entry.
Each design uses a different algorithm to decide which entry of the table will store a given pair.
To analyze the expected hit rate of a cache design, it suffices to estimate the expected number of keys the cache could hold after a sufficiently long warm-up period.
This is because, in our formulation, each cache access is uniformly random, so the cache hit rate is equal to the total number of cached keys divided by the size of the working set.
Moreover, the number of keys stored in all the cache designs we evaluate never decreases with an increasing number of cache accesses, and it has a maximum value of n × m. Therefore, it will eventually stop increasing.
Denote the final number of occupied entries in bucket i by c i .
The probability of a cache hit is equal toc 0 + · · · + c n−1 αnm .
(1)By symmetry, all c i have the same expected value.
Hence, by linearity of expectation, the expected cache hit rate is E[ci]/(αm).
For each cache design, we describe how its hit rate is estimated from a high level, and leave all the details to appendices.
We start with a simple design-a set-associative cache.
In an m-way set-associative cache, each item is mapped to a bucket by a hash function h, and each bucket has m slots.
Figure 3a SIMD-optimized Lookup To accelerate lookups, we use SIMD instructions to compare multiple fingerprints at the same time (similar to techniques used by Google's Swiss Tables [6]).
Figure 5 shows how the lookup works in a 4-way set-associative cache.
The stock OvS design does not use SIMD-accelerated reads for its microflow cache, so to ensure a fair basis for comparison, we implemented this optimization and use it as the baseline for comparison.To search for fingerprint f in a bucket, we first duplicate it four times and store it in a 64-bit integer match.
Then, we load the first 64 bits of the bucket into another 64-bit integer sig.
We compare the packed 16-bit integers in sig and match for equality, storing the results in cmp.
cmp consists of 4 16-bit integers r 0 ,r 1 ,r 2 and r 3 , where r i is 0xFFFF if f i = f and 0 otherwise.
We can then count the number of trailing zeros in cmp to figure out which slot f matches in the bucket.Lookup in an 8-way set-associative cache works similarly to the 4-way set-associative cache, but uses 128-bit integers instead of 64-bit integers.
For 2-4 cuckoo-lite, because the eight candidate fingerprints are not consecutive, we have to first copy the fingerprints from two buckets into one 128-bit integer, then perform packed integer comparison.SIMD-accelerated lookup in 2-4 BLP works as follows: The eight candidate fingerprints are not contiguous in memory (unlike cuckoo-lite), but are separated by the 64 packed value bits.
Therefore, instead of copying fingerprints, we load both buckets into a wider 256-bit integer and mask off all the value bits.
Eliminating the extra load instruction reduces the lookup latency by ∼ 10% and makes BLP more SIMD-friendly than cuckoo-lite.
Buffer Bucket In 2-4 BLP, if the lookup key hashes to the last bucket of the table, both the first and the last bucket are searched.
This corner case requires both a second cache line read and, more importantly, an extra branch.
To avoid incurring branch prediction misses, we added a buffer bucket following the original cache table.
This buffer bucket has minimal impact on the cache miss rate but improves lookup speed: When the lookup key hashes to the last bucket, and that bucket is full, the new key spills to the buffer bucket instead of wrapping around the table.
At lookup time, we search both the last bucket and this buffer bucket, which avoids the branch misprediction and allows for processor prefetching4.
One thing which worth mentioning is that this optimization is specific to BLP and does not work well with other cache designs -it breaks the alignment of the number of buckets (typically a power of 2).
Moreover, the extra space given by this buffer bucket is negligible compared to the size of the cache.
We therefore only apply the optimization to BLP.Batched Lookup with Prefetching We use batched lookup with prefetching to overlap bucket computation with memory reads, which minimizes the impact of DRAM access latency.4Note that this optimization means that no keys will ever spill into the first bucket.This technique is common in many existing packet processing applications and frameworks [40,22,11].
We present our evaluation top-down: We begin with a description of the experimental setup followed by a set of end-to-end benchmarks that compare the different cache table designs (described above) in the context of Open vSwitch.
These results demonstrate the benefits and generality of BLP in a realistic packet processing application.
Next, we use a set of microbenchmarks to understand more deeply the fundamental tradeoffs that each of the cache design brings to the table.
Our experiments are conducted on c220g2 instances from CloudLab [33].
Each of the instances is equipped with the following hardware:Hardware Description CPU 2× Intel Xeon E5-2660v3 CPUs (2.60GHz) DRAM 160 GiB DDR4 Memory L3 Cache 2× 24 MiB NIC Intel X520 dual-port 10GbEWe also controlled for the following factors, which otherwise had noticeable effects on our results: Throughout the experiments, we use PCG-32 [5], a fast and statistically robust algorithm for our random number generation.5Cache Warming As discussed in Section 4, 2-4 cuckoo-lite and 2-4-BLP do not displace keys.
Instead, they depend purely on cache warming to reach the maximum hit rate.
Therefore, in each experiment, we first warm the testing cache until it reaches a stable state, i.e., the cache hit rate stops increasing.All experimental results reported below are the average of five runs.
The variance was low, so we omit error bars from our graphs.
Because the differences between many of the designs are small-in the range of 10% or so-while the absolute performance differences between a high cache hit rate (low alpha) and a low cache hit rate (high alpha) are relatively large, we deliberately choose not to start axes at 0; the graphs are "zoomed-in" to the regions of interest.
As a concrete end-to-end benchmark using an important application, we modified the microflow cache in Open vSwitch (v2.10.1) to use the various cache designs described above.Open vSwitch was running on the a c220g2 instance with two 10Gb Ethernet ports, port 0 and port 1.
To accurately 5The quality of the random number generator directly affects the cache hit rates.
Unintended workload locality (i.e., back-to-back keys that hash to nearby buckets) produces higher than expected hit rates; poor random number generators exacerbate this effect.
Earlier in the research process for this work, a bad, hand-crafted random number generator caused this issue.
In a m-way set-associative cache with n buckets, for each bucket, the probability that there are exactly t keys mapped to it is񮽙 αnm t 񮽙 · n −t · (1 − 1/n) αnm−t = αnm(αnm − 1) · · · (αnm − t + 1) t!
· n t · (1 − 1/n) αnm−t which when t ≪ αnm, is approximately (αnm) t t!
· n t · (1 − 1/n) αnm = (αm) t t!
· (1 − 1/n) αnmwhich by the fact that 1 − ǫ ≈ e −ǫ for small ǫ, is approximately (αm) t t!
· e −αnm/n = (αm) t e αm t!
If t ≤ m, then all t keys will be cached; otherwise, only m will be cached.
Therefore, the expected number of keys that are cached in a bucket is approximatelym 񮽙 t=0 t · (αm) t e αm t!
+ ∞ 񮽙 t=m+1 m · (αm) t e αm t!
,which by the fact that 񮽙 Recall that a i is the number of keys from the working set that map to cache bucket i and b i is the number of keys spill from bucket i to i + 1 after a sufficiently long warm-up period.
For j ≥ 0, we have:Pr[a i = j] = 񮽙 αnm j 񮽙 · n −j · (1 − 1/n) αnm−j .
By the law of total expectation and Equation (2) in Section 4.5, for 0 < l < m, we havep l ≈        񮽙 m j ′ =0 񮽙 񮽙 m−j ′ j=0Pr[a i = j] 񮽙 · p j ′ if l = 0, 񮽙 l+m j=l Pr[a i = j] · p l+m−j if 0 < l < m,and by the definition of probability distribution, p 0 + · · · + p m = 1.
In the following, we prove that the expected hit rate obtained in Section 4.5 for the uniform distribution is always a lower bound for any other distribution.
Fix any distribution over the working set S, let p x be the probability of key x. Without loss of generality, we may assume p x > 0 for all x ∈ S, since otherwise, we could simply remove all x with zero probability from the working set.
Recall that c i denotes the final number of occupied entries in bucket i. Observe that c 0 , . . . , c n−1 are determined only by the hash function, i.e., how many keys are mapped to each bucket.
They do not depend on the probability distribution of the keys (as long as all keys have non-zero probability), the distribution only affects how fast the final numbers are achieved.After a sufficiently long warm-up period, all buckets achieved their final numbers of occupied entries.
Now, consider all possible memory configurations after the warm-up.
Further key lookups define a Markov chain over them, where the transition probability from memory configuration A to configuration B is the probability that A becomes B after one lookup.
Observe that this Markov chain is aperiodic (i.e., there does not exist a t > 1 and a state A such that A can only go back to itself after steps of multiples of t).
It is well-known that for any aperiodic Markov chain and any initial state, as the number of steps (key lookups) increases, the distribution of the state will approach some final stationary distribution (note that the stationary distribution may not be unique, hence the final stationary distribution may depend on the initial state).
The final hit rate is computed from this stationary distribution and the distribution of the keys.
More specifically, let q x be the expectation, over a random hash function and random lookups in the warm-up period (which determine the Markov chain and the distribution of initial state), of the probability that key x is cached according to the final stationary distribution.
Thus, the expected hit rate is equal to 񮽙 x ∈S p x q x .
Next, by linearity of expectation, 񮽙 x ∈S q x is equal to the expected total number of occupied entries in the data structure, E[ci] · n.
The key observation is that if p x ≥ p y then q x ≥ q y , i.e., if a key is more likely to occur, then it has a higher probability to appear in the final stationary distribution, over a random hash function and warm-up period.7 Let S high := {x : p x ≥ 1/|S|} be the set of keys that occur with at least the average probability and S low := {x : p x < 1/|S|} be the set of keys that occur with probability lower than the average, and let q := min x ∈S high q x .
Hence, q ≥ q x for all x ∈ S low .
We have񮽙 x ∈S p x q x = 񮽙 x ∈S q x |S| + 񮽙 x ∈S (p x − 1/|S|)q x = 1 |S| 񮽙 x ∈S q x + 񮽙 x ∈S high (p x − 1/|S|)q x + 񮽙 x ∈S low (p x − 1/|S|)q xwhich by the fact that p x ≥ 1/|S| and q x ≥ q for x ∈ S high , and the fact that p x < 1/|S| and q x ≤ q for x ∈ S low , is at least≥ 1 |S| 񮽙 x ∈S q x + 񮽙 x ∈S high (p x − 1/|S|)q + 񮽙 x ∈S low (p x − 1/|S|)q = E[ci] · n |S| + 񮽙 x ∈S p x · q − 񮽙 x ∈S 1 |S| · q = E[ci] · n |S| + q − q = E[ci] · n |S| .
The last quantity E[ci ]·n |S |is precisely the expected hit rate under the uniform distribution, as we argued in Section 4.
Therefore, the expected hit rate under any non-uniform distribution is always lower bounded by the hit rate under the uniform distribution.
In the following, we present an informal estimation on the relationship between the hit rate of BLP and its warm-up time.
As we argued in Section 4.5, the hit rate is equal to the number of occupied entries in the BLP divided by the size of the working set.
In each lookup in the warm-up period, the key may be either a) in the BLP already, or b) not in the BLP and the buckets are full, or c) not in the BLP and the bucket is not full.
Only in case c), do we increase the number of occupied entries by one.
Denote the final hit rate by r max .
When 7Note that this is not true if the hash function is fixed.
the current hit rate is r, we are going to approximate the probability of case c) by r max − r.
That is, we assume there is a fixed set of (r max − r) · (αmn) keys in the working set such that they are the missing keys from the BLP in order to achieve the maximum hit rate of r max .
Therefore, let L be the length of the warm-up, we have dr dL = r max − r αmn , and when L = 0, r = 0.
By solving this ordinary differential equation, we obtain r = r max (1 − e − L αmn ).
That is, when the length of the warm-up is a large constant times the working set size, the estimated hit rate becomes very close to r max .
For α = 0.95, we have verified by experiments that a warm-up period of length 20 times the working set size is sufficient to obtain a hit rate that is less than 1% lower than r max .
