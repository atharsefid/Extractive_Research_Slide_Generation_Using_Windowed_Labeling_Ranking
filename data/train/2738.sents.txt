One-sided network communication technologies such as RDMA and NVMe-over-Fabrics are quickly gaining adoption in production software and in datacenters.
Although appealing for their low CPU utilization and good performance, they raise new security concerns that could seriously undermine datacenter software systems building on top of them.
At the same time, they offer unique opportunities to help enhance security.
Indeed, one-sided network communication is a double-edged sword in security.
This paper presents our insights into security implications and opportunities of one-sided communication .
Traditional network communication in datacenters takes a twosided form where both the sender and the receiver machines' CPU and software (OS or user-level programs) are involved in processing network requests.
Several recent technologies in datacenters enable one-sided communication where the receiving machine's CPU and software are completely bypassed in the processing of incoming network requests.
Instead, network hardware devices handle incoming requests and directly read/write into host machine's memory (e.g., RDMA, Gen-Z [12], Omni-Path [4]), storage (e.g., NVMeover-Fabrics [37]), or GPU memory (e.g., GPUDirect [9]).
Because of its low CPU utilization and low-latency performance, one-sided communication has been adopted in distributed in-memory systems [3,10], fast storage systems [37], and new architectures like resource disaggregation [20,45].
Prior research efforts and production systems have focused on one-sided communication's performance, scalability, programmability, and network protocols, ignoring one important factor in datacenters, security.We call for attention to the security aspects of one-sided network communication.
We study the basic communication patterns and real hardware implementations of one-sided communication to understand its implications in security.
Based on our findings, we draw several key insights in fundamental issues, challenges, and potential solutions in achieving secure one-sided communication.
The most interesting high-level insight is that one-sided communication is a double-edged sword in security: it can cause security threats and offer opportunities in enhancing security at the same time.First, one-sided communication's feature of not having any software processing at the receiving side poses unique threats in distributed systems.
For example, malicious clients can read or write to remote storage servers with one-sided network requests without being noticed, making it hard to account for errors.
Attackers can also easily launch Denial of Service attacks using one-sided communication to swamp the network or the target machine's memory or storage resources.
Second, in order to bypass host processors, hardware devices that enable one-sided communication have to implement functionalities that are traditionally built in software.
We identified several security issues in one-sided hardware, some caused by features added for better performance, some by the need to bypass host CPU, and some by ill-designed implementation.Finally, one-sided communication provides a good opportunity to hide accesses from software, making it easier to achieve user privacy in datacenters.
We designed a secure data store system that leverages one-sided communication to enhance the performance of traditional oblivious RAM systems while delivering the same security guarantees.
This paper makes the following contributions: While the processing needs at receiver nodes increase CPU utilization in two-sided communication, the receiver's processing software stack provides the means to implement various security defenses.
In contrast, the lack of receiver involvement in one-sided network communication poses several security threats in datacenter environments.
This session discusses two fundamental threats of one-sided communication.Threats to accountability.
In distributed, multi-tenant datacenter environments, many parties can potentially cause errors (e.g., node failures, data corruption, stealing information).
It is important to account for errors when they happen.
Accountability enables the pinpointing of the party responsible for a problem and allows other parties to be proven innocent [42].
In a threat model where a server hosts data that multiple clients can read and write, an attacker can be one of the clients that desire to write malicious data or read other clients' data without being noticed.
Under two-sided communication, the receiver handles all incoming network requests and can identify their senders, providing a means for accountability.
However, it is fundamentally difficult for one-sided communication to be accountable, because CPU and software are completely bypassed at the receiving side.
Attackers can exploit this vulnerability to perform one-sided network operations without being detected.
For example, in RDMA-based in-memory storage systems that support one-sided writes [6,56,63], an attacker client can write malicious data to any locations in the store without being detected.
One possible way benign clients can avoid reading unaccountable data is to authenticate the writer of the data with encryption keys.
However, this mechanism needs a fair amount of computation, and the performance overhead is especially large considering one-sided networks' high speeds [32,36].
Threats in denial of service.
Without any processing at the receiving side, one-sided communication also makes it hard to throttle the speed of network requests from any particular sender.
This limitation can be exploited to launch different types of Denial of Service (DoS) attacks [43].
Our threat model here is a cloud environment where an attacker has one-sided network accesses to a server that provides some service (e.g., an RDMA-based key-value store, GPU acceleration with GPUDirect) to multiple clients.
An attacker can flood the network to the server with a large number of one-sided network requests without being detected.Another potential DoS attack that is different from traditional network DoS attacks is to exhaust a server's hardware resources that are used to provide one-sided communication services.
For example, RDMA NICs (RNICs), devices that enable one-sided RDMA operations, store metadata for network connections and memory spaces in on-NIC SRAM so that RNICs can process incoming one-sided requests without involving the host machine.
Attackers can fill the on-NIC SRAM by accessing many different memory spaces, forcing the receiver's RNIC to evict victims' metadata.
To make it worse, the server cannot tell which client is the attacker, because the attacker's one-sided network traffic cannot be detected by the server.
Discussion and defenses.
Attacks that can successfully exploit the above two vulnerabilities rely on the assumption that their traffic cannot be sniffed by trusted parties.
We believe this assumption to be reasonable because most packet sniffing tools require the root privilege to run and datacenter administrators usually prohibit packet sniffing.
For example, packet sniffing is prohibited by RNIC by default and requires the RNIC administrator privilege to change the default configuration [35,53,55].
While many traditional defense mechanisms may not work or will break the one-sided communication pattern, we identified two possible directions for future defenses.
the sender, we can employ defense mechanisms to monitor or control network activities before network requests are sent out with the help of an intermediate layer like LITE [56] or with traffic dump tools [35,53].
Second, at the receiver, we can enhance the hardware devices that handle one-sided requests with better security features, for example, by programming SmartNICs.
To be able to bypass CPU at the receiver machine, hardware devices that enable one-sided communication need to implement many functionalities that are traditionally implemented in software such as permission checking and address mapping.
However, hardware is not flexible and most existing one-sided hardware devices are not programmable.
It is hard to add security features in hardware and security is usually an afterthought in hardware design.
A major threat model we envision is a cloud environment that provides in-memory data store services to cloud users, similar to the threat model in Section 3.
A victim accesses data that a server hosts by issuing one-sided network requests from a client machine.
The attacker runs on another client machine and is another user of the cloud data store service (i.e., the attacker has no access to the victim or the server machines).
This section presents three real cases of security issues in one-sided hardware devices.
These security vulnerabilities could potentially be exploited to launch real attacks with the above threat model, but we leave the design of real attacks for future work.
Case 1: hardware-managed "keys" are not secret.
RNICs use a pair of "keys", lkey and rkey, to protect the local and remote access of each memory region or MR. An RNIC of a machine generates and stores lkey and rkey (together with the memory address) at the time when a user process registers an MR on the machine.
Applications running on other machines use the virtual memory address of a registered MR and its rkey to access it.
When receiving an RDMA request, the RNIC checks its access permission using the rkey.Surprisingly, we discovered that RNICs generate rkeys in a predictable, sequential pattern with Mellanox ConnectX-3, ConnectX-4, and ConnectX-5 RNICs, the three most popular generations of RNICs.
Figure 1 plots the values of lkey/rkey of the first 5000 MRs that are registered at a host in the order of the registration time (lkey and rkey have exactly the same values for all MRs).
For all the three generations of RNICs, there are clear and easy-to-guess patterns in lkey/rkey values.
Moreover, the first 500 MRs have sequentially increasing lkey/rkey values.Most RDMA-based in-memory storage systems use a small number of large MRs [10,56], making it even easier for attackers to guess MR keys.
After guessing both the rkey and the memory address of an MR (the latter can be guessed in a similar way as traditional buffer-overflow attacks), attackers can gain full access to the MR, overwriting victims' memory content or stealing their data.
Case 2: side channels in RNICs.
A key functionality that all RNICs support for one-sided communication is to map from virtual memory addresses to DMA (physical) addresses.
These mappings are essentially page table entries (PTEs) that are stored in host machines' DRAM.
For better performance, RNICs cache hot PTEs in its on-board SRAM.
In addition to PTEs, RNICs also cache MR metadata such as lkeys and rkeys for hot MRs. An RNIC fetches metadata from its host machine's main memory when receiving RDMA requests whose metadata is not in the RNIC's SRAM.We evaluated one-sided RDMA requests' latency when both the data's PTE and MR metadata is in SRAM (hit), when the data's PTE is not in SRAM (PTE miss), and when the data's MR metadata is not in SRAM (MR miss).
Figure 2 presents the timing differences of these three cases over 1000 trials of each case.
There is a clear difference between hits and misses, which can be used to establish side channels on RNICs.
Since RDMA accesses from different applications share the same RNIC SRAM, it is possible to further build real side-channel and covert-channel attacks based on these timing channels.
Case 3: exposing physical memory addresses to remote machines.
Another case where RNIC design can threat security is a feature designed to improve performance.
By default, user processes register MRs with RNICs using virtual memory addresses in their address spaces and RNICs store the virtual to physical memory address mapping in their SRAM.
To eliminate the space and performance overhead of address mapping, Mellanox ConnectX-4 and later versions of Mellanox RNICs introduce a new feature that allows user processes to register MRs directly using physical memory addresses [34].
Applications running on other machines can use these physical memory addresses to access the MRs.Exposing physical memory addresses poses many new security threats, since knowing physical memory address layout is the basis of many attacks.
For example, knowing physical memory addresses make both rowhammer [16,24,62] and throwhammer [52] attacks easier.
Although registering MRs with physical address improves performance, we recommend applications builders to take caution when using this feature.
Discussion.
While the three cases presented in this section all happen with Mellanox RNICs, we believe that similar issues can happen with other one-sided hardware and that there are deep-rooted reasons for why they happen.
There are clear tradeoffs between including more security functionalities in hardware devices and the devices' performance and cost.
For one-sided devices, vendors usually choose the latter over security, because what makes one-sided devices appealing is their superior performance and low cost (on saving CPU utilization).
For example, the side channel threats in Case 2 are a result of vendors choosing performance (caching hot metadata in on-board SRAM) and cost saving (maximize the utilization of SRAM by not isolating SRAM space across applications) over security.Mitigating security issues in one-sided hardware implementation is possible.
For example, RNICs can use cryptographically generated keys as lkey and rkey (Case 1); they can isolate SRAM for different applications or introduce noise to disturb timing differences (Case 2); and they can remove the exposure of physical memory addresses (Case 3).
Certain onesided hardware vendors have manufactured SmartNICs that supports one-sided network communication [32,33].
Various defense mechanisms can be implemented on these SmartNICs (e.g., encryption).
Despite the promise of the above mitigations, it is still challenging but important to deliver security features with minimal impacts on performance, cost, and hardware complexity.
Although one-sided communication poses different threats to datacenter security, it provides one great opportunity to enhance security.
Users can ensure their privacy by hiding their network activities from receiving servers using one-sided communication.
This session discusses how we can leverage this opportunity to develop secure and fast data storage systems.
ORAM, or oblivious RAM, is a type of technology that makes access patterns "oblivious" by continuously re-encrypting and reshuffling data blocks at storage servers [13][14][15]25,40,47,50,51,58].
There have been various secure storage systems built based on ORAM in the past [5,29,44,44,49,60].
One widely used ORAM system is Path ORAM [49].
The basic idea of Path ORAM is to organize the server storage as a binary tree where each node holds a number of (encrypted) objects (e.g., key-value pairs).
Each client caches a small amount of data locally in a stash.
Client read and write requests are handled in the same Path ORAM protocol to make reads and writes indistinguishable.
To perform a client request (read or write) to an object, the client identifies the path (nodes from a leaf to the root) that contains the object using a locally stored mapping table.
The client then reads the entire path into its local stash (read step).
Afterwards, it remaps the requested object to a random leaf node and writes all the objects in the path it has read in the read step (with a new value if it is a client write request).
If there are other objects on the path in the client's stash, they can also be written back together.We adopt the same trust model as previous ORAM systems [29,44,49,60], where trusted clients access an untrusted storage service provider (e.g., in-memory key-value store, NVMe-based storage).
Clients either directly access the server machine that the service provider runs on through one-sided communication or access a trusted proxy which then accesses the service provider through one-sided communication.We also adopt the same level of "security" and "privacy" as existing ORAM solutions [29,44,49,51,60], where the untrusted service provider should gain no information about client data or access patterns.
Thus, hiding the content of data through encryption alone is not enough.
In addition, no information should be leaked about: 1) which data is being accessed; 2) whether the access is a read or a write; 3) when the data was last accessed; 4) whether the same data is being accessed; or 5) the access pattern (sequential, random, etc).
Although proven to be cryptographically safe, existing ORAM-based systems have high performance overhead, making it too costly to be adopted in many datacenter environments.
We now present our improved ORAM system design.
Our system is based on Path ORAM [49] but can significantly outperform Path ORAM.We propose to leverage one-sided communication to hide data access information and to replace (costly) ORAM operations with one-sided operations, thereby improving the performance of ORAM.
Although the basic idea is simple, one needs to take extra care when applying it to provide the same level of security guarantees as ORAM.
For example, a receiver (the malicious service provider) cannot detect when a one-sided write happens.
But it can take snapshots of data content periodically and compare two snapshots to detect modifications in between.
It can then infer that a write happens by performing frequent snapshotting.
Thus, one-sided writes can still leak information to the service provider.
Because of this, we use the unmodified ORAM write protocol and only improve ORAM read performance.Reading data using one-sided communication can be completely invisible to the service provider, even if the provider performs snapshotting.
For a read-only storage service, we can safely replace all reads with one-sided communication.
However, in practice, most storage services are not read-only and we need to deliver the second guarantee in Section 5.1 -server should not be able to tell a read from a write.We use a simple technique to achieve this security goal.
By default, we perform one-sided reads for client read requests but randomly choose a certain amount of client read requests (e.g., X% of all read requests) to perform original ORAM operations.
These ORAM-based read operations cannot be distinguished from write operations.
Therefore, statistically, we can deliver all the security guarantees.
Meanwhile, performing a one-sided read is significantly faster than performing an ORAM operation, since the latter requires read and write of a whole path, while the former only performs a read to one object.
Although the algorithmic complexity of the modified Path ORAM is still identical to the original Path ORAM, the performance of our improved ORAM read is significantly better than the original Path ORAM.We implemented the original Path ORAM protocol and our modified read mechanism using RDMA and tested their performance with two network settings, a 40 Gbps InfiniBand network and a 100 Gbps InfiniBand network.
Figure 3 presents our modified read performance when changing X% from 10% to 100% (under 100%, our system falls back to original Path ORAM).
Here, we use a pure-read workload in the YCSB key-value store benchmark [7,23], with 32,000 512-byte key-value pairs.
We encrypt all data with AES256.
With 50% one-sided reads, we achieve around 2Ã— performance of pure Path ORAM.
One limitation of our one-sided ORAM solution is the requirement of prohibiting network packet sniffing.
As described in Section 3, most systems enable this prohibition by default.
However, if an attacker can bypass such prohibition mechanisms (e.g., when it controls a physical machine), it can observe one-sided traffic.
Thus, our threat model applies only to other cases, for example, when the attacker only owns a virtual machine.One-sided NICs usually writes to memory through DMA.
There are methods for a server to track (all) DMA activities (e.g., with processor counter monitor [21]).
However, it is still hard to pinpoint one-sided traffic, since other types of DMA activities may be happening at the same time.
Even if a server can detect one-sided network traffic, it is still challenging to tell whom the sender is and which data is being accessed.Our current design does not consider parallel client accesses to the secure data store, i.e., we only support either a single proxy that delivers all client requests to the data store or a single client that talks to the data store directly.
Supporting parallel client accesses to a secure data store is an important goal in many distributed data store systems [8,44,49,60].
Our design presented here serves as a building block in developing a fully distributed ORAM system.
This paper provides the first and initial look into the security aspect of one-sided network communication.
We demonstrate several vulnerabilities that are rooted in one-sided communication's basic design philosophies.
Although we have not built real attacks that exploit these vulnerabilities yet, our findings can be a starting point to explore potential attacks and defenses for future researchers and practitioners.This work is a warning for future one-sided hardware vendors, software developers who want to use one-sided communication, and datacenters that have or plan to deploy one-sided network systems.
We believe that these three parties should work together to achieve the security goals in datacenters while preserving one-sided communication's performance and cost benefits.
One promising direction is to leverage programmable network devices like SmartNIC [32,33] and programmable switches [22,27,28] to implement security defenses in hardware.Although adding security guarantees to existing one-sided communication technologies is not an easy job, we do not believe the future of one-sided communication to be diminishing.
For all the vulnerabilities that we have identified in this work, there are defense mechanisms that could potentially work.
Moreover, one-sided communication provides a great opportunity to improve privacy, the security property that is increasingly important in today's cloud environments.
