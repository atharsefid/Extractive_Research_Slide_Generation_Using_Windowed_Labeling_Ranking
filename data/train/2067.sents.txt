While it is compelling to process large streams of IoT data on the cloud edge, doing so exposes the data to a sophisticated, vulnerable software stack on the edge and hence security threats.
To this end, we advocate isolating the data and its computations in a trusted execution environment (TEE) on the edge, shielding them from the remaining edge software stack which we deem untrusted.
This approach faces two major challenges: (1) executing high-throughput, low-delay stream analytics in a single TEE, which is constrained by a low trusted computing base (TCB) and limited physical memory; (2) verifying execution of stream analytics as the execution involves untrusted software components on the edge.
In response, we present StreamBox-TZ (SBT), a stream analytics engine for an edge platform that offers strong data security, verifiable results, and good performance.
SBT contributes a data plane designed and optimized for a TEE based on ARM TrustZone.
It supports continuous remote attestation for analytics correctness and result freshness while incurring low overhead.
SBT only adds 42.5 KB executable to the TCB (16% of the entire TCB).
On an octa core ARMv8 platform, it delivers the state-of-the-art performance by processing input events up to 140 MB/sec (12M events/sec) with sub-second delay.
The overhead incurred by SBT's security mechanism is less than 25%.
Many key applications of Internet of Things (IoT) process a large influx of sensor 1 data, i.e. telemetry.
Smart grid aggregates power telemetry to detect supply/demand imbalance and power disturbances [76], where a power sensor is reported to produce up to 140 million samples per day [16,17]; oil producers monitor pump pressure, tank status, and fluid temperatures to determine if wells work at ideal operating points [55,60], where an oil rig is reported to produce 1-2 TB of data per 1 Recognizing that IoT data sources range from small sensors to large equipment, we refer to them all as sensors for brevity.
Commodity libs + OS Figure 1: An overview of StreamBox-TZ day [43]; manufacturers continuously monitor vibration and ultrasonic energy of industrial equipment for detecting equipment anomaly and predictive maintenance [104,120], where a monitored machine is reported to generate PBs of data in a few days [77].
The large telemetry data streams must be processed in time.
The high cost and long delay in transmitting data necessitate edge processing [98,100]: sensors send the data to nearby gateways dubbed "cloud edge"; the edge runs a pipeline of continuous computations to cleanse and summarize the telemetry data and reports the results to cloud servers for deeper analysis.
Edge hardware is often optimized for cost and efficiency.
According to a 2018 survey [45], modern ARM machines are typical choices for edge platforms.
Such a platform often has 2-8 CPU cores and several GB DRAM.Unfortunately, edge processing exposes IoT data to high security threats.
i) Deployed in the wild, the edge suffers from common IoT weaknesses, including lack of professional supervision [58,118], weak configurations [108,117], and long delays in receiving security updates [58,114].
ii) On the edge, the IoT data flows through a set of sophisticated components that expose a wide attack surface.
These components include a commodity OS (e.g. Linux or Windows), a variety of user libraries, and a runtime framework called stream analytics engine [37,42,83].
They reuse much code developed for servers and workstations.
Their exploitable mis-configurations [121] and vulnerabilities [23,35,109] are not uncommon.
iii) With data aggregated from multiple sources, the edge is a high-value target to adversaries.
For these reasons, edge is even more vulnerable than sensors, which run much simpler software with narrower attack surfaces.
Once attackers compromise the edge, they not only access confidential data but also may delete or fabricate data sent to the cloud, threatening the integrity of an entire IoT deployment.Towards secure stream analytics on an edge platform, our goal is to safeguard IoT data confidentiality and integrity, support verifiable results, and ensure high throughput with low output delay.
Following the principle of least privilege [95], we protect the analytics data and computations in a trusted execution environment (TEE) and limit their interface; we leave out the remaining edge software stack which we deem untrusted.
By doing so, we shrink the trusted computing base (TCB) to only the protected functionalities, the TEE, and the hardware.
We hence significantly enhance data security.We face three challenges: i) what functionalities should be protected in TEE and behind what interfaces?
ii) how to execute stream analytics on a TEE's low TCB and limited physical memory while still delivering high throughput and low delay?
iii) as both trusted and untrusted edge components participate in stream analytics, how to verify the outcome?Existing solutions are inadequate: pulling entire stream analytics engines to TEE [22,27,112] would result in a large TCB with a wide attack surface; the systems securing distributed operators [53,99,124] often lack stream semantics or optimizations for efficient execution in a single TEE, which are crucial to the edge; only attesting TEE integrity [65] or data lineages [50,99,102,124] is inadequate for verifying stream analytics.
We will show more evidences in the paper.Our response is StreamBox-TZ (SBT), a secure engine for analyzing telemetry data streams.
As shown in Figure 1, SBT builds on ARM TrustZone [2] on an edge platform.
SBT contributes the following notable designs: (1) Architecting a data plane for protection SBT provides a data plane exposing narrow, shared-nothing interfaces to untrusted software.
SBT's data plane encloses i) all the analytics data; ii) a new library of low-level stream algorithms called trusted primitives as the only allowed computations on the data; iii) key runtime functions, including memory management and cache-coherent parallel execution of trusted primitives.
SBT leaves thread scheduling and synchronization out of TEE.
(2) Optimizing data plane performance within a TEE In contrast to many TEE-oblivious stream engines that operate numerous small objects, hash tables, and generic memory allocators [32,82,122], SBT embraces unconventional design decisions for its data plane.
i) SBT implements trusted primitives with array-based algorithms and contributes new optimizations with handwritten ARMv8 vector instructions.
ii) To process high-velocity data in TEE, SBT provides a new abstraction called uArrays, which are contiguous, virtually unbounded buffers for encapsulating all the analytics data; SBT backs uArrays with on-demand paging in TEE and manages uArrays with a specialized allocator.
The allocator leverages hints from untrusted software for compacting memory layout.
iii) SBT exploits TrustZone's lesser-explored hardware features: ingesting data straightly through trusted IO without a detour through the untrusted OS; avoiding relocating streaming data by leveraging the large virtual address space dedicated to a TEE.
(3) Verifying edge analytics execution SBT supports cloud verifiers to attest analytics correctness, result freshness, and the untrusted hints received during execution.
SBT captures coarse-grained dataflows and generates audit records.
A cloud verifier replays the audit records for attestation.
To minimize overhead in the edge-cloud uplink bandwidth, SBT compresses the records with domain-specific encoding.Our implementation of SBT supports a generic stream model [1] with a broad arsenal of stream operators.
The TCB of SBT contains as little as 267.5 KB of executable code, of which SBT only constitutes 16%.
On an octa core ARMv8 platform, SBT processes up to 12M events (144 MB) per second at sub-second output delays.
Its throughput on this platform is an order of magnitude higher than an SGX-based secure stream engine running on a small x86 cluster with richer hardware resources [53].
The security mechanisms contributed by SBT incur less than 25% throughput loss with the same output delay; decrypting ingress data, when needed, incurs 4%-35% throughput loss with the same output delay.
While sustaining high throughput, SBT uses up to 130 MB of physical memory in most benchmarks.The key contributions of SBT are: i) a stream engine architecture with strongly isolated data and a lean TCB; ii) a data plane built from the ground up with computations and memory management optimized for a single TrustZone-based TEE; iii) remote attestation for stream analytics on the edge with domain-specific compression of audit records.
To our knowledge, SBT is the first system designed and optimized for data-intensive, parallel computations inside ARM TrustZone.
Beyond stream analytics, the SBT architecture should help secure other important analytics on the edge, e.g. machine learning inference.
The SBT source can be found at http://xsel.rocks/p/streambox.
As typical hardware for IoT gateways [45], recent ARM platforms offer competitive performance at low power, suiting edge well.
Most modern ARM cores are equipped with TrustZone [2], a security extension for TEE enforcement.
TrustZone logically partitions a platform's hardware resources, e.g. DRAM and IO, into a normal (insecure) and a secure world.
CPU cores independently switch between two worlds.
A TEE Trusted IO is a unique feature of ARM TrustZone, implemented through hardware components including TrustZone Address Space Controller (TZASC) and TrustZone Protection Controller (TZPC).
TZASC allows privilege software to logically partition DRAM between the normal and the secure worlds.
Similarly, TZPC allows to configure IO peripherals accessible to either world.
Any peripheral owned by the secure world is completely enclosed in the secure world.
We use trusted IO to support the trusted source-edge links on the cloud edge ( §3.1).
Stream Model We target stream analytics over sensor data.
A data stream consists of sensor events that carry timestamps defined by event occurrence, as illustrated in Figure 2(a).
Programmers specify a pipeline of continuous computations called operators, e.g. Select and GroupBy, that are extensively used for telemetry analytics [62,90].
As data arrives at the edge, a stream analytics engine ingests the data at the pipeline ingress, pushes the data through the pipeline, and externalizes the results at the pipeline egress.We follow a generic stream model [14,32,69,85,122].
Operators execute on event-time scopes called windows.
Data sources emit special events called watermarks.
A watermark guarantees no subsequent events in the stream will have event times earlier than the watermark timestamp.
A pipeline's output delay is defined as the elapsed time starting from the moment the ingress receives the watermark signaling the completion of the current window to the moment the egress externalizes the window results [82].
A pipeline may maintain its internal states organized by windows at different operators.
See prior work [20] for a formal stream model.
shows an example derived from an IoT scenario [62]: it predicts future household power loads based on power loads reported by smart power plugs.
The example pipeline ingests a stream of power samples and groups them by 1-second fixed windows and by houses.
For each house in each window, it aggregates all the loads and predicts the next-window load as an exponentially weighted moving average over the recent windows.
At the egress, the pipeline emits a stream of per-house load prediction for each window.Stream analytics engines Stream pipelines are executed by a runtime framework called a stream analytics engine [37,42,46,82,83,90].
A stream analytics engine consists of two types of function: data functions for data move and computations; control functions for resource management and computation orchestration, e.g. creating and scheduling tasks.
The boundary between the two is often blurry.
To amortize overheads, control functions often organize data in batches and invoke data functions to operate on the batches.
The edge faces common threats in IoT deployment.
i) IT expertise is weak.
Edge platforms are likely managed by field experts [58,114,118] rather than IT experts.
Such lack of professional supervision is known to result in weak configurations [108,117].
ii) The infrastructure is weak.
Deployed in the field, the edge often sees slow uplinks [84,114] and hence much delayed software security updates.
For cost saving, edge analytics may need to share OS and hardware with other highrisk, untrusted software such as web browsers [114].
Besides the common threats, existing edge software stacks entrust IoT data with commodity OSes, analytics engines, and language runtimes (e.g. JVM).
However, these components are incapable of offering strong security guarantees due to their complexity and wide interfaces.
Each of them easily contains more than several hundreds of KSLoC [116].
Exploitable vulnerabilities are constantly discovered [3,6,23,35,38], making these components untrusted in recent research [36,54,79,80].
By exploiting these vulnerabilities, a local adversary as an edge user program may compromise the kernel through the wide user/kernel interfaces [11,12] or attack an analytics engine through IPC [7]; a remote adversary, through the edge's network services, may compromise analytics engines [4] or the OS [10].
A successful adversary may expose IoT data, corrupt the data, or covertly manipulate the data.
Taking the application in Figure 2(b) as an example, the adversary gains access to the smart plug readings, which may contain residents' private information, and injects fabricated data.Objectives We aim three objectives for stream analytics over telemetry data on an edge platform: i) confidentiality and integrity of IoT data, raw or derived; ii) verifiable correctness and freshness of the analytics results; iii) modest security overhead and good performance.
IoT scenarios We target an edge platform that captures and analyzes telemetry data.
We recognize the significance of mission-critical IoT with tight control loops, but do not target it.
Our target scenario includes source sensors, edge platforms, and a cloud server which we dub "cloud consumer".
All the raw IoT data and analytics results are owned by one party.
The sensors produce trusted events, e.g. by using secure sensing techniques [49,73,97].
The cloud consumer is trusted; it installs analytics pipelines to the edge and consumes the results uploaded from the edge.
We consider untrusted source-edge links (e.g. public networks) which requires data encryption by the source, as well as trusted source-edge links (e.g. direct IO bus or on-premise local networks), and will evaluate the corresponding designs ( §9).
We assume untrusted edge-cloud links, which require encryption of the uploaded data.In-scope Threats We consider malicious adversaries interested in learning IoT data, tampering with edge processing outcome, or obstructing processing progress.
We assume powerful adversaries: by exploiting weak configurations or bugs in the edge software, they already control the entire OS and all applications on the edge.Out-of-scope Threats We do not protect the confidentiality of stream pipelines, in the interest of including only low-level compute primitives in a lean TCB.
We do not defend the following attacks.
i) Attacks to non-edge components assumed trusted above, e.g. sensors [111].
ii) Exploitation of TEE kernel bugs [8,9,56].
iii) Side channel attacks: by observing hardware usage outside TEE, adversaries may learn the properties of protected data, e.g. key skew [72].
Note that controlled-channel attack [119] cannot be applied to ARM TrustZone as it has separate page management within a separate secure OS unlike Intel SGX.
iv) Physical attacks, e.g. sniffing TEE's DRAM access [18,28].
Many of these attacks are mitigated by prior work [39,66,123,124] orthogonal to SBT.Note that TEE code authenticity and integrity are already ensured by the TrustZone hardware, i.e. only code trusted by the device vendor can run in TrustZone and its integrity is protected by TrustZone.
As shown in Figure 3, SBT protects its data functions in a trusted data plane in TEE.
SBT runs its untrusted control plane in the normal world.
The control plane invokes the data plane through narrow, shared-nothing interfaces.
The engine's TCB thus only consists of the TEE (including the data plane) and the hardware.
Streaming data always flows in TEE.
The data plane ingests the data through TrustZone's trusted IO.
After ingestion, it returns opaque references of the data batches to the control plane.
In turn, the control plane requests computations on the protected data by invoking the data plane with the opaque references.
The data plane generates opaque references as long, random integers.
It tracks all live opaque references, validates incoming opaque references, and only accepts ones that exist.
At the pipeline egress, the data plane encrypts, signs, and sends the result to the cloud.The analytics execution is continuously attested.
SBT captures complete and deterministic dataflows of the stream analytics as well as execution timing, and periodically reports to the cloud server.
The cloud server verifies if all ingested data is processed according to the pipeline (correctness), and if the edge incurs low delay (freshness).
Thwarted attacks SBT defeats the following attacks.
i) Breaking IoT data confidentiality or integrity.
As the raw and derived data enters and leaves the edge TEE through trusted IOs, adversaries on the edge cannot touch, drop, or inject data.
When the data is off the edge transmitted over untrusted networks, it is protected by encryption against networklevel adversaries.
ii) Breaking the data plane integrity.
Any fabricated opaque reference passed to the data plane will be rejected, since all opaque references are validated before use.
Through the data plane's interface, an adversary may exploit bugs in the data plane and compromise it.
By minimizing the date plane codebase and hardening its interface, SBT substantially reduces the data plane's attack surface and potential bugs that can be exploited.
iii) Breaking analytics correctness.
A compromised control plane may request computations deviating from pipeline declarations or the stream model.
For instance, it may invoke trusted computations on partial data, wrong windows, or valid but undesirable opaque references.
SBT defeats these attacks through attestation: since the cloud verifier possesses complete knowledge on ingested data and Table 2: Selected trusted primitives (23 in total) and operators they constitute.
These operators cover most listed in the Spark Streaming documentation [103].
pipelines, it detects such correctness violation and rejects the edge analytics results.
iv) Attacks on analytics performance or availability.
A compromised control plane may delay or pause invoking of trusted computations, violating the freshness guarantee.
As the execution timing of trusted computations is attested, the cloud verifier detects the attacks and can choose to prompt further investigation.
v) Attempting to trigger data race or deadlock.
By design, data race and deadlocks will never happen inside the data plane: the trusted computations do not share state concurrently and all locking happens outside of the TEE.
Our approach raises three challenges.
i) Architecting the engine with a proper protection boundary.
This hinges on a key trade-off among TEE functional richness, overhead of TEE entry/exit, and TCB size.
ii) Optimizing data functions within a TEE.
Processing of high-velocity data in a TEE strongly favors simple algorithms and compact memory.
Yet, existing stream engines often operate numerous short-lived objects indexed in hash tables or trees [32,69,82,90,122], e.g. for grouping events by key.
They manage these objects with generic memory allocators [82] or garbage collectors [87,122].
Such designs poorly fit a TEE's small TCB and limited DRAM portion, e.g. typically tens of MB for TrustZone TEE and up to 128 MB for Intel SGX enclave [31].
iii) Verifying stream analytics results.
This requires to track unbounded data flows in stream pipelines, validate if operators respect the temporal properties, e.g. windows, and minimize the resultant overhead in execution and communication.Why are existing systems inadequate?
First, many TEEbased systems [22,27,112] pull entire user applications and libraries to the TCB, as shown in Figure 4(a).
However, as we described in Section 2.2, a modern analytics engine and its libraries are large, complex, and potentially vulnerable.
Second, partitioning applications to suit a TEE, as shown in [71,93,101], is unsuitable for existing stream engines: partitioning does not change their hash-based data structures and algorithms, which by design mismatch a TEE.
Similarly, recent secure processing engines disfavor partitioning [89,91].
Third, recent systems use TEE to protect data in analytics or in network packet processing.
As summarized in Table 1, they lack support for stream analytics, key computation optimizations, or specialized memory allocation, which we will demonstrate as vital to our objective.Attesting TEE integrity [65,91] is insufficient to assert analytics correctness.
VC3 [99] and Opaque [124] verify correctness of batch analytics by checking the history of compute results, i.e. their data lineage [50,102].
Without tracking data being continuously ingested and lacking a stream model, data lineages cannot assert whether all ingested data is processed according to pipeline declarations, watermarks, and temporal windows, which are critical to stream analytics.
SBT builds on TrustZone [2] due to ARM's popularity for the edge and trusted IO benefiting stream analytics ( §2).
Programmability Programming SBT is similar to programming commodity engines such as Spark Streaming [122] and Flink [19].
Analytics programmers assemble pipelines with high-level, declarative operators as exemplified in Figure 2(c).
SBT provides most of the common operators offered by commodity engines, as summarized in Table 2.
These stream operators are widely used for analytics over telemetry data [62,90].
SBT supports User Defined Functions (UDFs) that are certified by a trusted party, which is a common requirement in TEE-based systems [91].
SBT architecture As shown in Figure 3, SBT's data plane incarnates as a TrustZone module.
SBT runs its control plane as a parallel runtime in the normal world.
The control plane invokes the data plane through a narrow interface (details in Section 9).
The control plane orchestrates the execution of analytics pipelines.
It creates plentiful parallelism among and within operators.
It elastically maps the parallelism to a pool of threads it maintains.
At a given moment, all threads may simultaneously execute one operator as well as different operators over different data.
Data plane & design choices SBT's data plane consists of only the trusted primitives and a runtime for them.
i) Trusted primitives are stateless, single-threaded functions that are oblivious to synchronization.
We do not enclose whole stream pipelines in the data plane, because a stream pipeline must be scheduled dynamically for parallelism and handling high-velocity data.
We do not enclose whole declarative operators in the data plane, because one operator instance has internal thread-level parallelism and hence requires thread management logic.
Our choice keeps the data plane lean, leaving out all control functions including scheduling and threading.
This contrasts to many other engines pulling whole analytics to TEE as shown in Table 1.
Although exporting low-level primitives entails more TEE switches, the costs are lower on modern ARM [25,56] and can be amortized by data batching, as will be discussed soon.ii) The data plane incorporates minimum runtime functions: memory management and paging, which are critical to TEE integrity; cache coherence of parallel primitives, which is critical to parallelism.
The data plane is agnostic to declarative operators and pipelines being executed.For attestation, the data plane generates audit records on data ingress/egress, watermarks, and primitive executions.
It reduces overhead via data batching and record compression.
Coping with secure memory shortage When compute cost or data ingestion rate is high, SBT may run short of secure memory.
To avoid data loss in such a situation, SBT adds backpressure to source sensors, slowing down data ingestion.
In the current implementation, SBT triggers backpressure when ingestion exceeds a user-defined threshold; we leave as future work automatic flow control, i.e. tuning the threshold online per available secure memory and backlog.
Parallel execution inside a TEE SBT exploits task parallelism without bloating the TEE with a threading library.
The control plane invokes multiple primitives from multiple worker threads, which then enter the TEE to execute the primitives in parallel.
All trusted primitives share one cachecoherent memory address space in TEE, which simplifies data sharing and avoids copy cost.
This contrasts to existing secure analytics engines that leave task parallelism untapped in a single TEE [53,99].
Array-based algorithms to suit TEE Unlike many popular stream engines using hash-based algorithms for lower algorithmic complexity, we make a new design decision.
We strongly favor algorithms with simple logic and low memory overhead, despite that they may incur higher algorithmic complexity.
Corresponding to contiguous arrays as the universal data containers in TEE, most primitives use sequential-access algorithms over contiguous arrays, e.g. executing Merge-Sort over event arrays and scanning the resultant array to calculate the average value per key.Trusted primitives and vectorization SBT's trusted primitives are generic.
They constitute most declarative stream operators, often referred to as Select-Projection-Join-GroupBy (SPJG) families, shown in Table 2.
These operators are considered representative in prior research [44].
To speed up the array-based algorithms inside TEE without TCB bloat, our insight is to map their internal data parallelism to vector instructions of ARM [21].
Despite their well-known performance benefit, vector instructions are rarely used to accelerate data analytics within TEEs, to our knowledge.
Vectorization incurs low code complexity as the performance gain comes from a CPU feature that is already part of the TCB.Our optimization focuses on Sort and Merge, two core primitives that dominate the execution of stream analytics according to our observation.
Inspired by vectorized sort and merge on x86 [26,64], we build new implementations for SBT by hand-writing ARMv8 NEON vector instructions.
Our sort outperforms the ones in the C/C++ standard libraries by more than 2×, as will be shown in evaluation.
This optimization is crucial to the overall engine performance.
Facing high-velocity streams in a TEE, SBT's memory allocator addresses two challenges: space efficiency: it must create compact memory layout and reclaim memory timely due to limited physical memory; lightweight: the allocator must be simple to suit a low TCB.
The challenges disqualify popular engines that organize events in hash tables (e.g. for grouping events by key) and rely on generic memory allocators [32,69,82,90,122].
The reasons are two: a hash table's principle of trading space for time mismatches TEE's limited memory; generic allocators often feature sophisticated optimizations, adding tens of KSLoC to TCB [41,59].
SBT specializes memory management for stream computations: it supports unbounded buffers as the universal memory abstraction ( §6.1); it places data by using (untrusted) consumption hints and large virtual address space ( §6.2).
We devise contiguous, virtually unbounded arrays called uArrays, a new abstraction as the universal data containers used by computations in TEE.
uArrays encapsulate all the data in a pipeline, including data flowing among trusted primitives as well as operator states traditionally kept in hash tables.An uArray is an append-only buffer in a contiguous memory region for same-type data objects.
Their lifecycles closely map to the producer/consumer pattern in streaming computations.
One uArray can be in three states.
Open: after an uArray is created, it dynamically grows as the producer primitive appends data objects to it.
Produced: the data production completes and the end position of the uArray is finalized.
uArray becomes read-only and no data can be appended.
Retired: the uArray is no longer needed and its memory is subject to reclamation.
The memory allocator places and reclaims uArrays regarding their states, as will be discussed in Section 6.2.
Types uArrays fall into different types depending on their scopes and enclosed data.
A streaming uArray encapsulates data flowing from a producer primitive to a consumer primitive.
A state uArray encapsulates operator state that outlives the lifespans of individual primitives.
A temporary uArray live within a trusted primitive's scope.Low abstraction overhead An uArray spans a contiguous virtual memory region and grows transparently.
The growth is backed by the data plane's on-demand paging that completely happens in the TEE.
For most of the time, growing an uArray only requires updating an integer index.
Compared to manually managed buffers, this mechanism waives bounds checking of uArray in computation code and hence allows the compiler to generate more compact loops.
uArrays always grow in place.
This contrasts to common sequence containers (e.g. C++ std::vector and java.util.ArrayList) that grow transparently but require expensive relocation.
We will experimentally compare uArray with std::vector in Section 9.
Co-locating uArrays The memory allocator co-locates multiple uArrays as a uGroup in order to reclaim them consecutively.
Spanning a contiguous virtual memory region, a uGroup consists of multiple produced or retired uArrays and optionally an open uArray at its end, as shown in Figure 5.
The grouping is purely physical: it is at the discretion of the allocator, orthogonal to stream computations, and therefore transparent to the trusted primitives and the control plane.
With the grouping, the allocator reclaims consumed uArrays by always starting from the beginning of an uGroup, as shown in Figure 5.
To place a new uArray, the allocator decides whether to create a new uGroup for the uArray, or append the uArray to an existing uGroup.
In doing so, the allocator seeks to i) ensure that each uGroup holds a sequence of uArrays to be consumed consecutively in the future; ii) minimize the total number of live uGroups, in order to compact TEE memory layout and minimizes the cost in tracking uGroups.
To this end, our key is to guide placement with the control plane's data consumption plan, as will be presented below.Consumption hints Upon invoking a trusted primitive T , the control plane may provide two optional hints concerning the future consumption order for the output of T :• Consumed-in-parallel ( k ): the control plane will schedule k worker threads to consume a set of uArrays in parallel.
• Consumed-after (b 1 ⇐b 2 ): the control plane will schedule worker threads for consuming uArray b 2 after uArray b 1 .
The consumed-after relation is transitive.
uArrays may form multiple consumed-after chains.The control plane may specify these relations between new output uArrays (yet to be created) and existing uArrays.Hint-guided placement The hints assist the data plane to generate compact memory layout and reclaim memory effectively.
Upon allocating a uArray, the allocator examines the existing hints regarding to the uArray.
(⇐) prompts the allocator to place the uArrays on the same consumed-after chain in the same uGroup.
Starting from the new uArray b under question, the allocator tracks back on its consumed-after chain, and places b after the first uArray that is both in state produced (i.e. its growth has finished) and is located at the end of an uGroup.
If no such uArray is available on the chain, the allocator creates a new uGroup for b. ( k ) prompts the allocator to place uArrays b 1.
.
k in separate uGroups, so that delay in consuming any of the uArrays will not block the allocator from reclaiming the other uArrays.
Our rationale is that despite b 1.
.
k are created at the same time, they are often consumed at different moments in the future: i) since SBT's control plane threads independently fetch new uArrays for processing as they become available ( §4), the starting moments for processing b 1.
.
k may vary widely, especially when the engine load is high; ii) even when k worker threads start processing b 1.
.
k simultaneously, straggling workers are not uncommon, due to non-determinism of a modern multicore's thread scheduling and memory hierarchy [24].
The impacts of misleading hints SBT detects misleading hints in retrospect through remote attestation ( §7).
As the hints only affect TEE memory placement policy on the edge, misleading hints never result in data loss ( §4.2) or violation of data security and TEE integrity.
Yet, such hints may slow down analytics and therefore violate result freshness.Managing virtual addresses All uGroups grow in place within one virtual address space.
To avoid collision and expensive relocation, the allocator places them far apart by leveraging the large virtual address space dedicated to a TrustZone TEE.
The space is 256TB on ARMv8, 10,000× larger than the physical DRAM (a few GBs).
Hence, the allocator simply reserves for each uGroup a virtual address range as large as the total TEE DRAM.
We will validate this choice in Section 9.
SBT collects evidences for cloud consumers to verify two properties: correctness, i.e. all ingested data is processed ac- [50,102] is insufficient to guarantee correctness, i.e. all data ingested so far is processed according to the stream pipeline.
iii) The windows of stream computations and watermarks triggering the computations must be attested, which are keys to stream model ( §2).
iv) As the volume of evidences can be substantial, evidences must be compacted to save uplink bandwidth [84,114].
Therefore, SBT provides the following verification mechanism.
Agnostic to the pipeline being executed, the data plane monitors dataflows among primitive instances at the TEE boundary, and then generates audit records.
For low overhead, it eschews building data lineages on-the-fly unlike much prior work [50,74,99].
The data plane compresses audit records and flushes to the cloud both periodically and upon externalizing any analytics result.
We describe details below.Audit records As being invoked by the control plane, the data plane generates audit records.
As illustrated in Figure 6, the records track i) ingested and externalized uArrays, ii) associations between uArrays and windows, and iii) primitive executions (with optional hints supplied by the control plane) which establish derived-from relations among uArrays.
The records further include ingested watermark values, which are crucial for determining output delays as will be discussed below.
The data plane timestamps all the records.
It generates monotonically increasing identifiers for recorded uArrays.
We will evaluate the overhead of audit records in Section 9.
Attesting analytics correctness The cloud verifier checks if all ingested uArrays flow through the expected trusted primitives.
Such dataflows are deterministic given the arrivals of input data (including their windows), the watermarks, and the pipeline declaration.
Hence, the verifier replays all ingestion records on its local copy of the same pipeline.
It checks if all the records resulting from the replay match the ones reported by the edge (except timestamps).
The replay is symbolic without actual computations and hence fast.Note that the verification works for stateful operators as well.
The state of a stream operator (e.g. temporal join) is only determined by all the inputs the operator has ever received.Since the cloud can verify that all the ingested uArrays correctly flow through the expected trusted primitives and thus stream operators, it knows that the operator's current state must be correct, and then all results derived from the operator state must be correct.
The key for the verifier to calculate the delay of an output result R is to identify the watermark that triggers the externalization of R, according to the delay definition in Section 2.2.
From the egress record of R, the verifier traces backward following the derived-from chain(s) until it reaches an execution record indicating that a watermark W triggers the execution.
The verifier looks up the ingress record of W .
It calculates the difference between W 's ingress time and R's egress time to be the delay of R.Example In Listing 1, an uArray with identifier 0xF0 is ingested and segmented into two uArrays (0xF1 and 0xF2) for window 0 and 1 respectively.
Sort consumes uArray 0xF1 and produces uArray 0xF3.
A watermark with value 100 arrives and completes window 0.
Triggered by the watermark, SUM consumes uArray 0xF3 of window 0 and produces uArray 0xF5 as the result of window 0.
ts = 1 INGRESS data =0 xF0 ts = 5 WND data_in =0 xF0 win_no =0 data_out =0 xF1 ts =10 SORT data_in =0 xF1 data_out =0 xF3 ts =15 INGRESS data =0 xF4 ( watermark =100) ts =25 SUM data_in =0 xF3 ,0 xF4 data_out =0 xF5 ts =28 WND data_in =0 xF0 win_no =1 data_out =0 xF6 ts =30 EGRESS data =0 xF5Listing 1: Sample audit records for the pipeline in Figure 2.
Format is simplified.
ts means processing timestamp.The cloud verifier replays the ingress records on its local pipeline copy and learns that uArray 0xF1 is processed adhering to the pipeline declaration while uArray 0xF2 is yet to be processed.
It will assert analytics incorrectness if 0xF2 remains unprocessed until a future watermark completes window 1 (not shown).
To verify result freshness, the verifier traces result 0xF5 backward to find its trigger watermark 0xF4 and calculates the output delay to be 15 (30 − 15).
Columnar compression of records The data plane compresses audit records by exploiting locality within one record field and known data distribution in each field.
The data plane produces raw audit records in memory (with the format shown in Figure 6) and in a row order, i.e. one record after the other.
Before uploading a sequence of records, it separates the record fields (i.e. columns) and applies different encoding schemes to individual columns: i) Huffman encoding for primitive types and data counts, the two columns likely contain skewed values; ii) delta encoding for timestamps, uArray identifiers, and window numbers, which increment monotonically.
Our compression is inspired by columnar databases [107].
We will evaluate the efficacy of compression in Section 9.
We build SBT for ARMv8 and atop OP-TEE [70] (v2.3).
SBT reuses most control functions of StreamBox [82], an open-source research stream engine for x86 servers.
Yet, as StreamBox mismatches a TEE ( §4.1), SBT contributes a new architecture and a new data plane.
SBT communicates with source sensors and cloud consumers over ZeroMQ TCP transport [57] which is known for good performance.
The new implementation of SBT includes 12.4K SLoC.Input batch size, a key parameter of SBT, trades off between delays in executing individual primitives, the rate of TEE entry/exit, and attestation cost.
We empirically determine it as 100K events and will evaluate its impact ( §9).
Opaque references for uArrays are 64-bit random integers generated by the data plane.
It keeps the mappings from references to uArray addresses in a table, and validates opaque references by table lookup.
This incurs minor overhead, as live opaque references are often no more than a few thousands.
We answer the following questions through evaluation:• Does SBT result in a small TCB?
( §9.1)• What is SBT's performance and how is it compared to other engines?
What is the overhead?
( §9.2) • How do our key designs impact performance ( §9.3)?
9.1 TCB Analysis TCB size Table 4 shows a breakdown of the SBT source code.
Despite a sophisticated control plane, the data plane only adds 5K SLoC to the TCB.
SBT's memory management is in 740 SLoC, 9× fewer than glibc's malloc and 20× fewer than jemalloc [41].
The size of data plane is 42.5 KB, a small fraction (16%) of the entire OP-TEE binary.TCB interface The SBT's data plane exports only four entry functions: two for data plane initialization/finalization, one for debugging, and one shared by all 23 trusted primitives.
The last function accepts and returns opaque references ( §4).
No state is shared across the protection boundary.Comparison with alternative TCBs Compared to enclosing whole applications in TCB [22,27,112], SBT keeps most of the engine out, shrinking the TCB by at least one order of magnitude.
Compared to directly carving out [71,93] the original StreamBox's data functions for protection, SBT completely avoids sophisticated data structures (e.g. AtomicHashMap [47] used by StreamBox) that mismatch TCB.
Compared to VC3 [99] that implements Map/Reduce operators in a TCB with ∼9K SLoC, SBT supports much richer stream operators within a 2× smaller TCB.
Methodology We evaluate SBT on a HiKey board as summarized in Table 3.
We chose HiKey for its good OP-TEE support [70] and that it is among the few boards with TrustZone programmable by third parties.
We built Generator, a program sends data streams over ZeroMQ TCP transport [57] to SBT.
We run the cloud consumer on an x86 machine.
Data streams are encrypted with 128-bit AES.In the face of HiKey's platform limitations, we set up the engine ingestion as follows.
i) Although Gigabit Ethernet on edge platforms is common [5,88], Hikey's Ethernet interface (over USB) only has 20MB/sec bandwidth.
We have verified that the interface is saturated by SBT with 4 cores.
Hence, we report performance when SBT and Generator both run on HiKey communicating over ZeroMQ TCP, which still fully exercise the TCP/IP stack and data copy.
ii) Although HiKey's TEE is capable of directly operating Ethernet interface as trusted IO, our OP-TEE version lacks the needed drivers.
Hence, we emulate SBT's direct data ingestion to TEE by running the ingestion in a privileged process in the normal world, and bypassing data copy across the TEE boundary.
Our test harness continuously replays pre-allocated secure memory buffers populated with events.As summarized in Table 5, we test SBT as well as three modified versions: SBT ClearIngress ingests data in cleartext; this is allowed if source-edge links are trusted as defined in our threat model ( §3).
SBT IOviaOS does not exploit TrustZone's trusted IO: the untrusted OS ingested (encrypted) data and copies the data across TEE boundary to the data plane.
Insecure completely runs in the normal world with ingress and egress in cleartext, showing native performance.
This is basically StreamBox [82] with SBT's optimized stream computations ( §5).
We report the engine performance as its maximum input throughput when the pipeline output delay (defined in §2.2) remains under a target set by us.
Benchmarks We employ six benchmarks of processing sensor data streams from prior work [32,62,63,67,82].
They (5) Filtering (Filter) filters out input data, of which field falls into to a given range in each window.
We set 1% selectivity as done in prior work [67].
(6) Power Grid (Power), derived from a public challenge [62], finds out houses with most high-power plugs.
Ingesting a stream of perplug power samples, it calculates the average power of each plug in a window and the average power over all plugs in all houses in the window.
For each house, it counts the number of plugs that have a higher load than average.
It emits the houses that have most high-power plugs in the window.
The event for this benchmark is composed of 4 fields (16 Bytes).
Benchmark 2, 4, and 6 use real-world datasets; others use synthetic data sets of which fields are 32-bit random integers.
Note that SBT's GroupBy operator bases on sort and merge and is insensitive to key skewness [15].
End-to-end performance Figure 7 shows the throughputs of all benchmarks as a function of hardware parallelism.
SBT can process up to multiple millions of events within subsecond output delays (labeled atop each plot).
For simpler pipelines such as WinSum and Filter, SBT processes around 12M events/sec (140 MB/sec).
This throughput saturates one GbE link which is common on IoT gateways [88].
Overall, SBT can use all 8 cores in a scalable manner.SBT's absolute performance is state of the art.
We test three popular, insecure stream engines: Flink [19], designed for distributed environment and known for good single-node performance [68]; Esper [46], designed for a single machine; SensorBee [90], designed for sensor data processing on a single device.
As shown in Figure 8, on the same hardware (HiKey) and the same benchmark (WinSum), we have measured that SBT's throughput is at least one order of magnitude higher than the others.
This is because i) our Insecure baseline has high performance for its rich task parallelism (inherited from StreamBox [82]) and native, vectorized stream computations (new contributions); ii) SBT only imposes modest security overhead, as will be shown later.Comparison to secure stream engines The comparison is challenged by that TrustZone was rarely exploited for protecting data-intensive computations.
To our knowledge, i) no analytics engines use TrustZone for data protection and ii) no systems can partition an insecure stream engine for TrustZone.
Note that popular secure analytics engines, e.g. VC3 [99] and Opaque [124], not only require SGX but also target batch processing instead of stream analytics.
To this end, we qualitatively compare SBT with SecureStreams [53], the closest system we are aware of.
Designed for an x86 cluster, SecureStreams uses SGX to protect stream operators and targets strong data security.
On a benchmark similar to WinSum it Since memory usage fluctuates at run time, the error bars show two standard deviations below and above the average.
was reported to achieve 10 MB/sec, one magnitude lower than SBT on WinSum.
Furthermore, SecureStreams achieved such performance on a small x86 cluster which has much richer resource than HiKey: the former has faster CPUs (8x i7-6700@3.4GHz versus 8x Cortex-A53@1.2GHz), larger DRAM (16 GB versus 2 GB), higher power (130W versus 36W), and higher cost ($600 versus $65).
SBT's advantage comes from i) data exchange via coherent memory inside one TEE, instead of exchanging encrypted messages among workers; ii) memory management specialized for streaming, and iii) vectorized computations.Security overhead We investigate the overhead of the new security mechanism contributed by SBT -its isolated data plane.
We assess the overhead as the throughput loss of SBT ClearIngress as compared to Insecure (i.e. native performance as StreamBox [82] invoking SBT's stream computations), both paying same costs for data ingress.
The target output delays are the same (labeled atop each plot in Figure 7).
The security overhead is less than 25% in all benchmarks.
This is similar to or lower than the reported overhead (20-70%) in recent TEE systems [22,71,112].
Overhead analysis: The security overhead mostly comes from world switch, among operators and inside each operator.
To understand the switch cost within an operator, we profile GroupBy, one of the most costly operators.
We test different input batch sizes, which have a strong impact on TEE entry/exit rates and hence isolation overhead ( §4).
Figure 9 shows a run time breakdown.
When each input batch contains 128K (close to the value we set for SBT) or more events, more than 90% of the CPU time is spent on actual computations in TEE.
The CPU usage of TEE memory management is as low as 1-2%.
In the extreme case where each input batch contains as few as 8K events, the overhead of world switch starts to dominate.
Most of the world switch overhead comes from OP-TEE instead of the CPU hardware (a few thousand cycles per switch), suggesting room for OP-TEE optimization.Impact of decrypting ingress data Decrypting ingress data is needed if source-edge links are untrusted ( §3) and source must send encrypted data.
It has substantial performance impact.
By comparing SBT to SBT ClearIngress, turning on/off ingress decryption leads to 4% -35% throughput difference when all 8 cores are in use.
The performance gap is more pronounced for simple pipelines, which has higher ingestion throughput leading to higher decryption cost.TEE memory usage While sustaining high throughput, SBT consumes a moderate amount of physical memory, ranging from 20 MB to 130 MB as shown in Figure 7.
The memory usage is as low as 1-6% of the total system DRAM.
The virtual memory usage is also low, often 1-5% of the entire virtual address space in OP-TEE.
The memory usage increases with the throughput, since there will be more in-flight data.
On the same platform, Flink's memory consumption is 3× higher, due to its hash-based data structures and the use of JVM.
This validates our choice of uArrays.Attestation overhead Attestation incurs minor overhead to both the edge and the cloud.
We measured that SBT produces 300-400 audit records per second across all our benchmarks, and spends a few hundred cycles on producing each record.
Compressing such record streams on HiKey consumes 0.2% of total CPU time.
Our consumer written in Python on a 4-core i7-4790 machine replays 57K records per second with a single core, suggesting a capability of attesting near 500 SBT instances simultaneously.
We will evaluate the efficacy of record compression in Section 9.3.
Exploitation of trusted IO As shown in Figure 7, a comparison between SBT and SBT IOviaOS demonstrates the advantage of directly ingesting data into TEE and bypassing the OS: SBT outperforms the latter by up to 20% in throughput due to reduction in moving ingested data.Trusted primitive vectorization ( §5) Our optimizations with ARM vector instructions are crucial.
To show this, we examine GroupBy, one of the top hotspot operators.
When we replace the vectorized Sort that underpins GroupBy with two popular implementations (qsort() from the the OP-TEE's libc and std::sort() from the standard C++ library), we measured the throughput of GroupBy drops by up to 7× and 2×, respectively.
We have similar observation on other operators.Efficacy of hint-guided memory placement ( §6.2) We compare to an alternative design: the modified allocator acts based on the heuristics that all the uArrays produced by the same primitive belong to the same generation and are likely to be reclaimed altogether.
Accordingly, the modified allocator places these uArrays in the same uGroup.
As shown in Fig- ure 10, in three benchmarks, the modified allocator increases memory usage by up to 35%.
This is because, without hints, it cannot place uArrays based on future consumption.uArray on-demand growth ( §6.1) We compare uArray to std::vector, a widely used C++ sequence container with ondemand growth.
We run a microbenchmark of N-way merge, an intensive procedure in trusted primitives.
It iteratively merges 128 buffers (uArrays or vectors), each containing 512 KB (128K 32-bit random integers) until obtaining a monolithic buffer; as merge proceeds, buffers grow dynamically.
As shown in Figure 11, uArrays is 4× faster than std::vector, because the allocation and paging in TEE that back uArray growth is much faster than that of a commodity OS.Compression of audit records ( §7) The compression significantly saves the uplink bandwidth.
We test two benchmarks (WinSum and Power) on two extremes of the spectrum of computation cost, and test two very different input batch sizes.
This is because simpler computations and smaller batch sizes generate audit records at higher rates.
Figure 12 shows that SBT compresses audit records by 5×-6.7×.
In an offline test using gzip to compress the same records, we find our compression ratios are 1.9× higher than gzip.
2-40 KB/sec of uplink bandwidth is saved, which is significant compared to the uploaded analytics results, which are 144 bytes/sec for WinSum and 400 bytes/sec for Power.
Secure data analytics DARKLY [61] protects sensor data by isolating computations in an OS process, resulting in a large TCB.
VC3 [99] and SecureStreams [53] use SGX to protect the operators in distributed analytics.
They lack optimizations for parallel execution in one TEE on the edge.
To process data confidentiality, STYX [106] computes over encrypted data, a method likely prohibitively expensive to edge platforms.
Opaque [124] protects data access patterns of distributed operators, targeting a threat out of our scope.TCB minimization Minimizing TCB is a proven approach towards a trustworthy system.
Flicker [80] directly executes security-sensitive code on baremetal hardware.
Trustvisor [79] shrinks its TCB to a specialized hypervisor.
Sharing a similar goal, SBT addresses unique challenges in supporting data-intensive computation on a minimal TCB.Trusted Execution Environments Much work isolates security-sensitive software components.
Terra [48] supports isolation with a virtual machine.
Many systems used TrustZone and SGX [81] for TEE.
Some systems enclose in TEE whole applications [22,27,51,112], while others partition existing programs for TEE [71,93,101].
These approaches often result in larger TCBs and/or higher overhead than SBT and are thus less desirable for SBT.
TEE also sees various novel usage, including protecting mobile app classes [96], enforcing security policies [30], remote attestation of application control flows [13], and controlling data access [34].
None addresses data-intensive computation as SBT does.
Edge processing evolves from a vision [98,100] to practice [37,42,83].
Most works focused on programming paradigms [94], developing and deploying application [29,52,114], and resource management [86].
Complementary to them, SBT focuses on secure analytics on the edge.
Stream processing systems, in response to big data challenges, evolve from single-threaded [33,40,78,105,110] to massive parallel systems [14,69,85,92,92,113,122].
The existing systems focus on challenges, such as fault tolerance [122], fast reconfiguration [115], high parallelism [32,82], and the use of GPUs [67].
Few systems achieve data security and performance simultaneously as SBT does.
This paper presents StreamBox-TZ (SBT), a secure stream analytics engine designed and optimized for a TEE on an edge platform.
SBT offers strong data security, verifiable results, and competitive performance.
On an octa core ARM machine, SBT processes up to tens of millions of events per second; its security mechanisms incur less than 25% overhead.
The authors thank the anonymous reviewers and our shepherd, Eyal de Lara, for their insightful comments.
For this project: the authors affiliated with Purdue ECE were supported in part by NSF Award #1718702, #1619075, Purdue University CP-S/IoT Seed Grant Program, and a Google Faculty Award; the author affiliated with Northeastern University was supported in part by NSF Award #1748334.
