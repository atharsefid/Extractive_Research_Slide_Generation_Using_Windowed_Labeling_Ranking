In this paper we address the problem of detecting topics in large-scale linked document collections.
Recently, topic detection has become a very active area of research due to its utility for information navigation, trend analysis, and high-level description of data.
We present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the nodes are limited to the documents containing the term.
This tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models.
We develop a topic score measure for each term, using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity.
Our approach is based on the intuition that if a term is relevant to a topic, the documents containing the term have denser connectivity than a random selection of documents.
We extend our algorithm to detect a topic represented by a set of terms, using the intuition that if the co-occurrence of terms represents a new topic, the citation pattern should exhibit the synergistic effect.
We test our algorithm on two electronic research literature collections , arXiv and Citeseer.
Our evaluation shows that the approach is effective and reveals some novel aspects of topic detection.
The availability of large-scale linked document collections such as the Web and specialized research literature archives [6,3] presents new opportunities to mine deep knowledge about the community activities behind the document collections.
Topic discovery is one example of such knowledge mining that has recently attracted considerable research interest [12,14,9,22,23,24,17,4,16,7,15].
Topics are semantic units that can function as basic building blocks of knowledge discovery.
Once discovered they can be used in a number of ways including information navigation, trend analysis, and high-level descrption of data [22,15].
In this paper, we present a unique approach to topic detection that uses the correlation between the distribution of terms representing a topic and the distribution of links in the citation graph among the documents containing these terms.
This distinguishes our work from other approaches to topic detection that focus on textual data alone [9,22,23,14] or which detect topics and communities by studying graph properties without considering text features [10,19,11,20].
Our appoach is based on the intuition that documents related to a topic should be more densely connected in the citation graph than a random selection of documents are connected in the citation graph.
We therefore extract topics from the corpus by examining the structure of the term citation graph for each term in the corpus.
A term citation graph of a term A is a subgraph of the full citation graph restricted to the documents that contain the term A and the edges between these term-specific nodes.
If the term citation graph of a term A shows denser connectivity than a random subgraph of the full citation graph, it is likely that the term A represents a topic.An illustration of our approach to topic detection is as follows.
Let's imagine that we have a set of all documents containing a term α: for example "sensor network" or "association rule mining".
Intuitively, if α represents a topic, then the documents containing this term will be interconnected in a relatively dense citation network (Figure 1.
a) ).
This contrasts with another term η, for example "practical examples" or "six months", that are non-topic terms (i.e., general terms) for which the citation links among containing documents will be relatively sparse (Figure 1.
b) ).
The notions of "dense" and "sparse" connectivity are relative to the connectivity of a citation graph consisting of a random selection of documents and their citation edges from the full citation graph.We develop topic score measures that are log odds ratios of binary hypotheses based on a probabilistic description of graph connectivity.
For each term in the corpus, we take a look at its term citation graph.
Our topic score measure tells, with statistical confidence, whether the connectivity of the term citation graph is significantly denser than what is expected from the citation graph of a random selection of documents.
As a first approximation, we assume that a topic can be represented by a single term.
We then extend our algorithm to detect topics that are not represented by a single term, but by the relation of a set of terms.
We test our algorithms on two electronic research literature collections, arXiv and Citeseer.
Our experiments produce a ranked list of terms that on examination by field experts and based on our observations match prevailing topics in the corpus.
Our evaluation of the lists uncovers a number of interesting characteristics of the lists of terms, including the discovery of topics in varying scale, the prevalence and specificity of topics, and the time evolution of topics.This paper is structured as follows.
Section 2 and Section 3 present our algorithm to detect topics represented by a single term and by a set of terms, respectively.
Section 4 shows the results obtained by applying the algorithm to arXiv and to Citeseer.
Section 5 reviews the related work.
Section 6 discusses a few issues raised in the work.
Section 7 concludes.
The problem statement of this paper is "How do we detect topics in a linked textual corpus, such as a collection of research papers?"
.
We address this research problem by producing a ranked list of terms where terms are ordered according to how likely a term represents a topic and how significant the topic represented by a term is.
To accomplish this goal, we look at the citation graph of the corpus at the resolution of an individual term level.
Definition 1.
In this paper, a "term" is defined as an ngram phrase that consists of any n consecutive words from a document, where n is any positive integer.
For example, "network", "for the", "association rule mining" are all valid examples of a term.Conventionally, the citation graph of a corpus is a directed graph with nodes being the documents or research papers in the corpus, and with edges being the hyperlinks or the citation links.
In this paper, we only consider the undirected version of the citation graph.
We denote the undirected citation graph of the entire corpus as G all .
The term citation graph of a term A, GA, refers to a subgraph of the entire citation graph G all with nodes restricted to the documents that contain the term A and the links between these documents.
Precisely, Definition 2.
GA, the term citation graph of a term A, is defined byV (GA) = {d|document d contains a term A, d ∈ V (G all )} E (GA) = {e (di, dj) |di, dj ∈ V (GA) , e (di, dj) ∈ E (G all )}where V (G) denotes the set of vertices in G, E (G) denotes the set of edges in G, and e (di, dj ) is an edge between the nodes di and dj .
Given a term, we want to make a binary decision with statistical confidence about whether the term is relevant to a topic or not.
We use the following intuition.
If a term represents a topic, then the document nodes in its term citation graph will be well-connected by citations.
On the other hand, if a term does not represent a topic, the documents in its term citation graph are not related to each other, thus their distribution is random with respect to citation patterns.
Figure 1 shows this intuition.
Figure 1 a) is the term citation graph for a topic term α showing dense connectivity.
Figure 1 b) is the term citation graph of a non-topic term η showing sparse connectivity comparable to that of a random selection of documents.We formalize this notion by setting up two hypotheses.
Given a term A, hypothesis H1 says that A is relevant to a topic, and hypothesis H0 says A is not.
We make an observation O (GA) about the connectivity of the term citation graph of A, GA.
We compute the loglikelihood of the observation O (GA) under hypothesis H1 and the loglikelihood of O (GA) under hypothesis H0.
The difference of the two loglikelihoods becomes the topic score for the term A.TopicScore (A) = log (P (O (GA) |H1)) − log (P (O (GA) |H0)) = log " P (O (GA) |H1) P (O (GA) |H0) «(1)The topic score represents how well hypothesis H1 explains the connectivity observation, compared to hypothesis H0.
We take the observation O (GA) to indicate, for each node in GA, whether the node has at least one link to the rest of the graph or not.
Under hypothesis H1, it is very likely that a node in the graph is connected to the rest of the graph by at least one link.
The document either cites or is cited by another document that shares the topic.
We use the parameter pc, with a value close to 1, to denote this probability of a node in GA having at least one link to any other node in GA.
We present the result with pc set to 0.9 in Section 4.
1 Then, the loglikelihood of O (GA) with Figure 2: The term citation graphs from arXiv.
a) for a topic term "black hole", b) for a stop phrase "we show" hypothesis H1 is given as follows.log (P (O (GA) |H1)) = log Y i P (Oi (GA) |H1) !
= X i log (P (Oi (GA) |H1)) = nc,A log (pc) + (nA − nc,A) log (1 − pc) (2)where nA is the number of nodes in GA, and nc,A is the number of nodes in GA that have at least one link that points to another node within GA, and Oi (GA) is the pernode observation for node i.The loglikelihood of O (GA) with hypothesis H0 is more interesting.
Under the null hypothesis H0 that a term A is not relevant to a topic, the documents in GA are not related to each other.
Thus, given a node i in GA and one of its citation links, the probability that the other end of this link points to any node within GA is n A −1N−1, where nA is the number of nodes in GA and N is the number of nodes in the entire corpus.
That is, determining which node a citation link of a node i connects to can be considered as a random process with respect to GA, where any node in the entire corpus is equally likely to be the destination of the link.
Then, the probability that a node i in GA is connected to any other nodes in GA by at least one link is given as,1 − " 1 − n A −1 N−1 " l i ,where li is the number of all links of a node i.
The loglikelihood of O (GA) with hypothesis H0 is given as follows.log (P (O (GA) |H0)) = X i log (P (Oi (GA) |H0)) = X i∈Vc(G A ) log 1 − " 1 − nA − 1 N − 1 « l i !
+ X i∈(V (G A )−Vc(G A )) li · log " 1 − nA − 1 N − 1 « (3)where Vc (GA) denotes the set of nodes in GA that has at least one link to any other node in GA.It should be noted that our null hypothesis H0 is based on the randomness of the citation connectivity, not on the absolute sparseness of the connectivity.
This enables our topic score to effectively filter out high-frequency common phrases as non-topic terms.
This is illustrated in Figure 2.
2 Figure 2 a) shows the term citation graph derived from arXiv for a prevalent topic term "black hole" and Figure 2 b) for a stop phrase "we show".
As shown, it is not easy to discern from the graph visualization that the topic relevance of "black hole" is much greater than that of "we show".
However, as will be seen in Section 4, our topic score measure assigns the highest score to "black hole", and the lowest score to "we show".
This is because, for the term "we show", the random connectivity assumption of the null hypothesis H0 defaults to the dense connectivity as shown in Figure 2 b), while the hypothesis H1 assumes even denser connnectivity.If we generate the topic scores in Eq.1 for all possible terms in the corpus and order them, we get a ranked list of terms, where terms are ranked according to how likely they represent the topics of the corpus.
The terms at the top ranks are the terms representing the topics prevalent in large scale.
This is because the term citation graphs of the topics prevalent in large scale have many instances of per-node observations that support the hypothesis H1 over H0.As hinted above, the bottommost ranked terms have clear intuitive interpretation as well.
These terms are the stop words or common phrases, as their term citation graphs exhibit the large scale statistical evidence that can be better explained by H0 than by H1.
Some topics are not detectable by a single term but by the appearance of a set of terms.
This may occur, for example, when a new term is not coined for a topic, but the topic is represented by the relation between a few general terms.
For example, let's consider a topic M represented by the co-occurrence of two terms "quantum computer" and "quantum dot".
This is a research topic in physics about using "quantum dot" as a hardware device for "quantum computer".
However, each individual term "quantum dot" or "quantum computer" represents a much broader research topic than the given topic M .
The term "quantum computer" represents any topic related to quantum computing: examples are quantum computer algorithms, fault tolerant quantum computing, and many kinds of hardware devices for quantum computer.
"quantum dot" is a nano-scale semiconductor material.
The term "quantum dot" represents a broad research topic including material property study, and using quantum dot to make applications such as laser, quantum computer logic gate, etc.
Thus, looking at a single term is not going to reveal the topic M .
The problem of detecting a topic represented by a set of 2 To aid the visualization, the term citation graphs from arXiv are illustrated in the following ways.
The vertical axis is a time scale where time follows downward.
The horizontal axis spans 7 research fields of arXiv.
A paper at a particular time and a field is placed in the small rectangle at the corresponding position.
The darkness of a rectangle represents the number of papers contained in the rectangle.The links between rectangles are the citation links between the papers in the rectangles.
terms but not by an individual term is different from finding the co-occurrence counts of terms.
The mere high count of co-occurrence is not what we want.
The co-occurrence count of two stop words might be high, but it does not carry topic information.
Also, the normalized co-occurrence, defined as the co-occurrence count divided by the occurrence count of a single term, is not what we want either.
At the extreme, we may think of terms A and B that always occur together with high frequency.
But this topic is detectable by looking at a single term A or B by the method explained in Section 2.
Finally, it should be noted that our goal is different from association rule mining [1]: the above example of terms A and B co-occurring with high frequency qualifies for an association rule, but not for detecting a topic represented by a set of terms.
Then, how do we detect topics represented by a set of terms?We again look at the term citation graphs and use the following intuition that is illustrated in Figure 3.
In Figure 3, a small rectangle containing α is a document containing a term α.
Similarly, a small rectangle containing β is a document containing a term β.
A small rectangle containing both α and β is a document containing both terms.
A link connecting two documents is a citation link.
The left big circle encloses the term citation graph of the term α, which is Gα.
The right big circle encloses G β .
The documents and links within the intersection of the two circles constitute the citation graph for the documents containing both terms, which we denote as G α∩β .
Figure 3 shows that the documents containing both terms α and β are significantly more densely connected than Gα or G β .
This indicates that there is a nontrivial topic represented by the co-occurrence of α and β, but not by one of them.
On the other hand, if there is no significant topic represented by the marriage of α and β then the occurrence of the term β within Gα or the occurrence of the term α within G β will not be correlated to the citation pattern.
In this case, G α∩β should have the link connectivity comparable to that of the same size random subgraph of Gα or G β .
We formalize this notion as follows.
Given a term A and a term B, we want to detect whether the connectivity of GA∩B is significantly higher than what we could normally expect from the connectivity of GA or GB.
To account for the connectivity of any term citation graph G, we use an observation 3 that considers, for each citation link of each node in G, whether the link ends with a node within G or outside G.
If for each link of a node in G the probability that it ends with another node within G is p, then the loglikelihood of the connectivity observation on G is,ln (P (O (G) |p)) = X i (ci (G) ln (p) + (li − ci (G)) ln (1 − p))(4)where li is the total number of citation links of a node i, and ci (G) is the number of citation links of a node i that fall within G. Let p * (G) be the value of p that maximizes Eq.4.
With the number of nodes in G fixed, p * (G) tends to increase, as the connectivity of G gets denser.
Let's consider GA and GA∩B under the hypothesis that the co-occurrence of terms A and B does not represent a new topic.
Under this null hypothesis, the generative process of determining which document in GA contains the term B is an independent random process with respect to the distribution of the citation links of GA.
Thus, if we let p0A be our guess for p * (GA∩B) under the null hypothesis, our best guess for p0A is the probability that maximizes the average loglikelihood of the following subgraphs of GA.
The subgraphs we consider are any subgraphs of GA that have the same number of nodes as that of GA∩B, and the citation links between them.
Formally, p0A is given as follows.p0A = arg max p 1 nCk X Gσ ln (P (O (Gσ) |p))(5)where n is the number of nodes in GA, k is the number of nodes in GA∩B, and the summation over Gσ runs over any graph Gσ that satisfies the followingV (Gσ) ⊂ V (GA) , |V (Gσ) | = |V (GA∩B) |, E (Gσ) = {e (vi, vj ) |e (vi, vj ) ∈ E (GA) , vi, vj ∈ V (Gσ)}(6)p0A can be analytically obtained to bep0A = (k − 1) P i∈V (G A ) ci (GA) (n − 1) P i∈V (G A ) li(7)Now, we think of the alternative hypothesis that says the co-occurrence of terms A and B represents a new topic.
Under this hypothesis, our guess for p * (GA∩B), which we denote as p1A, should be significantly higher than p0A.
We set it as p1A = m · p0A, where m is a mulplicative parameter greater than 1.
The following score TA (A, B) is our confidence about how likely the co-occurrence of terms A and B represents a new topic, with respect to a term A.TA (A, B) = ln " P (O (GA∩B) |p1A) P (O (GA∩B) |p0A) «(8)Note that our guess for p1A need not be exactly p * (GA∩B) nor even close to it.
The actual value of p * (GA∩B) only needs to be relatively closer to p1A than to p0A to make TA (A, B) positive.
In particular, if the actual p * (GA∩B) is significantly larger than p0A, TA (A, B) will be positive for servation O(G) for a single term topic detection in Section 2.
The choice is made so that the new observation O(G) can account for graph connectivity in a continuous spectrum.
a wide range of m. Thus, with large m, we could filter false positives, while we may only lose false negatives with weak confidence.
We experimented on several values for m in the range of [2,10].
While the result does not sensitively change over a wide range of m, the choice of m = 6 seems to provide a good balance between false positives and false negatives.In Section 4, we present the result with m = 6.
We then get TB (A, B) in the similar way by looking at GA∩B and GB.
Our final score for judging whether the cooccurrence of terms A and B represents a new topic or not is given by taking the minimum of TA (A, B) and TB (A, B), reflecting our belief that the link density of GA∩B should show a significant departure from that of both GA and GB.TopicScore (A, B) = min (TA (A, B) , TB (A, B)) We use arXiv and Citeseer for evaluation.
We restrict the terms we consider to all possible bigrams in the corpus.
We choose bigram as our term unit, because bigrams typically convey more concrete ideas than unigrams, yet higher grams might suffer from the explosion of the number of terms and sparseness of data for each term.
But, it is only a choice of convenience and our algorithm can be applied to any n-grams.
We further restrict the terms by pruning out low frequency terms that appear in less than 5 documents in the corpus and by pruning out 35 stop words.
arXiv is an actively maintained online repository of research papers in physics.
We take papers from year 1991 to year 2006 that span 7 major arXiv areas.
This is in total 214,546 papers and 2,165,170 citation links between them, which amounts to 10.09 per-document citations.
For each paper, we use its abstract as its document.We perform the following experiments.
First, for all possible terms appearing in the corpus, we compute the single term topic score measure of Eq.1, and get a ranked list of topics.
Second, for all possible term pairs in the corpus, we compute the topic score of two terms as in Eq.9, and get a ranked list of topics.The running time is reasonable.
We used a pentium IV PC with 2GB memory.
It took 45 minutes to generate the term citation graphs for all terms and their inverted index.
It took 4 minutes to compute the single term topic scores for all terms.
It took about 10 hours to compute the topic scores of two terms for all possible term pairs.To consider all pairs of terms for two term topic scores could be prohibitive as there are a huge number of terms.
But, we need to consider a pair of terms only when the two terms appear together in at least one paper.
This cooccurrence matrix is pretty sparse when a document is an abstract.
Thus we achieve a reasonable running time for the two term topic score experiment.
Computing the topic scores for each term in the corpus according to Eq.1 gives a ranked list of topics.
The ranked list of terms has 137,098 entries (terms), where top entries constitute topic terms and bottom entries constitute non-topic terms.
Table 1 shows the top 15 entries from the ranked list.
The first 2 columns represent the rank and the topic term respectively.
The third column labeled as < n, nc, |E| > is an information about the term citation graph of the topic term: n is the number of nodes in the citation graph of a topic term, nc is the number of nodes that has at least one link connecting to any other node within the graph, |E| is the number of edges in the graph.An objective and quantitative evaluation of the result is difficult due to the lack of standard formal measures for topic detection tasks.
However, when the results were examined by the domain experts, they recognized the topics presented in Table 1.
Also, we have an informal evidence that these top ranked terms do represent highly prevalent topics in the physics literature.
When we typed in each topic term of the top 20 ranks as a search query to www.google.com, 19 of them returned Wikipedia entries within the top 5 of the google search results.
The inspection of the Wikipedia articles reveals that most of them have serious physics research oriented contents.
The one topic term that did not return the Wikipedia entry was "heavy quark".
But, the second rank entry of its google search result is "The 5th international workshop of heavy quark physics", indicating that it also is a prevalent research topic in physics.The topic terms at the top ranks are topics in large scale, as we can see from the term citation graph information of < n, nc, |E| > column.
The topic term entries down to a few thousand'th level of the ranked list still present meaningful topics.
Table 2 shows a few entries of topic terms around 100'th, 500'th, 1000'th, 2000'th ranks.
There is an apparent trend of topic scale getting smaller as we go down to lower ranked topic terms, as seen from < n, nc, |E| > column.
Topics discovered at these levels could be more interesting as they tend to represent more specific ideas than the more generic and prevalent top ranked topic terms.
Figure 4 shows the term citation graphs of the topic terms at 100'th, 990'th, 1971'th ranks, respectively (Refer to Footnote 2 for how to read the graphs).
We see the scale difference of the topic terms at different ranks.
Figure 4 c) suggests that even at 1971'th rank, there is still a meaningful topic that binds the papers in the term citation graph.As explained in section 2, the bottommost entries of the ranked list are stop words or common phrases, whose term citation graphs are much better explained by hypothesis H0 than by hypothesis H1.
Table 3 shows the bottommost 15 terms of the ranked list.It should be noted that the topics discovered by our algorithm have a varying degree of prevalence and specificity, that are natural in the given corpus.
This is because we do not assume a predefined number of topics to discover, as language model approaches or graph-based clustering approaches do.
Fixing the number of topics to discover has the effect of determining the scale of topics in advance.To see the overall property of the entire ranked list, we present two plots, Figure 5 and Figure 6.
Figure 5 is a plot of term rank vs. the log size of the term citation graph averaged over 100 consecutive terms.
It shows that the term frequency gets higher, as the rank gets close to either the highest or the lowest ranks.
This is because in a large-scale term citation graph one hypothesis is strongly preferred over the other due to many instances of per-node observations that support the hypothesis.To show the connectivity of term citation graphs, we devise the following measure and use it in the plot of Figure 6.
Given a term citation graph GA, ci (GA) denotes the number of links of a node i that falls within GA, li denotes the top topic (term) < n, nc, P i c i (G A )/ P i l i n A /Nas normalized edge containment.
The normalized edge containment should default to 1 if the citation pattern of GA is random.
Figure 6 shows a plot of term rank vs. the normalized edge containment.
As expected, the topic terms at high ranks show high normalized edge containment, while the non-topic terms at low ranks show low normalized edge containment.
What is interesting to note is that the graph is not monotonically decreasing: up to the top few thousand ranks, the normalized edge containment keeps increasing.
This agrees with our observation that the middle rank topics are more specific than the top rank topics.
By computing the term pair topic scores of Eq.9 for all possible pairs of terms in arXiv corpus, we get a ranked list top topic (term) < n, nc, |E| > rank . . ..
where each entry is a pair of terms that might represent a topic.
Since we are looking at the intersection citation graph of two terms, we get a sparser graph to look at.
In order to alleviate the sparseness, we stemmed our corpus.
Table 4 shows the top 12 entries of the ranked list.
These entries are the topics that are represented not by a single term, but by the relation involving a set of terms.
For example, the rank 1 entry has "phase transit(ion)" and "standard model" as its topic terms.
"phase transition" is a general term meaning a change in macroscopic state of a large-scale system.
"standard model" is a prevalent theory of particle physics that describes the fundamental interactions of elementary particles.
It turns out that the papers at the intersection of the two terms talk about the "phase transition" occuring in "standard model" or the "phase transition" occuring in minimal supersymmetric "standard model" which is an extension of "standard model".
The individual term "phase transition" or "standard model" has a much broader research context than the topic identified.
The rank 2 entry has "gauge theory" and "matrix model" as its topic terms.
It turns out that there was a heavily cited paper that started the whole idea of analyzing "gauge theory" using the computational techniques from "matrix model", and the majority of papers in the intersection graph talk about the further development of this idea.
As explained in the previous section 3, the papers of the rank 7 entry talk about using "quantum dot" as a hardware implementation of "quantum computer".
The last three columns of table 4 show the citation graph information < n, nc, |E| > for term A, term B, and their intersection, respectively.
They show that the connectivity of the intersection graph GA∩B exhibits significant departure from the same size random subgraph of GA or GB.
Our Citeseer data contains 716,771 papers, with 1,740,326 citations.
This amounts to 2.43 citations per paper.
For each paper, we use its title and abstract combined as its document.
The number of bigrams in the corpus after pruning out the low-frequency bigrams and 35 stop words is 631,839.
The majority of papers are from year 1994 to year 2004.
We divided the documents into two different document sets.
One set contains all the documents up to year 1999, and the other set contains all the documents since year 2000.
We performed the single term topic score measure of Eq.1 to each set.
The top 25 topic entries of each set are shown in parallel in Table 5.
We see that the top rank topics have changed significantly between the two time periods.
We see that many top rank topics of the time frame since 2000 carry recent trends that were not significant before.
Examples are "sensor networks", "(ad) hoc networks", "wireless sensor", "intrusion detection", "semantic web", "xml data", and "image retrieval".
"support vector (machine)" was ranked 35th in the document set up to 1999, and it has risen to the rank 5 in the document set since 2000.
"congestion control" more or less maintains its topic rank through the different time periods.
We observe the fall of many top ranked topics of the document set up to 1999, in the time period since 2000.
The ranked list of topic terms is quite instructive as well: initially, we did not recognize the 7th rank "interior point" of the time frame up to 1999 as a topic.
But, it turns out "interior point" represents an important family of algorithms in linear programming.As in the case of arXiv evaluation, we see that the ranked list of topic terms from Citeseer has meaningful topics even around a thousand'th level with the apparent trend of topic scale getting smaller as we go down the ranks.
Due to the space limitation, however, we do not present the result.In order to see the time evolution of topics more clearly, we performed the following experiment.
We ran the single term topic score measure for the entire Citeseer document collection.
Then, for each term in top 70, we generated a plot where x-axis is years spanning from 1994 to 2004 and yaxis is the number of documents of the term citation graph in a particular year normalized by the total number of documents in that year.
Figure 7 shows the plots for 12 such topic terms out of top 70 terms.
We see a sharp recent rise of rank term A Table 4: The top 12 entries of two term topic scores from arXiv "sensor networks" and "semantic web", a significant rise of "support vector" and "energy consumption", a rise of "xml data" in a smaller scale, the fall of "logic programs", "petri nets", "interior points".
"congestion control", "association rules", and "genetic programming" show less dramatic dynamics.
between the term distribution and the citation link distribution for a topic.
Second, we use for a topic measure the log odds ratio of binary hypotheses based on a probabilistic description of graph connectivity.
Previous work on topic detection can be largely divided into two groups.
The majority of papers take a language model based approach.
This approach tends to focus on text, but a few papers extend the model to incorporate links.
Another group of work is based on studies of graph properties.
Most of these papers address a problem related to topic detection: community detection.
They tend to use the non-probablistic aspects of graph properties.
There are also related papers that share some of the ideas used in this paper.
Specifically, these ideas are examination of patterns at individual term level, usage of log odds ratio to detect patterns, and investigation of the notion of term informativeness.
The language modeling approach [9,22,23,24,7,16] assumes a multi-stage generative process where semantically meaningful modalities such as topics or authors are chosen as an intermediate step, and then the words are drawn from the multinomial distribution conditioned on these modalities.
These papers differ in the design choice for the generative process.
Examples of design decisions are the choice of modalities or the final features that will be produced.
[9] uses the document generation process conditioned on topic distributions.
[22] uses authors as distribution over topics as additional modalities.
[23] detects topics over time by letting the generative process produce the timestamps of words as well as the words themselves.
A number of papers extend the model to incorporate links.
[7] treats the reference list of a paper as another final feature to produce, in addition to the bag of words.
[16,24] apply the language model approach to social network analysis where documents are the communication links such as e-mail messages between people.
[4] aims to overcome the inability of latent dirichlet allocation used in the papers above for describing the correlation of topics, by including the correlation matrix of topics in the generative process.
[17] computes the themes of a document collection by the mixture model using the EM algorithm.Graph properties are used to study community structures by [10,8,11,20,19,13,2].
As a distance metric [10] uses the similarity of citation patterns, [8,11] use the notion that Figure 7: The topic evolution over time in Citeseer.
a) "logic programs", b) "sensor networks", c) "support vector", d) "congestion control", e) "petri nets", f ) "association rules", g) "genetic programming", h) "semantic web", i) "energy consumption", j) "xml data", k) "image retrieval", l) "interior point" nodes have more links to the members of the same community than to other nodes, [20] introduces the concept of edge betweenness, and [19] uses the measures from bibliometry and graph theory.
Some papers in this group combine the information from text as well.
[13] extracts storylines for a query by identifying densely connected bipartites from the document-term graph of the search results.
[2] improves the document categorization performance by starting from a text-based categorization result and then iteratively relabeling the documents to further satisfy the constraints imposed by the link proximity relation.Our approach of looking at citation patterns at an individual term level and using the loglikelihood to explain the observation is inspired by [12].
[12] detects a topic as a burst of activities represented by the state transition in a markov chain.
In an experiment on paper titles and presidential speeches the paper shows that topics can be effectively detected as time bursts in a single term level.
The idea of anomaly detection by log odds ratio is used in a number of papers related to topic detection.
[18] uses the log odds ratio of event frequencies to detect space-time clusters.
[14] discovers a set of words as topic signature in a supervised learning setting by comparing the log odds ratio of word frequency in topic documents and non-topic documents.
The ranked list of terms for topics produced by our algorithm shows a continuous spectrum of term informativeness in representing topics.
The notion of term informativeness is explored in a number of related contexts.
[5] detects the terms informative about the citation links to use them as features for document categorization.
For this purpose, they use the expected entropy loss measure, which resembles the one used in the decision tree feature selection.
[21] detects the informative terms for named entity detection, using the idea that informative terms are better modeled by a mixture of two unigram models while non-informative terms are better modeled by a single unigram model.
It is worthwhile to note that graph connectivity observations used in our algorithms are pluggable.
One can plug in an observation that best suits one's need.
As different observations may represent different aspects of graph connectivity, the choice of an observation affects the topic score result.
For example, the observation used in Section 2 for a single term topic score concerns whether a node has a connection to the rest of the graph or not, but it does not distinguish how many connections a node has.
Thus, the observation is generous on loosely connected topics.
As a result, the highest topic scores are given to the large-scale prevalent topics even though these topics are not as tightly connected as the more specific smaller scale topics.Another point to note is that because our algorithms discover topics without imposing any constraint on the relationship among topics such as restricting the number of topics to be discovered or assuming implicit mutual exclusion among topics, the topics discovered are suitable for expressing the complex relationship among topics.
Specifically, with our topics, a single document can be involved in multiple topics, and topics could have hierarchical covering relations or nonhierarchical overlapping relations among themselves.
Understanding the structure of the relationship between the topics is left as future work.
In this paper, we presented algorithms to detect topics from a linked textual corpus based on the unique approach of using the correlation between the term distribution and the link distribution for topics.
Our algorithms produce a ranked list of terms for topics represented by a single term and for topics represented by a set of terms.
Our evaluation on arXiv and Citeseer data show that the method is effective.
Topics discovered by our algorithms reveal novel aspects of topic detection.
The ranked list shows a continuous spectrum of topics of varying prevalence and specificity that are natural in the given corpus.
The relations among the terms that represent topics are revealed by the two term topic score measure.
As an interesting by-product, our algorithm can discern common phrases without the prior knowledge of stop word notion.
The possibility of discovering complex topic relations and the pluggable characteristic of graph connectivity observations are discussed.
We would like to thank Simeon Warner for providing arXiv data, and Isaac G. Councill for providing Citeseer data.
We would like to thank Prof. John E. Hopcroft for his input on graph properties and his encouragement, and Prof. Thorsten Joachims for valuable discussions.
This work was supported in part by National Science Foundation under award numbers IIS-0430906, 0227648, 0227888, and 0424671.
