In distributed services shared by multiple tenants, managing resource allocation is an important prerequisite to providing dependability and quality of service guarantees.
Many systems deployed today experience contention, slowdown , and even system outages due to aggressive tenants and a lack of resource management.
Improperly throttled background tasks, such as data replication, can overwhelm a system; conversely, high-priority background tasks, such as heartbeats, can be subject to resource starvation.
In this paper, we outline ve design principles necessary for eeec-tive and eecient resource management policies that could provide guaranteed performance, fairness, or isolation.
We present Retro, a resource instrumentation framework that is guided by these principles.
Retro instruments all system resources and exposes detailed, real-time statistics of per-tenant resource consumption, and could serve as a base for the implementation of such policies.
Today, many distributed services are shared by multiple tenants, both on private and public clouds and datacenters.
Ideally, multi-tenant service providers should be able to implement resource management policies with various high-level goals -e.g., admission control, fairness, guaranteed performance, or usage limits.
ese policies enable the provider to guarantee service-level objectives (SLOs) to a client, while simultaneously supporting other clients with diiering workload characteristics.
Equally important, these policies can ensure that a client does not trigger a system-wide outage by adversarially or inadvertently starving essential background tasks of required resources.Traditionally, resource isolation has been enforced using OS-level primitives at the granularity of processes or users (e.g., cgroups []) or using hypervisors that provide similar isolation among virtual machines.
ere is also some progress in providing network performance guarantees to groups of VMs [, ].
However, in distributed systems there is a mismatch in granularity between resource management and the existing mechanisms: on the one hand tenants share the same processes, thus using the same data structures, thread pools, and locks; on the other hand, several processes spanning machines work on behalf of the same tenant, requiring coordinated management.
Prior approaches rely on ad-hoc enforcement techniques that are diicult to apply to other systems [, ].
In this paper, we consider some of the challenges faced by resource management policies, which we observed in practice: ) due to extensive APIs, a system can bottleneck on any hardware (e.g., disk) or soware resource (e.g., locks); ) because users share resources at the application level, it may not be apparent which user is responsible for system load; ) tenants interfere not only among themselves but with potentially expensive internal system tasks; ) the resource requirements for each request issued by a client can vary substantially based on its type, arguments, and the system state; and ) it can be unpredictable on which machines a request will execute and for how long.While ignoring some of these challenges can lead to resource management policies that work in restricted scenarios ( §), we argue that one has to consider all of them to build resource management policies that are eeective, i.e., that work in a general setting, and eecient, i.e., that achieve their objectives without being overly aggressive and wasting resources.
ese observations motivate ve design principles necessary to implement such policies ( §), and the design of Retro ( §), our prototype framework for resource tracking and enforcement in distributed systems.
Retro tracks the tenants of a system across a comprehensive set of resources, exposes usage statistics in realtime, and provides hooks back into the system to eeect resource management decisions.
Retro's design is guided by the principles we outline, and our preliminary evaluation ( §) shows evidence that it could be used by policies to properly manage resources in a shared distributed system.
is section describes design principles that are necessary for a resource management policy to be eeective and efcient.
e principles are motivated by our experiences and observations from multiple data, compute, and communication oriented systems.
For concreteness we present our observations in the context of HDFS.
Observation: Multiple request types can contend on unexpected resources.
For our purposes, it suuces to say that an HDFS cluster has a single NameNode (NN) that manages the metadata for the lesystem, and many DataNodes (DNs) that store replicated le blocks.
Since the core functionality of HDFS involves reading and writing les from DNs, one could consider disk and network as the primary resources that require explicit resource management.
On the other hand, HDFS contains many other request types -in fact, there are over API calls [] -such as listing les in a directory, creating directories, creating symbolic links, or renaming les.Figure demonstrates how the latency of an HDFS client can be adversely aected by other clients executing very different types of requests, contending at diierent resources such as queues and locks.
at experiment was run in a very simple setup with one NN and one DN, running on two machines.
However, the ability of a single tenant to adversely aect other tenants' performance generalizes beyond this simple scenario.
For instance, a Hadoop job that reads many small les can stress the storage system with disk seeks, like workload A in the gure, and impact all other tenants using the disks.
Similarly, a tenant that repeatedly resubmits a job that fails quickly puts a large load on the NN, like workload B, as it has to list les in the job input directories.
In communication with Cloudera [], they acknowledge several instances of aggressive tenants impacting the whole cluster, saying "anything you can imagine has probably been done by a user".
e bottleneck resource in each of these instances varies from locks, thread pool queues, to the storage and the network.
While it might be tempting to design throttling and scheduling policies based only on the primary APIs and resources, our experiments show that this would be incomplete.
us, robust resource management requires a comprehensive accounting of all resources that clients can potentially bottleneck on, and consideration of all possible API calls.
Our rst principle comes from this.Principle: Consider all request types and all resources in the system Observation: Contention may be caused by only a subset of tenants.
Distributed systems comprise multiple time [sec] hp latency t1 thr'put t2 thr'put Figure : e gures show the impact of manually throttling two background tenants (tt,tt), demonstrating that only tt impacts the high priority tenant (hp) processes across many machines, and diierent tenants contribute diierent load to the system.
Resource contention may be localized to a subset of machines or resources.
Some tenants may not be accessing these machines or resources, while other tenants may be consuming more than their fair share.
If a goal of the system was to reduce contention on these resources, it would be ineecient and unfair to penalize all tenants equally when only a subset may be culpable.Figure demonstrates the eeect in HDFS on the latency of a high priority tenant, when we manually throttle the request rates of two other tenants.
e gure shows a high-priority tenant, t h p , sending MB write requests, sharing the service with two low-priority tenants.
Tenant t submits kB random reads, while tenant t lists les in a directory.
When we separately throttle the request rates of the background tenants, we observe an eeect on the latency of t h p only when throttling t .
In the above example, if our goal was to decrease the latency of t h p , we would only beneet from reducing t 's request.
A non-trivial system should be capable of targeting the cause of contention -the tenants, machines, and resources responsible.
is motivates our second principle.
From these observations we derive our second principle.
Resource management policies should treat both systemand client-generated tasks as rst class entities.Principle: Treat foreground and background tasks uniformly.Observation: Resource demands are very hard to predict.
Many schedulers [, , ] need the cost of a request to be speciied a priori, oen in a multidimensional space representing the diierent resources.We argue that resource requirements estimated oine would be insuucient for a number of reasons; a) the resources requested by a task could be innuenced by one or more of the arguments of the API call; b) a model would need to encode both the total cost and the rate of resource consumption; c) the presence of other tenants could innuence the behavior of a request (eg, by evicting cache entries); d) in order to handle localized congestion a model would need to know which machines will execute a request; and e) the state of the system can aect the success of operations (eg, renaming a non-existent le).
Principle: Estimate resource usage at runtime.
Admission control is a common example of resource management, whereby requests are admitted or rejected at entry to the system based on their perceived impact.
However, in systems with partitioned data, a large set of requests might be directed to the same disk holding a popular piece of data, creating a hot spot.
While at the entry point, the overall load of the requests might seem small in comparison to the system's total capacity, the localized load introduced by the requests could be substantial if all directed to a single machine (e.g., if all read from a single DN).
Additionally, the duration of one API call or background task can vary substantially; an HDFS rename call executes in a matter of milliseconds, but writing a MB data block takes many seconds even in an uncontended system.
Once admitted, a long request will continue to consume resources until completion.
If a tenant of higher priority were to suddenly start competing for resources, the lower priority requests already admitted to the system would contend with the high priority tenant for a potentially long period of time.
In this case, the system should be able to intervene to throttle, pause or cancel the lower priority requests if necessary.Our last principle stems from this: a resource management policy should be able to act on requests while they are in--ight.Principle: Schedule early, schedule oen.
Based on the challenges and principles outlined in the previous section, we argue that an eecient resource management system will require detailed and timely tracking of the resources used by each tenant and each request.
is motivates the design of Retro, a prototype framework for resource tracking and throttling in distributed systems.
At a high level, Retro collects per-tenant, per-request resource usage at diierent points as the system executes, both within processes and across nodes.
It then aggregates these in near real time.
Tenants in Retro can be both foreground clients and background tasks.
Finally, Retro provides hooks to throttle particular tenants or requests.
While Retro does not currently perform any action automatically, it is the rst step towards a complete system that implements resource management policies that are robust and eecient.
We describe Retro in more detail below, and some early results in § suggest that Retro can inform and help enforce such policies with acceptably low overhead.Tenant abstraction Retro uniformly abstracts foreground clients and background tasks as tenants.
A tenant is a collection of tasks that serve some common purpose, such as tasks from a speciic user account, heartbeat requests to detect failed nodes, or data replication tasks.
In Retro, a tenant is the granularity of resource attribution and management, and is identiied by a unique ID.End-to-End ID Propagation Borrowing from end-toend causal tracing [, , , ], Retro propagates tenant IDs along the execution path of a request, such that at any point in time, the instrumentation knows on behalf of which tenant an operation is executing.
At the beginning of a request, Retro associates the thread executing the request with a tenant by storing the tenant ID in a thread local variable, removing this association when the request completes.
e resource instrumentation queries this variable to charge the resource usage to a speciic tenant.
Retro uses AspectJ [] to automatically instrument all hardware resources and resources exposed through the Java standard library.
It captures disk and network usage by intercepting constructor and method calls on le and network streams; it tracks CPU usage by the time a thread is associated with a tenant using QueryThreadCycleTime in Windows or clock_gettime in Linux.
To capture locking we instrumented Java monitor locks and all implementers of the Lock interface, while we instrumented thread pools using Java's Executors framework.
e only manual instrumentation required is for applicationlevel resources created by the developer, such as custom queues, thread pools, or pipeline processing stages.
When a resource is intercepted, Retro determines the tenant associated with the current thread, and increments in-memory counters that track the per-tenant resource use.
ese counters include the number of resource operations started and ended, total latency spent executing in the resource and any operationspeciic statistics such as bytes read or queue time.
A separate thread reads the counters and reports the values to any subscribed clients at a regular interval.
Retro exposes hooks for a developer to specify entry and throttling points in their application.
An entry point establishes the tenant initiating the execution.
For example, in our HDFS instrumentation we added entry points in the HDFS client API, and in code that initiates background tasks.
A throttling point can impose a scheduler on queues in the system, rate-limit a tenant by pausing threads, or take advantage of more sophisticated mechanisms such as distributed rate limiters [].
For example, in our HDFS instrumentation we added throttling points at the high-level system entry points, the NameNode RPC queue, and in the DataNode block transfer threads.
Retro satisses each of the design principles outlined in §: by measuring resource consumption at runtime, it exposes the resources actually being consumed by a tenant, eschewing the need for a priori models of request types.
Retro tracks multiple resources (it is extensible to new types of resource), and end-to-end tracing allows it to distinguish the tenant responsible for resource usage within and across processes.
e entry point mechanism allows Retro to treat foreground and background tasks uniformly, and throttling points allow it to impose scheduling decisions in multiple places on the execution path.
We instrumented HDFS with Retro prototype, and show early evidence that it can be useful for resource management policies, while keeping low overhead.
We also show integrating Retro presents a low burden for developers.
Experiments e experiments outlined in § give examples of how Retro could be useful to resource management policies, showing how it exposes granular information to identify bottlenecks, how the cause of a bottleneck can then be targeted, and how both foreground and background tasks can be acted upon.
Figure demonstrates per-tenant and per-resource statistics that Retro can record in real-time.
A policy could easily identify the bottleneck resource for each of the three diierent background workloads, and which tenants are contending on that resource.
Figure shows two diierent manual interventions using Retro's throttling points, and demonstrates how an eecient resource management policy could target only the tenant responsible for congestion in order to achieve some high level goal.
Figure demonstrates that policies could act on client requests, or on internal system tasks, taking advantage of how Retro treats both as rst-class entities.
Overhead of Retro.
Retro propagates a tenant ID (( bytes) along the execution path of a request, incurring up to ns of overhead (see Table ) to serialize and deserialize when making network calls.
e overhead to record a single resource operation is approximately ns, which includes intercepting the thread, recording timing, CPU cycle count (before and aer the operation), and operation latency, and aggregating these into a per-tenant report.
To estimate the impact of Retro on throughput and end-to-end latency, we benchmark HDFS and HDFS instrumented with Retro using three diierent request types (rename, and reads of kB and MB), see minute runs.
Per-request latency increases the most for the rename API, by .
.
%, because it performs in-memory operations on the NameNode with a comparatively high number of trace points; its throughput drops by .
.
%.
e read operations slow down by .
.
% and .
.
%, respectively, because they spend most of their time reading from disk.
e average throughput of read MB increases with Retro, most likely due to the large variance observed.Developer Eort While Retro requires manual developer intervention to propagate tenant IDs across network boundaries and to verify correct behavior of Retro's automatic instrumentation, our experience shows that this requires little work.
Instrumenting resource operations for HDFS was handled automatically using AspectJ.
To instrument HDFS we only added about lines of code.
We manually instrumented HDFS's Protocol Buuer [] messages, and data transfer packets, to include Retro metadata.
We speciied entry points at the client RPC server on the NN, and in the source code where heartbeats, block replication, and block invalidation was initiated.
We also placed throttling points at these entry points.
Many shared distributed systems implement some variant of performance isolation, such as fair sharing [, , ], throttling aggressive tenants [, ], and providing latency or throughput guarantees [, , , ] Cake [] breaks HDFS requests into equal-sized chunks, then assumes disk as a bottleneck and uniform cost for each chunk.
In our communication with operators running planet-scale cloud services at Microso, unexpectedly long-running requests in new workloads oen force adjustments to admission control rules and mechanisms.In some cases, careful system design or restricted operating environments can obviate some of our observations.
For example, some approaches beneet from having xedor bounded-sized requests; IOFlow [] provides guarantees for networked storage, and enforces them at the network packet level; Cake [] explicitly sub-divides large HDFS reads into smaller, equal-sized chunks.
Amazon's DynamoDB [] hedges that uniform load distribution and exible durability guarantees are suucient to satisfy client latency requirements.In all cases we observed, the enforcement mechanisms for high-level policies were manually implemented.
For example, Cake [] manually instruments the RPC entry points of HDFS and HBase to add queues and associates tenants based on an identiier from the HDFS RPC headers; IOFlow [] modiies queues in key resources (e.g., NIC, disk driver) on the data path; and Pisces [] modiies the scheduling and queueing code of Membase and directly updates tenant weights at these queues.Banga and Druschel addressed the mismatch between OS abstractions and the needs of resource accounting with resource containers [], which, albeit in a single machine, aggregate resource usage orthogonally to processes, threads, or users.
Retro achieves per-tenant, distributed resource accounting by combining previous results in resource monitoring [], automatic source code instrumentation [], and end-to-end metadata propagation [, , ].
Retro extends the notion of a resource to arbitrarily include hardware and soware resources and its throttling point abstraction can automatically insert mechanisms such as distributed rate limiters at resources.
In this work we presented Retro, a framework for eeecting resource management decisions in distributed systems.
Retro tackles important challenges in this direction: it addresses the requirements of a robust and eecient resource management mechanism in shared distributed systems, and is a practical approach with low overheads.
We view Retro as a step to developing broader resource management policies that are applicable to a variety of systems.
It oers a platform for future work to develop such policies, and guidance to designers of alternative mechanisms.
