A FLUSH command has been used for decades to enforce persistence and ordering of updates in a storage device.
The command forces all the data in the volatile buffer of the storage device to non-volatile media to achieve per-sistency.
This lump-sum approach to flushing has two performance consequences.
First, it slows down non-volatile materialization of the writes that actually need to be made durable.
Second, it deprives the writes that do not need to be made durable of an opportunity for absorbing future writes and coalescing.
We attempt to characterize the problems of this semantic gap of flushing in storage devices and propose RFLUSH that allows a fine-grained control over non-volatile materialization.
The RFLUSH command delivers a range of logical block addresses (LBAs) that need to be flushed and thus enables the storage device to force only a subset of data in its buffer.
We implemented this fine-grained flush command in a storage device using an open-source flash development platform and modified the F2FS file system to make use of the command in processing fsync requests as a case study.
Performance evaluation using the prototype shows that the inclusion of RFLUSH improves the throughput by up to 6.5x; reduces the write traffic by up to 43%; and eliminates the long tail in the response time.
Historically, storage devices have made use of a volatile buffer for various purposes.
For hard disk drives (HDDs), the volatile buffer has been used for absorbing writes and minimizing seeks, while solid state drives (SSDs) have used the buffer for improving their random write performance and masking the limited endurance of the underlying non-volatile media [6,13,15,19,38,39,44].
The adoption of a volatile buffer, however, can bring with it data loss and improper ordering of updates in a power outage.
The FLUSH command has been introduced to resolve this issue; forcing all the pending writes to non-volatile media, ensuring persistence and proper serialization of updates.Unfortunately, this lump-sum approach to enforcing persistency has undesired performance consequences [6,13,38,39,44].
To faithfully implement the flush semantics, the storage device must empty all the dirty pages in its volatile buffer, whereas a flush request is commonly issued with less stringent requirements.
As an example, consider a concurrent execution of two applications: an on-line banking application that requires to persist each transaction immediately, and a big-data analytics application that writes a large amount of intermediate results, which is a common scenario in modern complicated and multi-tenant storage platforms.
In this scenario, a flush request for a committed transaction by the banking application will end up with forcing a large amount of dirty data (most of it from the analytics application, and thus irrelevant) in the storage device, which slows down what is actually needed (forcing the dirty data from the banking application).
This paper attempts to cure the performance problem of the conventional flush mechanism outlined above by refactoring the storage device interface.
The refactoring is to include a command called RFLUSH (Range Flush) which allows a fine-grained control over non-volatile materialization of dirty data in the buffer.
The RFLUSH command transfers a range of logical block addresses (LBAs) that specifies data to be persisted with it, helping the storage device to optimize its non-volatile materialization.
This command not only speeds up the non-volatile materialization of the target LBAs but also enhances buffering and coalescing of other dirty data in the buffer.Our work is in line with a collection of recent studies.
In the past, computer systems have been built upon a standard block device interface consisting of a small set of commands over its logical address space: read, write, and flush.
This abstract view of a storage device allows a host system to readily access non-volatile media in an efficient manner.
However, as emerging storage media such as flash memory and other non-volatile memories (NVMs) are more commonly used, the possibility of extending the conventional block device interface to leverage the full potential of the new storage media is being actively explored [1,4,7,9,23,24,27,31,32,36,46].
A TRIM command has been proposed to prevent useless data from being copied around and lowering the endurance of flash memory [36].
As another example, recent storage device interfaces support atomic writes, which can be efficiently supported in flash-based storage devices [7,31].
Also, storage interface extensions such as those for delegating block allocation [1,9,27,32,46], multi-streamed SSDs [18,29], host manageable storage devices [4,23], and user programmable SSDs [35] have been studied to provide an extended functionality and/or achieve better performance in high-end storage systems.The benefits of RFLUSH seem straightforward, but realizing it efficiently in a storage device and augmenting file systems and/or database systems to make an effective use of it are not without challenges.
We implemented RFLUSH in a storage device using an open-source flash development platform [23] and modified a file system (F2FS) [22] to make use of the extended interface 1 .
The modified F2FS uses the RFLUSH command in the handling of fsync (and its variants).
In this way, user applications do not need to be modified since the interface (i.e., fsync) and its semantics are faithfully preserved.The rest of this paper is organized as follows.
We give our motivation for RFLUSH and briefly review the related technology trends ( §2).
We then present the RFLUSH command and describe its prototype implementation ( §3).
We present results from performance evaluation using the prototype ( §4), and finally conclude ( §5).
Flush Optimization using Non-volatile Memory: Many prior works have pointed out that the in-storage buffer flush is a critical contributor to performance variation and unexpected slowdown in storage devices [15,19].
One approach to lessening the detrimental performance effects of flush is to use super capacitors for providing enough energy to force all the dirty data in the volatile buffer at the time of a power outage.
SSD manufacturers incorporate super capacitors in their high-end SSD devices to make them tolerant on power outages, offering high performance and reliability at the same time [21].
As a similar approach, Xiangfeng presents a modern SSD architecture that uses non-volatile memory for a write buffer while maintaining a read cache as volatile [44].
However, both approaches intrinsically increase the manufacturing cost, resulting in lower competitiveness of the intended products.
The two approaches are, however, complementary to RFLUSH in the sense that they allow the RFLUSH command to return immediately while giving priority for replacement to those dirty data that were the target of the command to make room in the buffer for future writes.Flush Optimization in a Host: The problem of flushing mechanism also exists in a page cache between a host and a device, because the page cache adopts a flushing mechanism to ensure persistence and ordering of up-dates in its volatile buffer.
As opposed to a storage intreface, POSIX file system interfaces provide fine-grained control over the flushing mechanism through fsync and fdatasync system calls, in addition to sync.
However, the flushing activity is still costly in a larger size page cache, and thus there have been numerous studies to mitigate this problem.
Likewise as on the storage side, specialized hardware such as battery-backed main memory has been considered to avoid flushing cost [5,43].
As a software-based approach, Nightingale et al. present an externally synchronized file system called xsyncfs [30], which allows an application to avoid blocking during the long-latency synchronization.
The xsyncfs allows a requesting application to immediately return from the synchronization request, but makes the updates visible when they become consistently durable, leading to improvement in responsiveness.
Chidambaram et al. present a new crash-consistency protocol that decouples ordering and durability, thereby providing data consistency with high performance [6].
Instead of forcing a low-level disk promptly to flush its buffer, they allow a storage device to optimize a flushing mechanism within a time limit, while still satisfying the ordering constraints.
While such optimization obtained through a trade-off between durability and performance is worthwhile to consider in storage interface extension, this paper, as an initial and fundamental approach, focuses on storage interfaces for enhancing performance without any compromising of durability.SSD Trends: The demand to improve the flush interface is particularly high at this moment because the cost of a flush is amplified when it is combined with nextgeneration SSD technologies.
As host interfaces such as the NVMe [10,16] become fast, the performance bottleneck is being shifted from the host interface to the flash device.
The flash memory latencies for reading, programming, and erasing are also steadily increasing although device density is improving.
Therefore, the latest SSDs attempt to use an increasingly larger buffer (e.g., 512 MB to 2 GB) to compensate for flash memory's low performance and endurance [25,33,34,37].
With this trend, it is obvious that cache flushing results in more serious performance degradations in the presence of a larger buffer.Besides, there are SSDs that exploit a portion of the host memory as a dedicated in-storage buffer, which may seriously suffer from cache flushing.
Such SSDs help to improve the performance while cutting off the cost by not using DRAM in the storage device [8,33], but a tandem with a classical flush interface might incur GBs of data being flushed from the host to the storage device on a regular basis.
Considering the high cost of data transfer between the host and the device, the existing flush mechanism would degrade the storage performance severely.
Also, the page size of flash memory is getting bigger, which will affect the overall performance as an eager flushing forfeits the possibility of consolidation and realignment of pending writes, yielding a large number of underutilized pages [20].
High Demand on Isolation The need for improving the flush interface is also evident with respect to performance isolation.
With the latest innovations of data centers, computation is rapidly being moved from standalone desktops to cloud systems.
With this trend, performance isolation and accurate accounting across applications are more important than ever.
Techniques for isolating storage performance on the host side have been researched extensively.
IceFS isolates related data with a container-based grouping and eliminates shared physical resources or access dependencies among containers in a file system [13].
Differentiated Storage Services (DSS) [26] and IOFlow [41] propose to tag data across layers to determine which process issues a request at any given layer.
Yang et al. present a split-level I/O scheduling framework that provides a set of hooks for acquiring knowledge needed for accurate accounting and fair scheduling [45].
However, not much research has been performed on the storage side to prevent interference among applications.
Prior works on in-storage buffers mostly focus on the replacement policy [15,19], and there is not much previous research on curing the inefficiency of the flush mechanism despite its huge impact on the performance and endurance of the storage device.
We believe our analysis and proposal in this paper are highly timely and contribute to driving the storage interface to be in harmony with fast-advancing storage technologies.
The concept of RFLUSH is simple but there are many design issues to be addressed since it involves from the application down to the storage device.
In §3.1, we discuss places where RFLUSH can be useful.
Then, in §3.2, we explain how to identify data related to RFLUSH.
Data associated with RFLUSH is not limited to user data but includes metadata.
In §3.3, we discuss how to handle metadata for RFLUSH.
We describe how to integrate RFLUSH into storage protocols in §3.4.
Since RFLUSH is more general than its counterpart FLUSH and allows finer-grained control over what to flush, there can be many use cases where it can be effective.
In this paper, we focus on its use for optimizing the fsync and fdatasync system calls.
(Hereafter we use fsync to denote both fsync and fdatasync.)
There are some obvious benefits in implementing fsync using RFLUSH.
First, no application modifications are needed since the fsync semantics can be faithfully preserved.
Second, information about the user data and metadata that are affected by the fsync is readily available.
Third, there can be noticeable performance gains from isolating regions to flush by fsync.Although we leave for future research the use of RFLUSH by the file system itself other than in the processing of the fsync, we can easily identify other potential use cases for RFLUSH.
For example, many file systems use journaling for recovery purposes and they typically use write-ahead logging (WAL) [28] that requires logging be performed before the logged updates are written to their home locations.
The RFLUSH command can be used to give priority to the non-volatile materialization of data in the log.
The same write-ahead logging is used by almost all the database systems today and they can be equally benefited by the use of RFLUSH.
The next challenge in using RFLUSH lies in how to identify the associated data for a given fsync request.
The file system needs to identify the set of pages that are associated with a file and thus has to be forced to persist.
Among such pages, some are in the page cache in a dirty state.
The file system can flush such pages to the storage device followed by an RFLUSH command targeting them.
A problematic case is when some of the pages that need to be forced to persist have already been sent to storage, meaning that they can be either in a clean state or evicted from a page cache.
Unfortunately, it is overly intricate to keep track of such data blocks, but if they are missing, the semantics of the fsync system call can be violated.We address this challenge by specifying whole data blocks of a file.
This approximation is made efficient by fundamental file-system design principles; most file systems allocate data blocks for a file as consecutively as possible so as to benefit from spatial locality [14].
This idea has been adopted to reduce the seek time for HDDs, but it holds true for SSDs as well since a high degree of spatial locality means better performance for SSDs because it allows for more efficient address translation and interleaving over multiple channels/chips in the SSD.
With this policy, the data blocks of a file are likely to be encoded by only a few extents, which means only a small number of RFLUSH commands are needed.However, this might not always be the case because there could be more fragmentations over time, in particular for larger files.
To address this, our final design choice is to transfer the inode number of the target file, instead of a set of LBAs.
This approach can faithfully preserve the fsync semantics, without excessive over-head needed to specify the range of data blocks to persist with RFLUSH.
The implementation details of the inodebased RFLUSH protocol will be described in Section 3.4.
One thing that must not be overlooked is to flush file system metadata that has a dependency on the target file of the fsync; otherwise, there is a danger of data corruption or loss on a system crash.
We explain using the F2FS file system as an example.
The on-disk layout of F2FS has two areas; metadata area and main area.
The metadata area keeps information for file system maintenance such as block allocation bitmaps and orphan inode lists [22].
In contrast, the main area is used to store normal data blocks and file metadata including inode and indirect blocks.
Upon a write request, a set of blocks needs to be updated in an atomic manner to provide crash consistency [3].
Specifically, since F2FS is a log-structured file system, it allocates and updates a new data block outof-place, requiring the updating of related metadata (i.e., inode) and indirect blocks to properly point to the new block.
In turn, the block allocation bitmap and several tables that maintain information for space management should also be updated.
This behavior leads to many small random writes to blocks containing the file system metadata; encoding of those writes as a set of ranges would be complicated.
To get away with this complication, we decide to encode a full range of the metadata area, which is a superset of metadata to be updated, and send it along with the RFLUSH command.This approximation seems to have a problem when fsync requests from multiple files are interfered with each other because their metadata shares a single LBA.
Consider a case in which there are two different files A and B, whose inode structures are located in a single block.
When the fsync requests occur for the files concurrently, forcing the entire metadata area by one fsync request might corrupt data integrity, violating ordering constraints between data and metadata of another file (e.g., file B's metadata is persisted before file B's data block).
However, this is not the case because current file systems are carefully designed so as not to let this happen.
For example, F2FS logs individual inode structure on an update, instead of an entire block, thereby preventing undesired interference that can be caused by interleaved fsyncs.
Ext4 resolves this issue by forcing all dependant data prior to persisting the modified metadata block.
Thus, in the above example, both data A and B are flushed to non-volatile storage before the metadata block when an fsync request occurs either for file A or B. To make use of the RFLUSH primitive, the host interface should be extended.
While this extension is difficult to be incorporated into mature storage interfaces such as SATA [12] or SAS [17], it is a viable option for emerging storage interfaces like NVMe [10,16] to add proprietary extensions.
Another possibility for incorporating extensions into the standard storage API is to use the open-channel SSD architecture [4,23].
In this architecture, the host system implements many of the functionalities needed to manage flash memory (e.g., garbage collection).
Also, by utilizing veiled information behind the storage device interface, this architecture enables the management of flash memory to meet the demands of the host system.
We use the latter approach since the hostmanageable architecture allows easy integration of the extensions for RFLUSH.Our prototyping system implements the inode-based RFLUSH protocol through storage interface extension and F2FS file system modification.
We add the range flush protocol to BlueDBM, which is an open-channel flash development platform from MIT [23], facilitating the construction of a host-manageable storage device.
Specifically, we extend the host storage interface to support the RFLUSH primitive in which the inode number is encoded.
Then, we augment the in-storage buffer handler in the FTL to locate the associated data blocks and flush them selectively upon an RFLUSH request.
The buffer handler maintains the pending updates in a hash table using an inode number as a key.
Note that this mechanism requires a write command that also includes an inode number such that the device controller determines which file the data block belongs to.
However, the openchannel SSD half of which the FTL runs on the host side can easily determine this by referencing the kernel data structure with the transferred write request, which is used in our implementation.On the host side, F2FS, the modified file system, communicates with BlueDBM through a block device interface and makes use of the RFLUSH primitive in implementing the fsync system call.
When an fsync request arrives from the application, F2FS writes all dirty pages of the requested file and the associated metadata from a page cache to a storage device.
Then, F2FS issues a pair of RFLUSH commands that include the inode numbers associated with the target file and the metadata area.
The RFLUSH command is forwarded to the storage device controller through the underlying block I/O layer and device driver where a host side component of BlueDBM runs.
BlueDBM completes the RFLUSH request by forcing writes associated with the given inode number.
We evaluate the proposed RFLUSH using a prototype implementation.
The next section explains our evaluation methodology.
In §4.2, we report results on the effectiveness of RFLUSH from experiments using both micro-and macro-benchmarks.
We modified both the file system (F2FS) [22] and the storage device (BlueDBM) [23] to implement the RFLUSH protocol in Linux 4.7.2.
Figure 1 shows the architecture of our experimental platform.
When the user issues an fsync request through the system call interface, the sync handler module inside the file system generates RFLUSH commands to BlueDBM.
The range flush handler module within the FTL of BlueDBM handles the request by forcing the associated data from its volatile buffer to the non-volatile media.Our experiments were performed on Intel Core i7 running at 3.3GHz with 64GB of DDR4 memory.
The detailed configurations of BlueDBM are given in Table 1.
To understand the performance consequence of the RFLUSH primitive, we first evaluate the prototype using a micro-benchmark based on FIO [11], which generates a synthetic workload that models a best-case scenario for RFLUSH.
Then, we use a set of macro-benchmarks to examine the effectiveness of RFLUSH in a real environment.
In our experiments, the storage device is accessed in a direct mode (unless otherwise specified) to observe the behavior of RFLUSH more clearly in a controlled environment.
The performance is measured five times for each scenario and their median is reported.
Micro-Benchmark: To assess the potential performance gain made possible by RFLUSH, we used a microbenchmark based on FIO [11] that approximates a typical scenario where there is a mixture of asynchronous and synchronous writes.
The micro-benchmark consists of both syncing and non-syncing threads.
Both types of thread perform the same task except for their syncing behavior.
Both write 2GB data randomly to a file with a 4KB granularity in a direct mode.
The difference is a syncing thread issues an fsync request after writing a The write traffic is measured at the interface between the in-storage buffer and the flash memory when the buffer size is 1GB.
RFLUSH reduces write traffic by 24% to 43% for the fsync periods we considered.given amount of data.In the experiment, there were one syncing thread and 12 non-syncing threads, and we measured their performances for three possible configurations: FLUSH, RFLUSH, and NOFLUSH.
The FLUSH configuration forces to flash memory all data in the volatile buffer of the storage device, while the RFLUSH configuration forces only the data in a given LBA range.
In the NOFLUSH configuration, the storage device ignores all the sync requests.
In all configurations, if the number of dirty pages in the buffer is above a threshold (90% here), a certain number of pages are written-back to flash memory by a background activity in the storage device.
Figure 2 shows the performance of both the syncing and non-syncing threads in terms of IOPS and response time.
In the figures of the top row, the X-axis is the amount of data written before the syncing thread issues an fsync request.The results show that there is a large performance improvement for the syncing thread when RFLUSH is used instead of FLUSH.
This performance improvement is mainly due to the fact that the flushing activities of the syncing thread are not interfered by the flushing of nonurgent writes from non-syncing threads when RFLUSH is used.
For the same reason, RFLUSH also eliminates a long tail in the response time distribution for the syncing thread, which is critical to providing a consistent performance from a storage device.The results also show that even the performance of non-syncing threads is improved.
When RFLUSH is used, a prioritized flushing of data written by the syncing thread gives more time for the dirty data from nonsyncing threads to reside in the buffer.
The increased time in the buffer allows them to absorb more writes to the same LBA and also to be coalesced more with other writes, resulting in a better performance.
As a result, RFLUSH reduces the write traffic significantly compared to FLUSH as Figure 3 illustrates.
Its result even comes close to that of NOFLUSH.
In this scenario, each three of the 12 non-syncing threads access the same file, while a syncing thread accesses its own file.
Thus, the writes of the non-syncing thread have a locality.
F2FS basically updates the data in an out-of-place manner, but it allows overwrite once the data is copied for updates after the last checkpoint, unless the explicit fsync request occurs.
Therefore, F2FS benefits from the enhanced buffering effect of the RFLUSH primitive in the writes of non-syncing threads.A somewhat non-intuitive result is that when the fsync requests are issued too frequently, in some extreme cases RFLUSH even performs worse than FLUSH even though the former results in much less write traffic to the storage device.
Careful analysis over the results reveals that if fsyncs are too frequent, the performance is dominated by fsyncs rather than the actual write traffic associated with them.
Macro-Benchmarks: To assess the performance impact of RFLUSH in the real world, we selected three macro-benchmarks (Fileserver, Linkbench, and TPC-C) and measured their performances when a pair of them run concurrently.
Fileserver generates a large number of asynchronous writes acting like a multi-streaming server [40].
Linkbench is a graph processing application based on the Facebook Social Graph, containing a few kilobytes of writes with frequent sync requests [2].
TPC-C is an on-line transaction processing benchmark which issues small-sized random writes with frequent synchronization [42].
Table 2 summarizes various statistics about the three macro-benchmarks.
Figure 4 shows the results in terms of IOPS for each pair of the three macro-benchmarks.
The results show that the performance improvement by RFLUSH is most noticeable when asynchronous and synchronous workloads are mixed, as in the micro-benchmark we considered in the previous section.
For example, TPC-C and Linkbench show 4.5x and 6.5x higher IOPS with RFLUSH, when they run together with Fileserver, which is consistent with the micro-benchmark results in the previous section.The results also show that there are performance improvements even in the case where both benchmarks contain synchronous workloads.
For example, when TPC-C and Linkbench are running together, RFLUSH improves performance by up to 1.4x and 1.29x in TPC-C and Linkbench, respectively.
This result is due to timemultiplexed non-volatile materializations for fsyncs from the two benchmarks.
One counter-intuitive observation is that an RFLUSH outperforms a NOFLUSH in a mixture of Fileserver and TPC-C/Linkbench with a 1024MB buffer.
This improvement comes from that an RFLUSH replenishes free space more quickly by proactively writing back the buffered data on a synchronization request, which helps the efficent handling of the bulky writes generated from Fileserver.We also performed the same experiments in a buffered mode (i.e., with the page cache turned on).
Figure 5 reports the performance in the same format as in Figure 4.
Although the absolute values are different, the results show the same general trends as in a direct mode shown in Figure 4.
The performance gap between RFLUSH and FLUSH is reduced because of periodic flushing from the page cache but the difference is only marginal.
In this paper, we raised an issue about the negative performance impact of a lump-sum approach to persisting buffered data within a storage device and presented RFLUSH that allows a fine-grained persistence control.
We implemented an RFLUSH prototype by modifying a file system (F2FS) in Linux 4.7.2 as well as a storage device based upon an open-source flash development platform.
Performance evaluation using the prototype shows that RFLUSH increases overall I/O performance by up to 6.5x, and eliminates a long tail latency of synchronous writes.
We thank Ming Zhao (our shepherd) and the anonymous reviewers for their insightful comments.
This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education ( No. 2017R1D1A1B03031494) and by the Ministry of Science, ICT & Future Planning (No. 2014R1A1A3053505 andNo.
NRF-2017R1E1A1A01077410).
