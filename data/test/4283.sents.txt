In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access.
We taxonomize model extraction attacks around two objectives: accuracy, i.e., performing well on the underlying learning task, and fidelity, i.e., matching the predictions of the remote victim classifier on any input.
To extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model.
Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model-i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs.
Addressing these limitations , we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights.
We perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images.
In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.
Machine learning, and neural networks in particular, are widely deployed in industry settings.
Models are often deployed as prediction services or otherwise exposed to potential adversaries.
Despite this fact, the trained models themselves are often proprietary and are closely guarded.There are two reasons models are often seen as sensitive.
First, they are expensive to obtain.
Not only is it expensive to train the final model [1] (e.g., Google recently trained a model with 340 million parameters on hardware costing 61,000 USD per training run [2]), performing the work to identify the optimal set of model architecture, training algorithm, and hyper-parameters often eclipses the cost of training the final model.
Further, training these models also requires investing in expensive collection process to obtain the training datasets necessary to obtain an accurate classifier [3][4][5][6].
Second, there are security [7,8] and privacy [9,10] concerns for revealing trained models to potential adversaries.Concerningly, prior work found that an adversary with query access to a model can steal the model to obtain a copy that largely agrees with the remote victim models [8,[11][12][13][14][15][16].
These extraction attacks are therefore important to consider.In this paper, we systematize the space of model extraction around two adversarial objectives: accuracy and fidelity.
Accuracy measures the correctness of predictions made by the extracted model on the test distribution.
Fidelity, in contrast, measures the general agreement between the extracted and victim models on any input.
Both of these objectives are desirable, but they are in conflict for imperfect victim models: a high-fidelity extraction should replicate the errors of the victim, whereas a high-accuracy model should instead try to make an accurate prediction.
At the high-fidelity limit is functionally-equivalent model extraction: the two models agree on all inputs, both on and off the underlying data distribution.While most prior work considers accuracy [7,11,13], we argue that fidelity is often equally important.
When using model extraction to mount black-box adversarial example attacks [7], fidelity ensures the attack is more effective because more adversarial examples transfer from the extracted model to the victim.
Membership inference [9,10] benefits from the extracted model closely replicating the confidence of predictions made by the victim.
Finally, a functionally-equivalent extraction enables the adversary to inspect whether internal representations reveal unintended attributes of the input-that are statistically uncorrelated with the training objective, enabling the adversary to benefit from overlearning [17].
We design one attack for each objective.
First, a learningbased attack, which uses the victim to generate labels for training the extracted model.
While existing techniques already achieve high accuracy, our attacks are 16× more queryefficient and scale to larger models.
We perform experiments that surface inherent limitations of learning-based extraction attacks and argue that learning-based strategies are ill-suited to achieve high-fidelity extraction.
Then, we develop the first practical functionally-equivalent attack, which directly recovers a two-layer neural network's weights exactly given access to double-precision model inference.
Compared to prior work, which required a high-precision power side-channel [18] or access to model gradients [19], our attack only requires inputoutput access to the model, while simultaneously scaling to larger networks than either of the prior methods.We make the following contributions:• We taxonomize the space of model extraction attacks by exploring the objective of accuracy and fidelity.
• We improve the query efficiency of learning attacks for accuracy extraction and make them practical for millionsof-parameter models trained on billions of images.
• We achieve high-fidelity extraction by developing the first practical functionally-equivalent model extraction.
• We mix the proposed methods to obtain a hybrid method which improves both accuracy and fidelity extraction.
We consider classifiers with domain X ⊆ R d and range Y ⊆ R K ; the output of the classifier is a distribution over K class labels.
The class assigned to an input x by a classifier f is arg max i∈ [K] f (x) i (for n ∈ Z, we write [n] = {1, 2, . . . n}).
In order to satisfy the constraint that a classifier's output is a distribution, a softmax σ(·) is typically applied to the output of an arbitrary function f L : X → R K :σ( f L (x)) i = exp( f L (x) i ) ∑ j exp( f L (x) j ).
We call the function f L (·) the logit function for a classifier f .
To convert a class label into a probability vector, it is common to use one-hot encoding: for a value j ∈ [K], the one-hot encoding OH( j; K) is a vector in R K with OH( j; K) i = 1(i = j)-that is, it is 1 only at index j, and 0 elsewhere.
Model extraction concerns reproducing a victim model, or oracle, which we write O : X → Y .
The model extraction adversary will run an extraction algorithm A(O), which outputs the extracted modeî O.
We will sometimes parameterize the oracle (resp.
extracted model) as O θ (resp.
ˆ O θ ) to denote that it has model parameters θ-we will omit this when unnecessary or apparent from context.In this work, we consider O andˆOandˆ andˆO to both be neural networks.
A neural network is a sequence of operationsalternatingly applying linear operations and non-linear operations-a pair of linear and non-linear operations is called a layer.
Each linear operation projects onto some space R hthe dimensionality h of this space is referred to as the width of the layer.
The number of layers is the depth of the network.
The non-linear operations are typically fixed, while the linear operations have parameters which are learned during training.
The function computed by layer i, f i (a), is therefore computed as f i (a) = g i (A (i) a + B (i) ), where g i is the ith non-linear function, and A (i) , B (i) are the parameters of layer i (A (i) is the weights, B (i) the biases).
A common choice of activation is the rectified linear unit, or ReLU, which sets ReLU(x) = max(0, x).
Introduced to improve the convergence of optimization when training neural networks, the ReLU activation has established itself as an effective default choice for practitioners [20].
Thus, we consider primarily ReLU networks in this work.
The network structure described here is called fully connected because each linear operation "connects" every input node to every output node.
In many domains, such as computer vision, this is more structure than necessary.
A neuron computing edge detection, for example, only needs to use information from a small region of the image.
Convolutional networks were developed to combat this inefficiency-the linear functions become filters, which are still linear, but are only applied to a small (e.g., 3x3 or 5x5) window of the input.
They are applied to every window using the same weights, making convolutions require far fewer parameters than fully connected networks.Neural networks are trained by empirical risk minimiza-tion.
Given a dataset of n samples D = {x i , y i } n i=1 ⊆ X × Y ,training involves minimizing a loss function L on the dataset with respect to the parameters of the network f .
A common loss function is the cross-entropy loss H for a sample (x, y):H(y, f (x)) = − ∑ k∈[K] y k log( f (x) k ),where y is the probability (or one-hot) vector for the true class.
The cross-entropy loss on the full dataset is thenL(D; f ) = 1 n n ∑ i=1 H(y i , f (x i )) = − 1 n n ∑ i=1 ∑ k∈[K] y k log( f (x) k ).
The loss is minimized with some form of gradient descent, often stochastic gradient descent (SGD).
In SGD, gradients of parameters θ are computed over a randomly sampled batch B, averaged, and scaled by a learning rate η:θ t+1 = θ t − η |B| ∑ i∈B ∇ θ H(y i , f (x i )).
Other optimizers [21][22][23] use gradient statistics to reduce the variance of updates which can result in better performance.
A less common setting, but one which is important for our work, is when the target values y which are used to train the network are not one-hot values, but are probability vectors output by a different model g(x).
When training using the We now address the spectrum of adversaries interested in extracting neural networks.
As illustrated in Table 1, we taxonomize the space of possible adversaries around two overarching goals-theft and reconnaissance.
We detail why extraction is not always practically realizable by constructing models that are impossible to extract, or require a large number of queries to extract.
We conclude our threat model with a discussion of how adversarial capabilities (e.g., prior knowledge of model architecture or information returned by queries) affect the strategies an adversary may consider.
Model extraction attacks target the confidentiality of a victim model deployed on a remote service.
A model refers here to both the architecture and its parameters.
Architectural details include the learning hypothesis (i.e., neural network in our case) and corresponding details (e.g., number of layers and activation functions for neural networks).
Parameter values are the result of training.First, we consider theft adversaries, motivated by economic incentives.
Generally, the defender went through an expensive process to design the model's architecture and train it to set parameter values.
Here, the model can be viewed as intellectual property that the adversary is trying to steal.
A line of work has in fact referred to this as "model stealing" [11].
In the latter class of attacks, the adversary is performing reconnaissance to later mount attacks targeting other security properties of the learning system: e.g., its integrity with adversarial examples [7], or privacy with training data membership inference [9,10].
Model extraction enables an adversary previously operating in a black-box threat model to mount attacks against the extracted model in a white-box threat model.
The adversary has-by design-access to the extracted model's parameters.
In the limit, this adversary would expect to extract an exact copy of the oracle.The goal of exact extraction is to producêO θ = O θ , sothat the model's architecture and all of its weights are identical to the oracle.
This definition is purely a strawman-it is the strongest possible attack, but it is fundamentally impossible for many classes of neural networks, including ReLU networks, because any individual model belongs to a large equivalence class of networks which are indistinguishable from input-output behavior.
For example, we can scale an arbitrary neuron's input weights and biases by some c > 0, and scale its output weights and biases by c −1 ; the resulting model's behavior is unchanged.
Alternatively, in any intermediate layer of a ReLU network, we may also add a dead neuron which never contributes to the output, or might permute the (arbitrary) order of neurons internally.
Given access to input-output behavior, the best we can do is identify the equivalence class the oracle belongs to.
This perspective yields a natural spectrum of realistic adversarial goals characterizing decreasingly precise extractions.
The goal of functionally equivalent extraction is to construct anˆOanˆanˆO such that ∀x ∈ X , ˆ O(x) = O(x).
This is a tractable weakening of the exact extraction definition from earlier-it is the hardest possible goal using only input-output pairs.
The adversary obtains a member of the oracle's equivalence class.
This goal enables a number of downstream attacks, including those involving inspection of the model's internal representations like overlearning [17], to operate in the white-box threat model.
Given some target distribution D F over X , and goal similarity function S(p 1 , p 2 ), the goal of fidelity extraction is to construct anˆOanˆ anˆO that maxi-mizes Pr x∼D F S( ˆ O(x),O(x)).
In this work, we consider only label agreement, where S(p 1 , p 2 ) = 1(argmax(p 1 ) = arg max(p 2 )); we leave exploration of other similarity functions to future work.A natural distribution of interest D F is the data distribution itself-the adversary wants to make sure the mistakes and correct labels are the same between the two models.
A reconnaissance attack for constructing adversarial examples would care about a perturbed data distribution; mistakes might be more important to the adversary in this setting.
Membership inference would use the natural data distribution, including any outliers.
These distributions tend to be concentrated on a low-dimension manifold of X , making fidelity extraction significantly easier than functionally equivalent extraction.
Indeed, functionally equivalent extraction achieves a perfect fidelity of 1 on all distributions and all similarity functions.
Task Accuracy Extraction For the true task distribution D A over X × Y , the goal of task accuracy extraction is to construct anˆOanˆanˆO maximizing Pr (x,y)∼D A arg max( ˆ O(x)) = y .
This goal is to match (or exceed) the accuracy of the target model, which is the easiest goal to consider in this taxonomy (because it doesn't need to match the mistakes of O).
Existing Attacks In Table 1, we fit previous model extraction work into this taxonomy, as well as discuss their techniques.
Functionally equivalent extraction has been considered for linear models [8,13], decision trees [11], both given probabilities, and neural networks [19,25], given extra access.
Task accuracy extraction has been considered for linear models [11] and neural networks [12,16,19], and fidelity extraction has also been considered for linear models [11] and neural networks [7,15].
Notably, functionally equivalent attacks require model-specific techniques, while task accuracy and fidelity typically use generic learning-based approaches.
Before we consider adversarial capabilities in Section 3.4 and potential corresponding approaches to model extraction, we must understand how successful we can hope to be.
Here, we present arguments that will serve to bound our expectations.
First, we will identify some limitations of functionally equivalent extraction by constructing networks which require arbitrarily many queries to extract.
Second, we will present another class of networks that cannot be extracted with fidelity without querying a number of times exponential in its depth.
We provide intuition in this section and later prove these statements in Appendix A.Exponential hardness of functionally equivalent attacks.
In order to show that functionally equivalent extraction is intractable in the worst case, we construct of a class of neural networks that are hard to extract without making exponentially many queries in the network's width.
The precision p is the number of possible values a feature can take from [0,1].
In images with 8-bit pixels, we have p = 256.
The intuition for this theorem is that a width 3k network can implement a function that returns a non-zero value on at most a p −k fraction of the space.
In the worst case, p k queries are necessary to find this fraction of the space.Note that this result assumes the adversary can only observe the input-output behavior of the oracle.
If this assumption is broken then functionally equivalent extraction becomes practical.
For example, Batina et al. [25] perform functionally equivalent extraction by performing a side channel attack (specifically, differential power analysis [26]) on a microprocessor evaluating the neural network.We also observe in Theorem 2 that, given white-box access to two neural networks, it is NP-hard in general to test if they are functionally equivalent.
We do this by constructing two networks that differ only in coordinates satisfying a subset sum instance.
Then testing functional equivalence for these networks is as hard as finding the satisfying subset.Theorem 2 (Informal).
Given their weights, it is NP-hard to test whether two neural networks are functionally equivalent.Any attack which can claim to perform functionally equivalent extraction efficiently (both in number of queries used and in running time) must make some assumptions to avoid these pathologies.
In Section 6, we will present and discuss the assumptions of a functionally equivalent extraction attack for two-layer neural network models.Learning approaches struggle with fidelity.
A final difficulty for model extraction comes from recent work in learnability [27].
Das et al. prove that, for deep random networks with input dimension d and depth h, model extraction approaches that can be written as Statistical Query (SQ) learning algorithms require exp(O(h)) samples for fidelity extraction.
SQ algorithms are a restricted form of learning algorithm which only access the data with noisy aggregate statistics; many learning algorithms, such as (stochastic) gradient descent and PCA, are examples.
As a result, most learning-based approaches to model extraction will inherit this inefficiency.
A sample-efficient approach therefore must either make assumptions about the model to be extracted (to distinguish it from a deep random network), or must access its dataset without statistical queries.Theorem 3 (Informal [27]).
Random networks with domain {0, 1} d and range {0, 1} and depth h require exp(O(h)) samples to learn in the SQ learning model.
We organize an adversary's prior knowledge about the oracle and its training data into three categories-domain knowledge, deployment knowledge, and model access.
Domain knowledge describes what the adversary knows about the task the model is designed for.
For example, if the model is an image classifier, then the model output should not change under standard image data augmentations, such as shifts, rotations, or crops.
Usually, the adversary should be assumed to have as much domain knowledge as the oracle's designer.In some domains, it is reasonable to assume the adversary has access to public task-relevant pretrained models or datasets.
This is often the case for learning-based model extraction, which we develop in Section 4.
We consider an adversary using part of a public dataset of 1.3 million images [4] as unlabeled data to mount an attack against a model trained on a proprietary dataset of 1 billion labeled images [28].
Learning-based extraction is hard without natural data In learning-based extraction, we assume that the adversary is able to collect public unlabeled data to mount their attack.
This is a natural assumption for a theft-motivated adversary who wishes to steal the oracle for local use-the adversary has data they want to learn the labels of without querying the model!
For other adversaries, progress in generative modeling is likely to offer ways to remove this assumption [29].
We leave this to future work because our overarching aim in this paper is to characterize the model extraction attacker space around the notions of accuracy and fidelity.
All progress achieved by our approaches is complementary to possible progress in synthetic data generation.
Deployment knowledge describes what the adversary knows about the oracle itself, including the model architecture, training procedure, and training dataset.
The adversary may have access to public artifacts of the oracle-a distilled version of the oracle may be available (such as for OpenAI GPT [30]) or the oracle may be transfer learned from a public pretrained model (such as many image classifiers [31] or language models like BERT [32]).
In addition, the adversary may not even know the features (the exact inputs to the model) or the labels (the classes the model may output).
While the latter can generally be inferred by interacting with the model (e.g., making queries and observing the labels predicted by the model), inferring the former is usually more difficult.
Our preliminary investigations suggest that these are not limiting assumptions, but we leave proper treatment of these constraints to future work.
Model access describes the information the adversary obtains from the oracle, including bounds on how many queries the adversary may make as well as the oracle's response:• label: only the label of the most-likely class is revealed.
• label and score: in addition to the most-likely label, the confidence score of the model in its prediction for this label is revealed.
• top-k scores: the labels and confidence scores for the k classes whose confidence are highest are revealed.
• scores: confidence scores for all labels are revealed.
• logits: raw logit values for all labels are revealed.In general, the more access an adversary is given, the more effective they should be in accomplishing their goal.
We instantiate practical attacks under several of these assumptions.
Limiting model access has also been discussed as a defensive measure, as we elaborate in Section 8.
We present our first attack strategy where the victim model serves as a labeling oracle for the adversary.
While many attack variants exist [7,11], they generally stage an iterative interaction between the adversary and the oracle, where the adversary collects labels for a set of points from the oracle and uses them as a training set for the extracted model.
These algorithms are typically designed for accuracy extraction; in this section, we will demonstrate improved algorithms for accuracy extraction, using task-relevant unlabeled data.
We realistically simulate large-scale model extraction by considering an oracle that was trained on 1 billion Instagram images [28] to obtain (at the time of the experiment) stateof-the-art performance on the standard image classification benchmark, ImageNet [4].
The oracle, with 193 million parameters, obtained 84.2% top-1 accuracy and 97.2% top-5 accuracy on the 1000-class benchmark-we refer to the model as the "WSL model", abbreviating the paper title.
We give the adversary access to the public ImageNet dataset.
The adversary's goal is to use the WSL model as a labeling oracle to train an ImageNet classifier that performs better than if we trained the model directly on ImageNet.
The attack is successful if access to the WSL model-trained on 1 billion proprietary images inaccessible to the adversary-enables the adversary to extract a model that outperforms a baseline model trained directly with ImageNet labels.
This is accuracy extraction for the ImageNet distribution, given unlabeled ImageNet training data.We consider two variants of the attack: one where the adversary selects 10% of the training set (i.e., about 130,000 points) and the other where the adversary keeps the entire training set (i.e., about 1.3 million points).
To put this number in perspective, recall that each image has a dimension of 224x224 pixels and 3 color channels, giving us 224 · 224 · 3 = 150, 528 total input features.
Each image belongs to one of 1,000 classes.
Although ImageNet data is labeled, we always treat it as unlabeled to simulate a realistic adversary.
The first attack is fully supervised, as proposed by prior work [11].
It serves to compare our subsequent attacks to prior work, and to validate our hypothesis that labels from the oracle are more informative than dataset labels.The adversary needs to obtain a label for each of the points it intends to train the extracted model with.
Then it queries the oracle to label its training points with the oracle's predictions.
The oracle reveals labels and scores (in the threat model from Section 3) when queried.The adversary then trains its model to match these labels using the cross-entropy loss.
We used a distillation temperature of T = 1.5 in our experiments after a random search.
Our experiments use two architectures known to perform well on image classification: ResNet-v2-50 and ResNet-v2-200.
Results.
We present results in Table 2.
For instance, the adversary is able to improve the accuracy of their model by 1.0% for ResNetv2-50 and 1.9% for ResNet_v2_200 after having queried the oracle for 10% of the ImageNet data.
Recall that the task has 1,000 labels, making these improvements significant.
The gains we are able to achieve as an adversary are in line with progress that has been made by the computer vision community on the ImageNet benchmark over recent years, where the research community improved the state-of-the-art top-1 accuracy by about one percent point per year.
1 1 https://paperswithcode.com/sota/image-classification-on-imagenet For adversaries interested in theft, a learning-based strategy should minimize the number of queries required to achieve a given level of accuracy.
A natural approach towards this end is to take advantage of advances in label-efficient ML, including active learning [33] and semi-supervised learning [34].
Active learning allows a learner to query the labels of arbitrary points-the goal is to query the best set of points to learn a model with.
Semi-supervised learning considers a learner with some labeled data, but much more unlabeled data-the learner seeks to leverage the unlabeled data (for example, by training on guessed labels) to improve classification performance.
Active and semi-supervised learning are complementary techniques [35,36]; it is possible to pick the best subset of data to train on, while also using the rest of the unlabeled data without labels.The connection between label-efficient learning and learning-based model extraction attacks is not new [11,13,15], but has focused on active learning.
We show that, assuming access to unlabeled task-specific data, semi-supervised learning can be used to improve model extraction attacks.
This could potentially be improved further by leveraging active learning, as in prior work, but our improvements are overall complementary to approaches considered in prior work.
We explore two semi-supervised learning techniques: rotation loss [37] and MixMatch [38].
Rotation loss.
We leverage the current state-of-the-art semisupervised learning approach on ImageNet, which augments the model with a rotation loss [37].
The model contains two linear classifiers from the second-to-last layer of the model: the classifier for the image classification task, and a rotation predictor.
The goal of the rotation classifier is to predict the rotation applied to an input-each input is fed in four times per batch, rotated by {0 • , 90 • , 180 • , 270 • }.
The classifier should output onehot encodings {OH(0; 4), OH(1; 4), OH(2; 4), OH(3; 4)}, respectively, for these rotated images.
Then, the rotation loss is written:L R (X; f θ ) = 1 4N N ∑ i=0 r ∑ j=1 H( f θ (R j (x i )), j)where R j is the jth rotation, H is cross-entropy loss, and f θ is the model's probability outputs for the rotation task.
Inputs need not be labeled, hence we compute this loss on unlabeled data for which the adversary did not query the model.
That is, we train the model on both unlabeled data (with rotation loss), and labeled data (with standard classification loss), and both contribute towards learning a good representation for all of the data, including the unlabeled data.We compare the accuracy of models trained with the rotation loss on data labeled by the oracle and data with ImageNet labels.
Our best performing extracted model, with MixMatch with 4000 labels performs nearly as well as the oracle for both datasets, and MixMatch at 250 queries beats fully supervised training at 4000 queries for both datasets.of 64.5%, is trained with the rotation loss on oracle labels whereas the baseline on ImageNet labels only achieves 62.5% accuracy with the rotation loss and 61.2% without the rotation loss.
This demonstrates the cumulative benefit of adding a rotation loss to the objective and training on oracle labels for a theft-motivated adversary.
We expect that as semi-supervised learning techniques on ImageNet mature, further gains should be reflected in the performance of model extraction attacks.
[40].
MixMatch uses a combination of techniques, including training on "guessed" labels, regularization, and image augmentations.For both datasets, inputs are color images of 32x32 pixels belonging to one of 10 classes.
The training set of SVHN contains 73257 images and the test set contains 26032 images.
The training set of CIFAR10 contains 50000 images and the test set contains 10000 images.
We train the oracle with a WideResNet-28-2 architecture on the labeled training set.
The oracles achieve 97.36% accuracy on SVHN and 95.75% accuracy on CIFAR10.The adversary is given access to the same training set but without knowledge of the labels.
Our goal is to validate the effectiveness of semi-supervised learning by demonstrating that the adversary only needs to query the oracle on a small subset of these training points to extract a model whose accuracy on the task is comparable to the oracle's.
To this end, we run 5 trials of fully supervised extraction (no use of unlabeled data), and 5 trials of MixMatch, reporting for each trial the median accuracy of the 20 latest checkpoints, as done in [38].
Results.
In Table 3, we find that with only 250 queries (293x smaller label set than the SVHN oracle and 200x smaller for CIFAR10), MixMatch reaches 95.82% test accuracy on SVHN and 87.98% accuracy on CIFAR10.
This is higher than fully supervised training that uses 4000 queries.
With 4000 queries, MixMatch is within 0.29% of the accuracy of the oracle on SVHN, and 2.46% on CIFAR10.
The variance of MixMatch is slightly higher than that of fully supervised training, but is much smaller than the performance gap.
These gains come from the prior MixMatch is able to build using the unlabeled data, making it effective at exploiting few labels.
We observe similar gains in test set fidelity.
Learning-based approaches have several sources of nondeterminism: the random initializations of the model parameters, the order in which data is assembled to form batches for SGD, and even non-determinism in GPU instructions [41,42].
Non-determinism impacts the model parameter values obtained from training.
Therefore, even an adversary with full access to the oracle's training data, hyperparameters, etc., would still need all of the learner's non-determinism to achieve the functionally equivalent extraction goal described in Section 3.
In this section, we will attempt to quantify this: for a strong adversary, with access to the exact details of the training setup, we will present an experiment to determine the limits of learning-based algorithms to achieving fidelity extraction.We Table 4: Impact of non-determinism on extraction fidelity.
Even models extracted using the same SGD and initialization randomness as the oracle do not reach 100% fidelity.a new set of parameters f 2 θ (x).
If there are points x such that f 1 θ (x) = f 2 θ (x), then the prediction on x is dependent not on the oracle, but on the non-determinism of the learning-based attack strategy-we are unable to guarantee fidelity.We independently control the initialization randomness and batch randomness during training on Fashion-MNIST [43] with fully supervised SGD (we use Fashion-MNIST for training speed).
We repeated each run 10 times and measure agreement between the ten obtained models on the test set, adversarial examples generated by running FGSM with ε = 25/255 with the oracle model and the test set, and uniformly random inputs.
The oracle uses initialization seed 0 and SGD seed 0-we also use two different initialization and SGD seeds.Even when both training and initialization randomness are fixed (so that only GPU non-determinism remains), fidelity peaks at 93.7% on the test set (see Table 4).
With no randomness fixed, extraction achieves 93.4% fidelity on the test set.
(Agreement on the test set should should be considered in reference to the base test accuracy of 90%.)
Hence, even an adversary who has the victim model's exact training set will be unable to exceed ~93.4% fidelity.
Using prototypicality metrics, as investigated in Carlini et al. [44], we notice that test points where fidelity is easiest to achieve are also the most prototypical (i.e., more representative of the class it is labeled as).
This connection is explored further in Appendix B.
The experiment of this section is also related to uncertainty estimation using deep ensembles [42]; we believe a deeper connection may exist between the fidelity of learning-based approaches and uncertainty estimation.
Also relevant is the work mentioned earlier in Section 3, that shows that random networks are hard for learning-based approaches to extract.
Here, we find that learning-based approaches have limits even for trained networks, on some portion of the input space.It follows from these arguments that non-determinism of both the victim and extracted model's learning procedures potentially compound, limiting the effectiveness of using a learning-based approach to reaching high fidelity.
Having identified fundamental limitations that prevent learning-based approaches from perfectly matching the oracle's mistakes, we now turn to a different approach where the adversary extracts the oracle's weights directly, seeking to achieve functionally-equivalent extraction.This attack can be seen as an extension of two prior works.
• Milli et al. [19] introduce an attack to extract neural network weights under the assumption that the adversary is able to make gradient queries.
That is, each query the adversary makes reveals not only the prediction of the neural network, but also the gradient of the neural network with respect to the query.
To the best of our knowledge this is the only functionally-equivalent extraction attack on neural networks with one hidden layer, although it was not actually implemented in practice.
• Batina et al. [25], at USENIX Security 2019, develop a side-channel attack that extracts neural network weights through monitoring the power use of a microprocessor evaluating the neural network.
This is a much more powerful threat model than made by any of the other model extraction papers.
To the best of our knowledge this is the only practical direct model extraction result-they manage to extract essentially arbitrary depth networks.In this section we introduce an attack which only requires standard queries (i.e., that return the model's prediction instead of its gradients) and does not require any side-channel leakages, yet still manages to achieve higher fidelity extraction than the side-channel extraction work for two-layer networks, assuming double-precision inference.Attack Algorithm Intuition.
As in [19], our attack is tailored to work on neural networks with the ReLU activation function (the ReLU is an effective default choice of activation function [20]).
This makes the neural network a piecewise linear function.
Two samples are within the same linear region if all ReLU units have the same sign, illustrated in Figure 2.
By finding adjacent linear regions, and computing the difference between them, we force a single ReLU to change signs.
Doing this, it is possible to almost completely determine the weight vector going into that ReLU unit.
Repeating this attack for all ReLU units lets us recover the first weight matrix completely.
(We say almost here, because we must do some work to recover the sign of the weight vector.)
Once the first layer of the two-layer neural network has been determined, the second layer can be uniquely solved for algebraically through least squares.
This attack is optimal up to a constant factor-the query complexity is discussed in Appendix D.
As in [19], we only aim to extract neural networks with one hidden layer using the ReLU activation function.
We denote the model weights by A (0) ∈ R d×h , A (1) ∈ R h×K and biases by B (0) ∈ R h , B (1) ∈ R K .
Here, d, h, and K respectively referSymbol Definition d Input dimensionality h Hidden layer dimensionality (h < d) K Number of classes A (0) ∈ R d×hInput layer weightsB (0) ∈ R hInput layer bias A (1) ∈ R h×K Logit layer weights B (1) ∈ R K Logit layer bias Table 5: Parameters for the functionally-equivalent attack.
to the input dimensionality, the size of the hidden layer, and the number of classes.
This is found in Table 6.1.
We say that ReLU(x) is at a critical point if x = 0; this is the location at which the unit's gradient changes from 0 to 1.
We assume the adversary is able to observe the raw logit outputs as 64-bit floating point values.
We will use the notation O L to denote the logit oracle.
Our attack implicitly assumes that the rows of A (0) are linearly independent.
Because the dimension of the input space is larger than the hidden space by at least 100, it is exceedingly unlikely for the rows to be linearly dependent (and we find this holds true in practice).
Note that our attack is not an SQ algorithm, which would only allow us to look at aggregate statistics of our dataset.
Instead, our algorithm is very particular in its analysis of the network, computing the differences between linear regions, for example, cannot be done with aggregate statistics.
This structure allows us to avoid the pathologies of Section 3.3.
The algorithm is broken into four phases:• Critical point search identifies inputs {x i } n i=1 to the neural network so that exactly one of the ReLU units is at a critical point (i.e., has input identically 0).
• Weight recovery takes an input x which causes the ith neuron to be at a critical point.
We use this point x to compute the difference between the two adjacent linear regions induced by the critical point, and thus the weight vector row Ai .
By repeating this process for each ReLU we obtain the complete matrix A (0) .
Due to technical reasons discussed below, we can only recover the rowvector up to sign.
• Sign recovery determines the sign of each row-vector Aj for all j using global information about A (0) .
• Final layer extraction uses algebraic techniques (least squares) to solve for the second layer of the network.
For a two layer network, observe that the logit function is given by the equationO L (x) = A (1) ReLU(A (0) x+B (0) )+B (1) .
To find a critical point for every ReLU, we sample two random vectors u, v ∈ R d , and consider the functionL(t; u, v, O L ) = O L (u + tv).
for t varying between a small and large appropriately selected value (discussed below).
This amounts to drawing a line in the inputs of the network; passed through ReLUs, this line becomes the piecewise linear function L(·).
The points t where L(t) is non-differentiable are exactly locations where some ReLU i is changing signs (i.e., some ReLU is at a critical point).
Figure 3 shows an example of what this sweep looks like on a trained MNIST model.
Furthermore, notice that given a pair u, v, there is exactly one value t for which each ReLU is at a critical point, and if t is allowed to grow arbitrarily large or small that every ReLU unit will switch sign exactly once.
Intuitively, the reason this is true is that each ReLU's input, (say wx + b for some w, b), is a monotone function of t (w T ut + w T v + b).
Thus, by varying t, we can identify an input x i that sets the ith ReLU to 0 for every relu i in the network.
This assumes we are not moving parallel to any of the rows (where w T u = 0), and that we vary t within a sufficiently large interval (so the w T ut term may overpower the constant term).
The analysis of [19] suggests that these concerns can be resolved with high probability by varying t ∈ −h 2 , h 2 .
While in theory it would be possible to sweep all values of t to identify the critical points, this would require a large number of queries.
Thus, to efficiently search for the locations is piecewise linear, enabling a binary search.
of critical points, we introduce a refined search algorithm which improves on the binary search as used in [19].
Standard binary search requires O(n) model queries to obtain n bits of precision.
Therefore, we propose a refined technique which does not have this restriction and requires just O(1) queries to obtain high (20+ bits) precision.
The key observation we make is that if we are searching between two values [t 1 ,t 2 ] and there is exactly one discontinuity in this range, we can precisely identify the location of that discontinuity efficiently.
An intuitive diagram for this algorithm can be found in Figure 4 and the algorithm can be found in Algorithm 1.
The property this leverages is that the function is piecewise linearif we know the range is composed of two linear segments, we can identify the linear segments and compute their intersection.
In Algorithm 1, lines 1-3 describe computing the two linear regions' slopes and intercepts.
Lines 4 and 5 compute the intersection of the two lines (also shown in the red dotted line of Figure 4).
The remainder of the algorithm performs Algorithm 1 Algorithm for 2-linearity testing.
Computes the location of the only critical point in a given range or rejects if there is more than one.t 1 x t 2 t 1 x t 2 O(x) = exp.
ˆ O(x) exp.
ˆ O(x) O(x)Function f , range [t 1 ,t 2 ], εm 1 = f (t 1 +ε)− f (t 1 ) ε Gradient at t 1 m 2 = f (t 2 )− f (t 2 −ε) ε Gradient at t 2 y 1 = f (a), y 2 = f (b) x = a + y 2 −y 1 −(b−a)m 2 m 1 −m 2Candidate critical pointˆ y = y 1 + m 1 y 2 −y 1 −(b−a)m 2 m 1 −m 2Expected value at candidate y = f (x)True value at candidate ifˆyifˆ ifˆy = y then return x else return "More than one critical point" end if the correctness check, also illustrated in Figure 4; if there are more than 2 linear components, it is unlikely that the true function value will match the function value computed in line 5, and we can detect that the algorithm has failed.
After running critical point search we obtain a set {x i } h i=1 , where each critical point corresponds to a point where a single ReLU flips sign.
In order to use this information to learn the weight matrix A (0) we measure the second derivative of O L in each input direction at the points x i .
Taking the second derivative here corresponds to measuring the difference between the linear regions on either side of the ReLU.
Recall that prior work assumed direct access to gradient queries, and thus did not require any of the analysis in this section.
To formalize the intuition of comparing adjacent hyperplanes, observe that for the oracle O L and for a critical point x i (corresponding to ReLU i being zero) and for a random input-space direction e j we have∂ 2 O L ∂e 2 j x i = ∂O L ∂e j x i +c·e j − ∂O L ∂e j x i −c·e j = ∑ k A (1) k 1(A (0) k (x i + c · e j ) + B (0) k > 0)A (0) k j − ∑ k A (1) k 1(A (0) k (x i − c · e j ) + B (0) k > 0)A (0) k j = A (1) i 1(A (0) i · e j > 0) − 1(−A (0) i · e j > 0) A (0) ji = ±(A (0) ji A (1) i )for a c > 0 small enough so that x i ± c · e j does not flip any other ReLU.
Because x i is a critical point and c is small, the sums in the second line differ only in the contribution of ReLU i .
However at this point we only have a product involving both weight matrices.
We now show this information is useful.If we compute |A(0) 1i A (1) | and |A (0)2i A (1) | by querying along directions e 1 and e 2 , we can divide these quantities to obtain the value |A(0) 1i /A (0)2i |, the ratio of the two weights.
By repeating the above process for each input direction we can, for all k, obtain the pairwise ratios |A(0) 1i /A (0) ki |.
Recall from Section 3 that obtaining the ratios of weights is the theoretically optimal result we could hope to achieve.
It is always possible to multiply all of the weights into a ReLU by a constant c > 0 and then multiply all of the weights out of the ReLU by c −1 .
Thus, without loss of generality, we can assign A (0) 1i = 1 and scale the remaining entries accordingly.
Unfortunately, we have lost a small amount of information here.
We have only learned the absolute value of the ratio, and not the value itself.
Once we reconstruct the values |A(0) ji /A (0)1i | for all j we need to recover the sign of these values.
To do this we consider the following quantity:∂ 2 O L ∂(e j + e k ) 2 x i = ±(A (0) ji A (1) i ± A (0) ki A (1) i ).
That is, we consider what would happen if we take the second partial derivative in the direction (e j +e k ).
Their contributions to the gradient will either cancel out, indicating A ji and A (0) ki are of opposite sign, or they will compound on each other, indicating they have the same sign.
Thus, to recover signs, we can perform this comparison along each direction (e 1 + e j ).
Here we encounter one final difficulty.
There are a total of n signs we need to recover, but because we compute the signs by comparing ratios along different directions, we can only obtain n − 1 relations.
That is, we now know the correct signed value of A (0) i up to a single sign for the entire row.
It turns out this is to be expected.
What we have computed is the normal direction to the hyperplane, but because any given hyperplane can be described by an infinite number of normal vectors differing by a constant scalar, we can not hope to use local information to recover this final sign bit.Put differently, while it is possible to push a constant c > 0 through from the first layer to the second layer, it is not possible to do this for negative constants, because the ReLU function is not symmetric.
Therefore, it is necessary to learn the sign of this row.
Once we have recovered the input vector's weights, we still don't know the sign for the given inputs-we only measure the difference between linear functions at each critical point, but do not know which side is the positive side of the ReLU [19].
Now, we need to leverage global information in order to reconcile all of inputs' signs.Notice that recoveringˆArecoveringˆ recoveringˆA i .
Now, to begin recovering sign, we search for a vector z that is in the null space ofˆAofˆ ofˆA (0) , that is, ˆ A (0) z = 0.
Because the neural network has h < d, the null-space is non-zero, and we can find many such vectors using least squares.
Then, for each ReLU i , we search for a vector v i such that v i A (0) = e i where here e i is the ith basis vector in the hidden space.
That is, moving along the v i direction only changes ReLU i 's input value.
Again we can search for this through least squares.Given z and these v i we query the neural network for thevalues of O L (z), O L (z + v i ), and O L (z − v i ).
On each of these three queries, all hidden units are 0 except for ReLU i which recieves as input either 0, 1, or −1 by the construction of v i .
However, notice that the output of ReLU i can only be either 0 or 1, and the two {−1, 0} cases collapse to just output 0.
Therefore, if O L (z + v i ) = O L (z), we know that A (0) i · v i < 0.
Otherwise, we will find O L (z − v i ) = O L (z) and A (0)i · v i > 0.
This allows us to recover the sign bit for ReLU i .
Given the completely extracted first layer, the logit function of the network is just a linear transformation which we can recover with least squares, through making h queries where each ReLU is active at least once.
In practice, we use the critical points discovered in the previous section so that we do not need to make additional neural network queries.
Setup.
We train several one-layer fully-connected neural networks with between 16 and 512 hidden units (for 12,000 and 100,000 trainable parameters, respectively) on the MNIST [45] and CIFAR-10 datasets [40].
We train the models with the Adam [23] optimizer for 20 epochs at batch size 128 until they converge.
We train five networks of each size to obtain higher statistical significance.
Accuracies of these networks can be found in the supplement in Appendix C.
In Section 4, we used 140,000≈ 2 17 queries for ImageNet model extraction.
This is comparable to the number of queries used to extract the smallest MNIST model in this section, highlighting the advantages of both approaches.MNIST Extraction.
We implement the functionallyequivalent extraction attack in JAX [46] and run it on each trained oracle.
We measure the fidelity of the extracted model, comparing predicted labels, on the MNIST test set.Results are summarized in Table 6.
For smaller networks, we achieve 100% fidelity on the test set: every single one of the 10, 000 test examples is predicted the same.
As the network size increases, low-probability errors we encounter become more common, but the extracted neural network still disagrees with the oracle on only 2 of the 10, 000 examples.Inspecting the weight matrix that we extract and comparing it to the weight matrix of the oracle classifier, we find that we manage to reconstruct the first weight matrix to an average precision of 23 bits-we provide more results in Appendix C.CIFAR-10 Extraction.
Because this attack is dataindependent, the underlying task is unimportant for how well the attack works; only the number of parameters matter.
The results for CIFAR-10 are thus identical to MNIST when controlling for model size: we achieve 100% test set agreement on models with fewer than 200, 000 parameters and and greater than 99% test set agreement on larger models.Comparison to Prior Work.
To the best of our knowledge, this is by orders of magnitude the highest fidelity extraction of neural network weights.The only fully-implemented neural network extraction attack we are aware of is the work of Batina et al. [25], who uses an electromagnetic side channels and differential power analysis to recover an MNIST neural network with neural network weights with an average error of 0.0025.
In comparison, we are able to achieve an average error in the first weight matrix for a similarly sized neural network of just 0.0000009-over two thousand times more precise.
To the best of our knowledge no functionally-equivalent CIFAR-10 models have been extracted in the past.We are unable to make a comparison between the fidelity of our extraction attack and the fidelity of the attack presented in Batina et al. because they do not report on this number: they only report the accuracy of the extracted model and show it is similar to the original model.
We believe this strengthens our observation that comparing across accuracy and fidelity is not currently widely accepted as best practice.Investigating Errors.
We observe that as the number of parameters that must be extracted increases, the fidelity of the model decreases.
We investigate why this happens and discovered that a small fraction of the time (roughly 1 in 10,000) the gradient estimation procedure obtains an incorrect estimate of the gradient and therefore one of the extracted weightsˆAweightsˆ weightsˆA(0) i jis incorrect by a non-insignificant margin.Introducing an error into just one of the weights of the first matrixˆAmatrixˆ matrixˆA (0) should not induce significant further errors.
However, because of this error, when we solve for the bias vector, the extracted biasˆBbiasˆ biasˆB i j .
And when the bias is wrong, it impacts every calculation, even those where this edge is not in use.Resolving this issue completely either requires reducing the failure rate of gradient estimation from 1 in 10,000 to practically 0, or would require a complex error-recovery procedure.
Instead, we will introduce in the following section an improvement which almost completely solves this issue.
# of Parameters 12,500 25,000 50,000 100,000 Table 6: Fidelity of the functionally-equivalent extraction attack across different test distributions on an MNIST victim model.
Results are averaged over five extraction attacks.
For small models, we achieve perfect fidelity extraction; larger models have near-perfect fidelity on the test data distribution, but begins to lose accuracy at 100, 000 parameters.Difficulties Extending the Attack.
The attack is specific to two layer neural networks; deeper networks pose multiple difficulties.
In deep networks, the critical point search step of Section 6.3 will result in critical points from many different layers, and determining which layer a critical point is on is nontrivial.
Without knowing which layer a critical point is on, we cannot control inputs to the neuron, which we need to do to recover the weights in Section 6.4.
Even given knowledge of what layer a critical point is on, the inputs of any neuron past layer 1 are the outputs of other neurons, so we only have indirect control over their inputs.
Finally, even with the ability to recover these weights, small numerical errors occur in the first layer extraction.
These cause errors in every finite differences computation in further layers, causing the second layer to have even larger numerical errors than the first (and so on).
Therefore, extending the attack to deeper networks will require at least solving each of the following: producing critical points belonging to a specific layer, recovering weights for those neurons without direct control of their inputs, and significantly reducing numerical errors in these algorithms.
Until now the strategies we have developed for extraction have been pure and focused entirely on learning or entirely on direct extraction.
We now show that there is a continuous spectrum from which we can draw attack strategies, and these hybrid strategies can leverage both the query efficiency of learning extraction, and the fidelity of direct extraction.
Milli et al. demonstrate that gradient matching helps extraction by optimizing the objective functionn ∑ i=1 H(O(x i ), f (x i )) + α|∇ x O(x i ) − ∇ x f (x i )| 2 2 ,assuming the adversary can query the model for ∇ x O(x).
This is more model access than we permit our adversary, but is an # of Parameters 50,000 100,000 200,000 400,000 example of using intuition from direct recovery to improve extraction.
We found in preliminary experiments that this technique can improve fidelity on small datasets (increasing fidelity from 95% to 96.5% on Fashion-MNIST), but we leave scaling and removing the model access assumption of this technique to future work.
Next, we will show another combination of learning and direct recovery, using learning to alleviate some of the limitations of the previous functionallyequivalent extraction attack.
Recall from earlier that the functionally-equivalent extraction attack fidelity degrades as the model size increases.
This is a result of low-probability errors in the first weight matrix inducing incorrect biases on the first layer, which in turn propagates and causes worse errors in the second layer.
We now introduce a method for performing a learningbased error recovery routine.
While performing a fullylearning-based attack leaves too many free variables so that functionally-equivalent extraction is not possible, if we fix many of the variables to the values extracted through the direct recovery attack, we now show it is possible to learn the remainder of the variables.Formally, letˆAletˆ letˆA (0) be the extracted weight matrix for the first layer andˆBandˆ andˆB (0) be the extracted bias vector for the first layer.
Previously, we used least squares to directly solve forˆAforˆ forˆA (1) andˆBandˆ andˆB (1) assuming we had extracted the first layer perfectly.
Here, we relax this assumption.
Instead, we perform gradient descent optimizing for parameters W 0.
.2 that minimizeE x∈D f θ (x) −W 1 ReLU( ˆ A (0) x + ˆ B (0) +W 0 ) +W 2That is, we use a single trainable parameter to adjust the bias term of the first layer, and then solve (via gradient descent with training data) for the remaining weights accordingly.This hybrid strategy increases the fidelity of the extracted model substantially, detailed in Table 8.
In the worstperforming example from earlier (with only direct extraction) the extracted 128-neuron network had 80% fidelity agreement with the victim model.
When performing learning-based recovery, the fidelity agreement jumps all the way to 99.75%.
Transferability 100% 100% 100% 100% Table 8: Transferability rate of adversarial examples using the extracted neural network from our Section 7 attack.
Adversarial examples transfer: an adversarial example [47] generated on one model often fools different models, too.Transferability is higher when the models are more similar [7].
We should therefore expect that we can generate adversarial examples on our extracted model, and that these will fool the remote oracle nearly always.
In order to measure transferability, we run 20 iterations of PGD [48] with ∞ distortion set to the value most often used in the literature: for MNIST: 0.1, and for CIFAR-10: 0.03.
The attack achieves functionally equivalent extraction (modulo floating point precision errors in the extracted weights), so we expect it to have high adversarial example transferability.
Indeed, we find we achieve a 100% transferability success rate for all extracted models.
Defenses for model extraction have fallen into two camps: limiting the information gained per query, and differentiating extraction adversaries from benign users.
Approaches to limiting information include perturbing the probabilities returned by the model [11,13,49], removing the probabilities for some of the model's classes [11], or returning only the class output [11,13].
Another proposal has considered sampling from a distribution over model parameters [13,50].
The other camp, differentiating benign from malicious users, has focused on analyzing query patterns [51,52].
Non-adaptive attacks (such as supervised or MixMatch extraction) bypass query patternbased detection, and are weakened by information limiting.
We demonstrate the impact of removing complete access to probability values by considering only access to top 5 probabilities from WSL in Table 2.
Our functionally-equivalent attack is broken by all of these measures.
We leave consideration of defense-aware attacks to future work.Queries to a model can also reveal hyperparameters [53] or architectural information [14].
Adversaries can use side channel attacks to do the same [18,25].
These are orthogonal to, but compatible with, our work-information about a model, such as assumptions made in Section 6, empowers extraction.Watermarking neural networks has been proposed [54,55] to identify extracted models.
Model extraction calls into question the utility of cryptographic protocols used to protect model weights.
One unrealized approach is obfuscation [56], where an equivalent program could be released and queried as many times as desired.
A practical approach is secure multiparty computation, where each query is computed by running a protocol between the model owner and querier [57].
This paper characterizes and explores the space of model extraction attacks on neural networks.
We focus this paper specifically around the objectives of accuracy, to measure the success of a theft-motivated adversary, and fidelity, an oftenoverlooked measure which compares the agreement between models to reflect the success of a recon-motivated adversary.Our learning-based methods can effectively attack a model with several millions of parameters trained on a billion images, and allows the attacker to reduce the error rate of their model by 10%.
This attack does not match perfect fidelity with the victim model due to what we show are inherent limitations of learning-based approaches: nondeterminism (including only the nondeterminism on the GPU) prohibits training identical models.
In contrast, our direct functionally-equivalent extraction returns a neural network agreeing with the victim model on 100% of the test samples and having 100% fidelity on transfered adversarial examples.We then propose a hybrid method which unifies these two attacks, using learning-based approaches to recover from numerical instability errors when performing the functionallyequivalent extraction attack.Our work highlights many remaining open problems in model extraction, such as reducing the capabilities required by our attacks and scaling functionally-equivalent extraction.Proof.
We will start by constructing a 3-ReLU gadget with output ≥ 1 only when a i ≤ x i ≤ b i .
We will then show how to compose k of these gadgets, one for each index of the k-rectangle, to construct the k-rectangle bounded function.The 3-ReLU gadget only depends on x i , so weights for all other ReLUs will be set to 0.
Observe that the functionT i (x; a, b) = ReLU(x−a)+ReLU(x i −b i )−2ReLU(x i − (a i + b i )/2)is nonzero only on the interval (a i , b i ).
This is easier to see when it is written asReLU(x i − a i ) − ReLU(x i − (a i + b i )/2) − (ReLU(x i − (a i + b i )/2) − ReLU(x i − b i )).
The function ReLU(x − x 1 ) − ReLU(x − x 2 ) with x 1 < x 2 looks like a sigmoid, and has the following form:ReLU(x − x 1 ) − ReLU(x − x 2 ) =      0 x ≤ x 1 x − x 1 x 1 ≤ x ≤ x 2 x 2 − x 1 x ≥ x Now, T i (x; a i , b i ) · 1/(b i − a i ) has range [0, 1] for any value of a i , b i .
Then the function f a,b (x) = ReLU( ∑ i (T i (x; a i , b i )/(b i − a i )) − (k − 1))is k-rectangle bounded for vectors a, b. To see why, we need that no input x not satisfying a x b has∑ i (T i (x; a i , b i )/(b i − a i )) > k − 1.
This is simply because each term T i (x; a i , b i ) ≤ 1, so unless all k such terms are > 0, the inequality cannot hold.
Now that we know how to construct a k-rectangle bounded function, we will introduce a set of p k disjoint k-rectangle bounded functions, and then show that any one requires p k queries to extract when the others are also possible functions.Lemma 2.
There exists a family of k-rectangle bounded functions F such that extracting an element of F requires p k queries in the worst case.Here, p is the feature precision; images with 8-bit pixels have p = 256.
Proof.
We begin by constructing F .
The following p ranges are clearly pairwise disjoint:{( i−1 p , i p )} p i=1.
Then pick any k indices, and we can construct p k distinct k-rectangle bounded functions -one for each element in the Cartesian product of each index's set of ranges.
Call this set F .
The set of inputs with non-zero output is distinct for each function, because their rectangles are distinct.
Now consider the information gained from any query.
If the query returns a non-zero value, the function is learned.
If not, at most one function from F is ruled out -the function whose rectangle was queried.
Then any sequence of n queries to an oracle can rule out at most n of the functions of F , so that at least |F | = p k queries are required in the worst case.
Proof.
We prove this by reduction to subset sum.
A similar reduction (reducing to 3-SAT instead of Subset Sum) for a different statement appears in [58].
Suppose we receive a subset sum instance T, p, [v 1 , v 2 , · · · , v d ] -the set is v, the target sum is T , and the problem's precision is p.
We will construct networks f 1 and f 2 such that checking if f 1 and f 2 are functionally equivalent is equivalent to solving the subset sum instance.
We start by setting f 1 = 0 -it never returns a non-zero value.
We now construct a network f 2 that has nonzero output only if the subset sum instance can be solved (and finding an input with nonzero output reveals the satisfying subset).
The network f 2 has three hidden units in the first layer with incoming weight for the ith feature equal to v i .
This means the dot product of the input x with weights will be the sum of the subset {i|x i = 1}.
We want to force this to accept iff there is an input where this sum is T .
To do so, we use the same 3-ReLU gadget as in the proof of Theorem 1:f 2 (x; T, p, v) = ReLU(x · v − (T − p/2)) + ReLU(x · v − (T + p/2)) − 2ReLU(x · v − T ).
As before, this will only be nonzero in the range [T − p/2, T + p/2], and we are done.
We know from Section 5 that learning strategies struggle to achieve perfect fidelity due to non-determinism inherent in learning.
What remains to be understood is whether some USENIX Association 29th USENIX Security Symposium 1361 samples are more difficult than others to achieve fidelity on.
We investigate using recent work on identifying prototypical data points.
Using each metric developed in Carlini et al. [44], we can rank the Fashion-MNIST test set in order of increasing prototypicality.
Binning the prototypicality ranking into percentiles, we can measure how many of the 90 models we trained for Section 5 agree with the oracle's prediction.
The intuition here is that more prototypical examples should be more consistently learnable, whereas more outlying points may be harder to consistently classify.
Indeed, we find that this is the case -all metrics find a correlation between prototypicality and model agreement (fidelity), as seen in Figure 5.
Interestingly, the metrics which do not use ensembles of models (adversarial distance and holdout-retraining) have the best correlation with the model agreement metric-roughly the top 50% of prototypical examples by these metrics are classified the same by nearly all 90 models.
Accuracies for the oracles in Section 6 are found in 98.3% 1,572,000 45.9% Table 9: Statistics for the oracle models we train to extract.
Figure 6 shows a distribution over the bits of precision in the difference between the logits (i.e., pre-softmax prediction) of the 16 neuron oracle neural network and the extracted network.
Formally, we measure the magnitude of the gap | f θ (x) − f ˆ θ (x)|.
Notice that this is a different (and typically stronger) measure of fidelity than used elsewhere in the paper.
In this section, we briefly analyze the query complexity of the attack from Section 6.
We assume that a simulated partial derivative requires O(1) queries using finite differences.1.
Critical Point Search.
This step is the most nontrivial to analyze, but fortunately this was addressed in [19].
They found this step requires O(h log(h)) gradient queries, which we simulate with O(h log(h)) model queries.2.
Weight Recovery.
This piece is significantly complicated by not having access to gradient queries.
For each 3.
Global Sign Recovery.
For each ReLU, we require only three queries.
Then this step is O(h).
Extraction.
This step requires h queries to make the system of linear equations full rank (although in practice we reuse previous queries here, making this step require 0 queries).
Overall, the algorithm requires O(h log(h) + dh + h) = O(dh) queries.
Extraction requires Ω(dh) queries without auxillary information, as there are dh parameters in the model.
Then the algorithm is query-optimal up to a constant factor, removing logarithmic factors from Milli et al. [19].
We would like to thank Ilya Mironov for lengthy and fruitful discussions regarding the functionally equivalent extraction attack.
We also thank Úlfar Erlingsson for helpful discussions on positioning the work, and Florian Tramèr for his comments on an early draft of this paper.
3Here, we give the formal arguments for the difficulty of model extraction to support informal statements from Section 3.3.
Theorem 1.
There exists a class of width 3k and depth 2 neural networks on domain [0,1] d (with precision p numbers) with d ≥ k that require, given logit access to the networks, Θ(p k ) queries to extract.In order to prove Theorem 1, we introduce a family of functions we call k-rectangle bounded functions, which we will show satisfies this property.
Intuitively, a k-rectangle function only outputs a non-zero value on a multidimensional rectangle that is constrained in only k coordinates.
We begin by showing that we can implement k-rectangle functions for any a, b using a ReLU network of width k and depth 2.
Lemma 1.
For any a, b with k indices i such that a i = 0 or b i = 1, we can construct a k-rectangle bounded function for a, b with a ReLU network of width 3k and depth 2.
Here, we give the formal arguments for the difficulty of model extraction to support informal statements from Section 3.3.
Theorem 1.
There exists a class of width 3k and depth 2 neural networks on domain [0,1] d (with precision p numbers) with d ≥ k that require, given logit access to the networks, Θ(p k ) queries to extract.In order to prove Theorem 1, we introduce a family of functions we call k-rectangle bounded functions, which we will show satisfies this property.
Intuitively, a k-rectangle function only outputs a non-zero value on a multidimensional rectangle that is constrained in only k coordinates.
We begin by showing that we can implement k-rectangle functions for any a, b using a ReLU network of width k and depth 2.
Lemma 1.
For any a, b with k indices i such that a i = 0 or b i = 1, we can construct a k-rectangle bounded function for a, b with a ReLU network of width 3k and depth 2.
